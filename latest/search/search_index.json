{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Soong CLI","text":"<p>Effortless GPU instance management for Lambda Labs</p> <p>Soong is a powerful command-line tool that simplifies managing Lambda Labs GPU instances. Launch, monitor, and connect to GPU instances with a single command\u2014no manual configuration required.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#one-command-instance-management","title":"\ud83d\ude80 One-Command Instance Management","text":"<p>Launch GPU instances with your preferred model and instance type in seconds. No need to navigate web consoles or remember complex API calls.</p> <pre><code>soong start --model deepseek-ai/DeepSeek-R1 --instance-type gpu_1x_h100_pcie\n</code></pre>"},{"location":"#automated-ssh-tunneling","title":"\ud83d\udd12 Automated SSH Tunneling","text":"<p>Securely access your services through automatically configured SSH tunnels:</p> <ul> <li>Port 8000: SGLang inference server</li> <li>Port 5678: n8n workflow automation</li> <li>Port 8080: Instance status daemon</li> </ul>"},{"location":"#built-in-cost-controls","title":"\ud83d\udcb0 Built-in Cost Controls","text":"<p>Set maximum runtime limits to prevent unexpected charges. Your instance automatically stops when the time limit is reached.</p> <pre><code>soong start --max-hours 2  # Auto-stop after 2 hours\n</code></pre>"},{"location":"#smart-model-selection","title":"\ud83e\udd16 Smart Model Selection","text":"<p>Browse available models and get instant recommendations for the right GPU instance type based on VRAM requirements.</p> <pre><code>soong models --recommend deepseek-ai/DeepSeek-R1\n# Recommends: gpu_1x_h100_pcie (80GB VRAM)\n</code></pre>"},{"location":"#real-time-monitoring","title":"\ud83d\udcca Real-time Monitoring","text":"<p>Check instance status, uptime, and cost at any time:</p> <pre><code>soong status\n# Instance: i-abc123def456 (gpu_1x_h100_pcie)\n# Status: running\n# Uptime: 1h 23m\n# Cost: $2.46\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code># Configure your Lambda Labs credentials (one-time setup)\nsoong configure\n\n# Start an instance with DeepSeek-R1\nsoong start --model deepseek-ai/DeepSeek-R1\n\n# SSH into your instance\nsoong ssh\n\n# Check status and cost\nsoong status\n\n# Stop the instance when done\nsoong stop\n</code></pre>"},{"location":"#whats-next","title":"What's Next?","text":"<p>Ready to get started? Follow our step-by-step guide:</p> <ul> <li> <p> Getting Started</p> <p>Set up your Lambda Labs account, install the CLI, and launch your first GPU instance in 5 minutes.</p> </li> <li> <p> Configuration Reference</p> <p>Detailed explanation of all configuration options and advanced settings.</p> </li> <li> <p> Command Reference</p> <p>Complete documentation for every CLI command with examples.</p> </li> <li> <p> Architecture</p> <p>System design, cost controls, and how everything works together.</p> </li> </ul>"},{"location":"#why-soong-cli","title":"Why Soong CLI?","text":"<p>Traditional GPU instance management involves:</p> <ul> <li>Navigating multiple web consoles</li> <li>Manually configuring SSH keys and tunnels</li> <li>Tracking instance costs in spreadsheets</li> <li>Remembering which instance types support which models</li> </ul> <p>Soong CLI eliminates all of this. Configure once, then launch and manage instances with simple commands. Focus on your work, not infrastructure management.</p>"},{"location":"#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.10 or later</li> <li>Operating System: Linux, macOS, or Windows (with WSL)</li> <li>Lambda Labs Account: Sign up here</li> </ul>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub: Report issues or contribute</li> <li>Documentation: You're here!</li> <li>Lambda Labs: Official documentation</li> </ul> <p>New to Lambda Labs?</p> <p>Check out our Prerequisites Guide for step-by-step instructions on setting up your Lambda Labs account.</p>"},{"location":"architecture/","title":"Architecture Documentation","text":"<p>This section provides technical documentation of the soong CLI system architecture, design decisions, and implementation details.</p>"},{"location":"architecture/#contents","title":"Contents","text":"<ul> <li>System Design - Overall architecture and component interactions</li> <li>Cost Controls - Multi-layered cost protection mechanisms</li> </ul>"},{"location":"architecture/#overview","title":"Overview","text":"<p>The soong CLI is a production-ready system for managing GPU instances on Lambda Labs. It provides a robust CLI interface backed by a distributed architecture that emphasizes:</p> <ul> <li>Reliability: Retry logic with exponential backoff for API calls</li> <li>Cost Control: Multiple layers of protection against runaway costs</li> <li>Persistence: Integration with Lambda filesystems for state across sessions</li> <li>Security: Token-based authentication, secure configuration storage</li> <li>User Experience: Rich terminal UI with progress indicators and detailed status</li> </ul>"},{"location":"architecture/#key-components","title":"Key Components","text":"<ol> <li>CLI Tool (<code>soong</code>) - Python-based command-line interface</li> <li>Lambda API Client - HTTP client with retry logic and error handling</li> <li>SSH Tunnel Manager - Manages port forwarding for remote services</li> <li>Instance Manager - Lifecycle management and status polling</li> <li>Status Daemon (on GPU instance) - Health monitoring and lease management</li> <li>Configuration Manager - YAML-based configuration with validation</li> </ol>"},{"location":"architecture/#design-principles","title":"Design Principles","text":""},{"location":"architecture/#fail-safe-defaults","title":"Fail-Safe Defaults","text":"<p>All default settings prioritize safety and cost control:</p> <ul> <li>Default lease: 4 hours (not maximum)</li> <li>Idle timeout: 30 minutes</li> <li>Hard timeout: 8 hours (absolute maximum)</li> <li>Cost estimates shown before launch</li> <li>Confirmation required for destructive operations</li> </ul>"},{"location":"architecture/#single-source-of-truth","title":"Single Source of Truth","text":"<p>Configuration is centralized in <code>~/.config/gpu-dashboard/config.yaml</code> with secure permissions (0600). Command-line flags override defaults but do not modify saved configuration.</p>"},{"location":"architecture/#graceful-degradation","title":"Graceful Degradation","text":"<p>The system continues to function when optional services are unavailable:</p> <ul> <li>Cost estimates work without pricing API</li> <li>Status shows partial info when daemon unreachable</li> <li>History shows local cache when worker unavailable</li> </ul>"},{"location":"architecture/#progressive-disclosure","title":"Progressive Disclosure","text":"<p>The CLI presents information hierarchically:</p> <ol> <li>Critical information first (instance ID, status, IP)</li> <li>Cost and timing details for active instances</li> <li>Extended details via flags (<code>--history</code>, <code>--stopped</code>)</li> <li>Debug information via SSH and logs</li> </ol>"},{"location":"architecture/#next-steps","title":"Next Steps","text":"<ul> <li>System Design - Detailed architecture diagrams</li> <li>Cost Controls - Cost protection mechanisms</li> </ul>"},{"location":"architecture/cost-controls/","title":"Cost Controls","text":"<p>The soong CLI implements four layers of cost protection to prevent runaway expenses from GPU instances. These layers work together to provide defense-in-depth against forgotten instances, idle resources, and configuration errors.</p>"},{"location":"architecture/cost-controls/#overview","title":"Overview","text":"<p>GPU instances on Lambda Labs cost \\(0.50-\\)2.00 per hour depending on GPU type. Without proper controls, a forgotten instance could rack up hundreds of dollars in charges. The multi-layered approach ensures that even if one layer fails, others will catch the issue.</p>"},{"location":"architecture/cost-controls/#layer-1-idle-detection-30-minutes","title":"Layer 1: Idle Detection (30 Minutes)","text":"<p>Purpose: Automatically shut down instances that are not being actively used.</p> <p>How It Works:</p> <p>The status daemon on the GPU instance monitors activity signals:</p> <ul> <li>HTTP requests to SGLang API (port 8000)</li> <li>n8n workflow executions (port 5678)</li> <li>SSH connections to the instance</li> <li>Manual activity signals via <code>/activity</code> endpoint</li> </ul> <p>If no activity is detected for 30 minutes, the daemon initiates a graceful shutdown.</p> <p>Activity Detection:</p> <pre><code>class ActivityMonitor:\n    def __init__(self):\n        self.last_activity = datetime.now()\n        self.idle_timeout = timedelta(minutes=30)\n\n    def signal_activity(self):\n        \"\"\"Reset idle timer.\"\"\"\n        self.last_activity = datetime.now()\n\n    def check_idle(self):\n        \"\"\"Check if instance is idle.\"\"\"\n        elapsed = datetime.now() - self.last_activity\n        return elapsed &gt; self.idle_timeout\n</code></pre> <p>Manual Activity Signal:</p> <pre><code># Keep instance alive during long-running batch jobs\ncurl -X POST \\\n  -H \"Authorization: Bearer &lt;token&gt;\" \\\n  http://&lt;instance-ip&gt;:8080/activity\n</code></pre> <p>Bypass:</p> <p>Activity signals can be sent programmatically from batch jobs, notebooks, or scheduled tasks to prevent shutdown during legitimate long-running work.</p>"},{"location":"architecture/cost-controls/#layer-2-lease-system-4-hours-default","title":"Layer 2: Lease System (4 Hours Default)","text":"<p>Purpose: Require explicit time commitment for instance usage.</p> <p>How It Works:</p> <p>When launching an instance, you specify a lease duration (default 4 hours). The instance will automatically shut down when the lease expires, regardless of activity.</p> <p>Lease Configuration:</p> <pre><code># ~/.config/gpu-dashboard/config.yaml\ndefaults:\n  lease_hours: 4  # Default lease duration\n</code></pre> <p>Launch with Custom Lease:</p> <pre><code># 2-hour lease for quick task\nsoong start --hours 2\n\n# 8-hour lease for all-day work\nsoong start --hours 8\n</code></pre> <p>Extending Leases:</p> <p>Leases can be extended before they expire:</p> <pre><code># Extend by 2 hours\nsoong extend 2\n</code></pre> <p>Or via the web dashboard at http://localhost:8092.</p> <p>Lease Limits:</p> <ul> <li>Minimum: 1 hour</li> <li>Maximum: 8 hours (combined initial + extensions)</li> <li>Default: 4 hours (good balance of flexibility and safety)</li> </ul> <p>Implementation:</p> <pre><code>class LeaseManager:\n    def __init__(self, initial_hours: int):\n        self.created_at = datetime.now()\n        self.shutdown_at = self.created_at + timedelta(hours=initial_hours)\n        self.max_hours = 8\n\n    def extend(self, hours: int) -&gt; bool:\n        \"\"\"Extend lease if within max.\"\"\"\n        total_hours = (self.shutdown_at - self.created_at).total_seconds() / 3600\n        if total_hours + hours &gt; self.max_hours:\n            return False  # Would exceed maximum\n\n        self.shutdown_at += timedelta(hours=hours)\n        return True\n\n    def is_expired(self) -&gt; bool:\n        \"\"\"Check if lease expired.\"\"\"\n        return datetime.now() &gt; self.shutdown_at\n</code></pre>"},{"location":"architecture/cost-controls/#layer-3-hard-timeout-8-hours-maximum","title":"Layer 3: Hard Timeout (8 Hours Maximum)","text":"<p>Purpose: Absolute upper bound on instance lifetime.</p> <p>How It Works:</p> <p>Regardless of lease extensions or activity, every instance has a hard timeout of 8 hours from creation. After 8 hours, the instance will terminate even if:</p> <ul> <li>The lease has been extended multiple times</li> <li>The instance is actively processing requests</li> <li>Manual activity signals are being sent</li> </ul> <p>Rationale:</p> <p>This prevents:</p> <ul> <li>Accidental \"extend forever\" loops from scripts</li> <li>Misconfigured monitoring that keeps instances alive</li> <li>Forgetting to shut down after a long session</li> </ul> <p>Implementation:</p> <pre><code>def check_shutdown_conditions(self):\n    \"\"\"Determine if instance should shut down.\"\"\"\n    uptime = datetime.now() - self.created_at\n\n    # Hard timeout: 8 hours absolute maximum\n    if uptime &gt; timedelta(hours=8):\n        self.shutdown(reason=\"hard_timeout_8h\")\n        return True\n\n    # Lease expiration\n    if datetime.now() &gt; self.shutdown_at:\n        self.shutdown(reason=\"lease_expired\")\n        return True\n\n    # Idle timeout\n    if self.is_idle() and self.idle_elapsed() &gt; timedelta(minutes=30):\n        self.shutdown(reason=\"idle_30m\")\n        return True\n\n    return False\n</code></pre> <p>Override:</p> <p>There is no override for the hard timeout. If you need more than 8 hours of continuous GPU time, you must:</p> <ol> <li>Launch a new instance</li> <li>Migrate work to the new instance</li> <li>Terminate the old instance</li> </ol> <p>This is intentional to prevent infinite-running instances.</p>"},{"location":"architecture/cost-controls/#layer-4-cloudflare-watchdog-optional","title":"Layer 4: Cloudflare Watchdog (Optional)","text":"<p>Purpose: External monitoring to catch instances that fail to self-terminate.</p> <p>How It Works:</p> <p>A Cloudflare Worker monitors the status daemon health endpoint every 5 minutes. If the health check fails 2 consecutive times, the worker terminates the instance via the Lambda API.</p> <p>Health Check Flow:</p> <pre><code>sequenceDiagram\n    participant Worker as Cloudflare Worker\n    participant Daemon as Status Daemon\n    participant API as Lambda API\n    participant Instance\n\n    loop Every 5 minutes\n        Worker-&gt;&gt;Daemon: GET /health\n        alt Healthy\n            Daemon--&gt;&gt;Worker: 200 OK\n            Note over Worker: Reset failure count\n        else Unhealthy\n            Daemon--xWorker: Timeout/Error\n            Note over Worker: Increment failure count\n\n            alt 2 consecutive failures\n                Worker-&gt;&gt;API: POST /instance-operations/terminate\n                API-&gt;&gt;Instance: Terminate\n                Worker-&gt;&gt;Worker: Log termination event\n            end\n        end\n    end</code></pre> <p>Failure Scenarios Detected:</p> <ol> <li>Status daemon crashed or stopped</li> <li>Instance became unresponsive (network issue)</li> <li>Instance frozen or deadlocked</li> <li>Status daemon failed to self-terminate</li> </ol> <p>Implementation:</p> <p>The watchdog is implemented as a Cloudflare Worker (not included in this repo):</p> <pre><code>// Cloudflare Worker (conceptual)\nconst HEALTH_CHECK_INTERVAL = 5 * 60 * 1000; // 5 minutes\nconst FAILURE_THRESHOLD = 2;\n\nasync function checkHealth(instanceIp, token) {\n  try {\n    const response = await fetch(\n      `http://${instanceIp}:8080/health`,\n      { headers: { Authorization: `Bearer ${token}` } }\n    );\n    return response.ok;\n  } catch (error) {\n    return false;\n  }\n}\n\nasync function handleHealthCheck(instanceId, instanceIp) {\n  const healthy = await checkHealth(instanceIp, STATUS_TOKEN);\n\n  if (!healthy) {\n    failures[instanceId] = (failures[instanceId] || 0) + 1;\n\n    if (failures[instanceId] &gt;= FAILURE_THRESHOLD) {\n      await terminateInstance(instanceId);\n      await logTerminationEvent(instanceId, 'watchdog');\n      delete failures[instanceId];\n    }\n  } else {\n    failures[instanceId] = 0;\n  }\n}\n</code></pre> <p>Setup:</p> <p>The watchdog is optional and requires:</p> <ol> <li>Cloudflare Workers account</li> <li>Lambda API key stored in worker secrets</li> <li>Status daemon token shared between CLI and worker</li> <li>Worker deployed with scheduled trigger (5 min)</li> </ol> <p>Benefits:</p> <ul> <li>External monitoring (not dependent on instance)</li> <li>Catches cases where status daemon fails</li> <li>Provides termination history via worker KV store</li> <li>Low cost (&lt;$1/month for typical usage)</li> </ul>"},{"location":"architecture/cost-controls/#shutdown-decision-flowchart","title":"Shutdown Decision Flowchart","text":"<pre><code>flowchart TD\n    Start([Check Shutdown&lt;br/&gt;Every 60s]) --&gt; Uptime{Uptime &gt; 8 hours?}\n\n    Uptime --&gt;|Yes| HardTimeout[Shutdown: hard_timeout_8h]\n    Uptime --&gt;|No| LeaseCheck{Lease expired?}\n\n    LeaseCheck --&gt;|Yes| LeaseExpired[Shutdown: lease_expired]\n    LeaseCheck --&gt;|No| IdleCheck{Idle &gt; 30 min?}\n\n    IdleCheck --&gt;|Yes| IdleTimeout[Shutdown: idle_30m]\n    IdleCheck --&gt;|No| Continue[Continue running]\n\n    HardTimeout --&gt; Terminate[Graceful shutdown:&lt;br/&gt;1. Stop services&lt;br/&gt;2. Sync filesystem&lt;br/&gt;3. Terminate instance]\n    LeaseExpired --&gt; Terminate\n    IdleTimeout --&gt; Terminate\n\n    Continue --&gt; Start\n    Terminate --&gt; LogEvent[Log termination event]\n    LogEvent --&gt; End([Instance terminated])\n\n    style HardTimeout fill:#ff6b6b\n    style LeaseExpired fill:#ffa726\n    style IdleTimeout fill:#ffd93d\n    style Continue fill:#6bcf7f</code></pre>"},{"location":"architecture/cost-controls/#termination-events","title":"Termination Events","text":"<p>All terminations are logged with the following information:</p> <pre><code>{\n  \"timestamp\": \"2025-01-01T14:30:00Z\",\n  \"instance_id\": \"inst_abc123\",\n  \"event_type\": \"termination\",\n  \"reason\": \"idle_30m\",\n  \"uptime_minutes\": 125,\n  \"gpu_type\": \"gpu_1x_a100_sxm4_80gb\",\n  \"region\": \"us-west-1\"\n}\n</code></pre> <p>Termination Reasons:</p> Reason Description Layer <code>idle_30m</code> No activity for 30 minutes Layer 1 <code>lease_expired</code> Lease duration reached Layer 2 <code>hard_timeout_8h</code> 8-hour maximum runtime Layer 3 <code>watchdog</code> Cloudflare watchdog detected failure Layer 4 <code>manual</code> User terminated via CLI or dashboard User action <p>Viewing History:</p> <pre><code># Show last 24 hours of terminations\nsoong status --history\n\n# Show last 7 days\nsoong status --history --history-hours 168\n</code></pre>"},{"location":"architecture/cost-controls/#cost-estimation","title":"Cost Estimation","text":"<p>The CLI shows cost estimates before launching instances:</p> <pre><code>$ soong start --model deepseek-r1-70b\n\nCost Estimate\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nGPU: 1x A100 SXM4 (80 GB)\nRate: $1.29/hr\nDuration: 4 hours\n\nEstimated cost: $5.16\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\nProceed with launch? [Y/n]:\n</code></pre> <p>Cost Calculations:</p> <pre><code>def estimate_cost(gpu_type: str, hours: int) -&gt; float:\n    \"\"\"Estimate cost for instance lease.\"\"\"\n    pricing = api.get_instance_type(gpu_type)\n    hourly_rate = pricing.price_per_hour\n    return hourly_rate * hours\n\ndef show_cost_estimate(gpu_type, hours):\n    \"\"\"Display cost estimate and get confirmation.\"\"\"\n    cost = estimate_cost(gpu_type, hours)\n    console.print(Panel(\n        f\"GPU: {gpu_type.description}\\n\"\n        f\"Rate: ${gpu_type.price_per_hour:.2f}/hr\\n\"\n        f\"Duration: {hours} hours\\n\\n\"\n        f\"[bold yellow]Estimated cost: ${cost:.2f}[/bold yellow]\"\n    ))\n    return questionary.confirm(\"Proceed with launch?\").ask()\n</code></pre>"},{"location":"architecture/cost-controls/#real-time-cost-tracking","title":"Real-Time Cost Tracking","text":"<p>The status command shows accumulated costs:</p> <pre><code>$ soong status\n\nGPU Instances\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nID       Status  Uptime    Time Left  Cost Now  Est. Total\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ninst_abc active  2h 15m    1h 45m     $2.91     $5.16\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n</code></pre> <p>Columns Explained:</p> <ul> <li>Uptime: Time since instance launched</li> <li>Time Left: Time until lease expires</li> <li>Cost Now: Current accumulated cost (uptime \u00d7 hourly rate)</li> <li>Est. Total: Estimated total cost if lease runs to completion</li> </ul> <p>Expired Leases:</p> <p>When a lease expires, the time left shows in red:</p> <pre><code>Time Left: [red]EXPIRED[/red]\nCost Now: [red]$6.45[/red]  # Still accruing!\n</code></pre> <p>This indicates the instance should have shut down but hasn't. Possible causes:</p> <ul> <li>Status daemon failed</li> <li>Network issue preventing termination</li> <li>Manual override</li> </ul> <p>Action: Manually terminate the instance to stop charges.</p>"},{"location":"architecture/cost-controls/#best-practices","title":"Best Practices","text":""},{"location":"architecture/cost-controls/#for-development-sessions","title":"For Development Sessions","text":"<pre><code># Short exploratory work (2 hours)\nsoong start --hours 2\n\n# Full development session (6 hours)\nsoong start --hours 6\n\n# Extend if you need more time\nsoong extend 2\n</code></pre>"},{"location":"architecture/cost-controls/#for-batch-jobs","title":"For Batch Jobs","text":"<pre><code># Keep instance alive during processing\nimport requests\n\ndef keep_alive(instance_ip, token):\n    \"\"\"Signal activity every 20 minutes.\"\"\"\n    while processing:\n        requests.post(\n            f\"http://{instance_ip}:8080/activity\",\n            headers={\"Authorization\": f\"Bearer {token}\"}\n        )\n        time.sleep(20 * 60)  # 20 minutes\n</code></pre>"},{"location":"architecture/cost-controls/#for-long-running-tasks","title":"For Long-Running Tasks","text":"<p>If you need more than 8 hours:</p> <ol> <li>Break into chunks: Design tasks to complete in &lt;8 hour segments</li> <li>Save checkpoints: Persist state to filesystem between instances</li> <li>Use queues: Submit work items to a queue, process in batches</li> <li>Consider dedicated instances: For truly long-running work, use Lambda's dedicated instances (not via this CLI)</li> </ol>"},{"location":"architecture/cost-controls/#cost-monitoring","title":"Cost Monitoring","text":"<p>Set up alerting for costs:</p> <ol> <li>Lambda billing alerts: Configure in Lambda dashboard</li> <li>History monitoring: Check <code>soong status --history</code> daily</li> <li>Budget tracking: Set monthly GPU budget and track spending</li> </ol>"},{"location":"architecture/cost-controls/#emergency-shutdown","title":"Emergency Shutdown","text":"<p>If you suspect an instance is running but shouldn't be:</p> <pre><code># List all instances\nsoong status\n\n# Terminate immediately\nsoong stop --yes\n</code></pre> <p>Or via Lambda dashboard:</p> <ol> <li>Go to https://cloud.lambdalabs.com/instances</li> <li>Find the instance</li> <li>Click \"Terminate\"</li> </ol>"},{"location":"architecture/cost-controls/#testing-cost-controls","title":"Testing Cost Controls","text":"<p>Idle timeout test:</p> <pre><code># Launch instance\nsoong start --hours 1\n\n# Wait 30 minutes without activity\n# Instance should auto-terminate\n</code></pre> <p>Lease expiration test:</p> <pre><code># Launch with 1-hour lease\nsoong start --hours 1\n\n# Keep sending activity signals\nwhile true; do\n  curl -X POST -H \"Authorization: Bearer &lt;token&gt;\" \\\n    http://&lt;ip&gt;:8080/activity\n  sleep 10m\ndone\n\n# Instance should still terminate after 1 hour\n</code></pre> <p>Hard timeout test:</p> <pre><code># Launch instance\nsoong start --hours 4\n\n# Extend multiple times\nsoong extend 2  # Total: 6 hours\nsoong extend 2  # Total: 8 hours\nsoong extend 1  # Should fail (exceeds maximum)\n\n# Keep instance active\n# Should terminate after 8 hours regardless\n</code></pre>"},{"location":"architecture/cost-controls/#troubleshooting","title":"Troubleshooting","text":""},{"location":"architecture/cost-controls/#instance-didnt-shut-down","title":"Instance Didn't Shut Down","text":"<p>Check status daemon:</p> <pre><code>ssh ubuntu@&lt;instance-ip&gt;\nsystemctl status status-daemon\njournalctl -u status-daemon -n 50\n</code></pre> <p>Manually terminate:</p> <pre><code>soong stop --yes\n</code></pre>"},{"location":"architecture/cost-controls/#lease-extension-failed","title":"Lease Extension Failed","text":"<p>Common causes:</p> <ol> <li>Would exceed 8-hour maximum</li> <li>Lease already expired</li> <li>Instance unreachable</li> </ol> <p>Check total hours:</p> <pre><code>soong status\n# Look at \"Time Left\" and \"Uptime\" to calculate total\n</code></pre>"},{"location":"architecture/cost-controls/#costs-higher-than-expected","title":"Costs Higher Than Expected","text":"<p>Review history:</p> <pre><code>soong status --history --history-hours 168\n</code></pre> <p>Check for:</p> <ul> <li>Instances that ran longer than expected</li> <li>Multiple simultaneous instances</li> <li>Instances that didn't auto-terminate</li> </ul>"},{"location":"architecture/cost-controls/#implementation-reference","title":"Implementation Reference","text":"<p>Key files implementing cost controls:</p> <ul> <li><code>cli/src/soong/cli.py</code> - Cost estimation, confirmation prompts</li> <li>Status daemon (on instance) - Idle detection, lease management, hard timeout</li> <li>Cloudflare Worker (optional) - External watchdog monitoring</li> </ul>"},{"location":"architecture/cost-controls/#future-improvements","title":"Future Improvements","text":"<p>Planned enhancements to cost controls:</p> <ol> <li>Predictive cost alerts: Warn when approaching budget limits</li> <li>Usage quotas: Per-user or per-project spending limits</li> <li>Scheduled shutdowns: Auto-terminate at specific times (e.g., end of business day)</li> <li>Cost optimization: Automatic GPU selection for cost/performance</li> <li>Shared instances: Multi-user access to reduce per-user costs</li> </ol>"},{"location":"architecture/system-design/","title":"System Design","text":"<p>This document describes the overall architecture of the soong CLI system, including component interactions, data flow, and design decisions.</p>"},{"location":"architecture/system-design/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>graph TB\n    subgraph \"Local Machine\"\n        CLI[CLI Tool&lt;br/&gt;soong]\n        Config[Config Manager&lt;br/&gt;~/.config/gpu-dashboard/]\n        SSHMgr[SSH Tunnel Manager]\n\n        CLI --&gt; Config\n        CLI --&gt; SSHMgr\n    end\n\n    subgraph \"Lambda Labs API\"\n        API[REST API&lt;br/&gt;cloud.lambdalabs.com]\n    end\n\n    subgraph \"GPU Instance\"\n        StatusD[Status Daemon&lt;br/&gt;:8080]\n        SGLang[SGLang Server&lt;br/&gt;:8000]\n        N8N[n8n Workflows&lt;br/&gt;:5678]\n\n        StatusD -.monitors.-&gt; SGLang\n        StatusD -.monitors.-&gt; N8N\n    end\n\n    subgraph \"Persistent Storage\"\n        FS[Lambda Filesystem&lt;br/&gt;/lambda/nfs/coding-stack/]\n        Secrets[secrets/env.sh]\n        Projects[projects/]\n\n        FS --&gt; Secrets\n        FS --&gt; Projects\n    end\n\n    CLI --&gt;|Launch/Terminate| API\n    CLI --&gt;|Status Query| API\n    CLI --&gt;|SSH/Tunnel| StatusD\n\n    API --&gt;|Provision| GPU Instance\n    GPU Instance --&gt;|Mount| FS\n\n    style CLI fill:#e1f5ff\n    style StatusD fill:#fff3e0\n    style API fill:#f3e5f5\n    style FS fill:#e8f5e9</code></pre>"},{"location":"architecture/system-design/#component-details","title":"Component Details","text":""},{"location":"architecture/system-design/#cli-tool","title":"CLI Tool","text":"<p>Location: <code>cli/src/soong/cli.py</code></p> <p>Responsibilities:</p> <ul> <li>Parse user commands and arguments</li> <li>Load configuration from YAML</li> <li>Invoke Lambda API via LambdaAPI client</li> <li>Manage SSH tunnels</li> <li>Display rich terminal output</li> </ul> <p>Key Features:</p> <ul> <li>Interactive configuration wizard with intelligent defaults</li> <li>Cost estimation before instance launch</li> <li>Rich table display with color-coded status</li> <li>Progress indicators for long-running operations</li> <li>GPU/model recommendation system</li> </ul> <p>Entry Point:</p> <pre><code>@app.command()\ndef start(\n    model: Optional[str] = None,\n    gpu: Optional[str] = None,\n    region: Optional[str] = None,\n    hours: Optional[int] = None,\n    name: Optional[str] = None,\n):\n    \"\"\"Launch new GPU instance.\"\"\"\n</code></pre>"},{"location":"architecture/system-design/#lambda-api-client","title":"Lambda API Client","text":"<p>Location: <code>cli/src/soong/lambda_api.py</code></p> <p>Responsibilities:</p> <ul> <li>HTTP communication with Lambda Labs API</li> <li>Retry logic with exponential backoff</li> <li>Response parsing and error handling</li> <li>Instance lifecycle operations</li> </ul> <p>Retry Strategy:</p> <pre><code>RETRY_MAX_ATTEMPTS = 3\nRETRY_BASE_DELAY = 1\nRETRY_BACKOFF_MULTIPLIER = 2\n\n# Delays: 1s, 2s, 4s\n</code></pre> <p>Endpoints:</p> Endpoint Method Purpose <code>/instances</code> GET List all instances <code>/instance-operations/launch</code> POST Launch new instance <code>/instance-operations/terminate</code> POST Terminate instance <code>/instance-types</code> GET List GPU types with pricing <code>/ssh-keys</code> GET List SSH keys"},{"location":"architecture/system-design/#instance-manager","title":"Instance Manager","text":"<p>Location: <code>cli/src/soong/instance.py</code></p> <p>Responsibilities:</p> <ul> <li>Wait for instance to become ready</li> <li>Poll instance status</li> <li>Find active instances</li> </ul> <p>Ready Criteria:</p> <p>An instance is considered ready when:</p> <ol> <li>Status is <code>active</code></li> <li>IP address is assigned</li> <li>Not in terminal state (<code>terminated</code>, <code>unhealthy</code>)</li> </ol> <p>Polling Logic:</p> <pre><code>def wait_for_ready(instance_id: str, timeout_seconds: int = 600):\n    poll_interval = 10  # seconds\n    while elapsed &lt; timeout_seconds:\n        instance = api.get_instance(instance_id)\n        if instance.status == \"active\" and instance.ip:\n            return instance\n        time.sleep(poll_interval)\n</code></pre>"},{"location":"architecture/system-design/#ssh-tunnel-manager","title":"SSH Tunnel Manager","text":"<p>Location: <code>cli/src/soong/ssh.py</code></p> <p>Responsibilities:</p> <ul> <li>Start background SSH tunnels with port forwarding</li> <li>Stop tunnels gracefully</li> <li>Track tunnel PID for cleanup</li> <li>Interactive SSH sessions</li> </ul> <p>Port Forwarding:</p> <pre><code>ssh -N -f \\\n  -L 8000:localhost:8000 \\  # SGLang\n  -L 5678:localhost:5678 \\  # n8n\n  -L 8080:localhost:8080 \\  # Status daemon\n  -i ~/.ssh/id_rsa \\\n  ubuntu@&lt;instance-ip&gt;\n</code></pre> <p>PID Tracking:</p> <p>Tunnel process ID stored in <code>~/.config/gpu-dashboard/tunnel.pid</code> for cleanup.</p>"},{"location":"architecture/system-design/#configuration-manager","title":"Configuration Manager","text":"<p>Location: <code>cli/src/soong/config.py</code></p> <p>Responsibilities:</p> <ul> <li>Load/save YAML configuration</li> <li>Validate custom model definitions</li> <li>Secure file permissions</li> </ul> <p>Configuration Schema:</p> <pre><code>lambda:\n  api_key: string          # Lambda API key\n  default_region: string   # e.g., \"us-west-1\"\n  filesystem_name: string  # e.g., \"coding-stack\"\n\nstatus_daemon:\n  token: string            # Shared secret\n  port: int                # Default: 8080\n\ndefaults:\n  model: string            # Default model ID\n  gpu: string              # Default GPU type\n  lease_hours: int         # Default: 4\n\nssh:\n  key_path: string         # SSH private key path\n\ncustom_models:\n  &lt;model-id&gt;:\n    hf_path: string\n    params_billions: float\n    quantization: string\n    context_length: int\n</code></pre>"},{"location":"architecture/system-design/#status-daemon","title":"Status Daemon","text":"<p>Location: On GPU instance (deployed via cloud-init)</p> <p>Responsibilities:</p> <ul> <li>Health monitoring</li> <li>Lease management</li> <li>Idle detection</li> <li>Activity tracking</li> <li>Graceful shutdown</li> </ul> <p>API Endpoints:</p> Endpoint Method Purpose <code>/health</code> GET Health check <code>/status</code> GET Instance status <code>/extend</code> POST Extend lease <code>/shutdown</code> POST Graceful shutdown <code>/activity</code> POST Signal activity <p>Authentication:</p> <p>All endpoints require <code>Authorization: Bearer &lt;token&gt;</code> header.</p>"},{"location":"architecture/system-design/#data-flow-instance-launch","title":"Data Flow: Instance Launch","text":"<pre><code>sequenceDiagram\n    participant User\n    participant CLI\n    participant Config\n    participant API as Lambda API\n    participant Instance as GPU Instance\n    participant Daemon as Status Daemon\n\n    User-&gt;&gt;CLI: soong start\n    CLI-&gt;&gt;Config: Load configuration\n    Config--&gt;&gt;CLI: API key, defaults\n\n    CLI-&gt;&gt;API: GET /instance-types\n    API--&gt;&gt;CLI: Pricing info\n\n    CLI-&gt;&gt;User: Show cost estimate\n    User-&gt;&gt;CLI: Confirm\n\n    CLI-&gt;&gt;API: POST /instance-operations/launch\n    API--&gt;&gt;CLI: Instance ID\n\n    CLI-&gt;&gt;API: GET /instances (poll)\n    API--&gt;&gt;CLI: Status: booting\n\n    Note over Instance: Cloud-init runs\n    Note over Instance: Mount filesystem\n    Note over Instance: Deploy services\n\n    API--&gt;&gt;CLI: Status: active, IP assigned\n\n    CLI-&gt;&gt;Daemon: GET /health\n    Daemon--&gt;&gt;CLI: Healthy\n\n    CLI-&gt;&gt;User: Instance ready!</code></pre>"},{"location":"architecture/system-design/#launch-steps","title":"Launch Steps","text":"<ol> <li>User initiates launch</li> <li>Command: <code>soong start --model deepseek-r1-70b</code></li> <li> <p>CLI loads configuration from YAML</p> </li> <li> <p>API preparation</p> </li> <li>Fetch instance types for pricing</li> <li>Show cost estimate and get confirmation</li> <li> <p>Validate SSH keys exist in account</p> </li> <li> <p>Instance provisioning (Lambda API)</p> </li> <li>POST to <code>/instance-operations/launch</code></li> <li>Payload includes region, GPU type, SSH keys, filesystem</li> <li> <p>API returns instance ID immediately</p> </li> <li> <p>Polling for ready state</p> </li> <li>Poll <code>/instances</code> every 10 seconds</li> <li>Wait for status <code>active</code> and IP assignment</li> <li> <p>Timeout after 600 seconds (10 minutes)</p> </li> <li> <p>Bootstrap completion (cloud-init on instance)</p> </li> <li>Mount persistent filesystem</li> <li>Load secrets from <code>/lambda/nfs/coding-stack/secrets/</code></li> <li>Deploy SGLang, n8n, status daemon</li> <li> <p>Start monitoring</p> </li> <li> <p>Confirmation</p> </li> <li>CLI displays instance IP</li> <li>Shows SSH and status commands</li> <li>User can connect via <code>soong ssh</code></li> </ol>"},{"location":"architecture/system-design/#data-flow-status-query","title":"Data Flow: Status Query","text":"<pre><code>sequenceDiagram\n    participant User\n    participant CLI\n    participant API as Lambda API\n    participant Instance\n    participant Daemon as Status Daemon\n\n    User-&gt;&gt;CLI: soong status\n\n    CLI-&gt;&gt;API: GET /instances\n    API--&gt;&gt;CLI: Instance list\n\n    CLI-&gt;&gt;API: GET /instance-types\n    API--&gt;&gt;CLI: Pricing (cached)\n\n    CLI-&gt;&gt;Daemon: GET /status (optional)\n    Daemon--&gt;&gt;CLI: Uptime, model, lease info\n\n    Note over CLI: Calculate costs&lt;br/&gt;Format lease time&lt;br/&gt;Color-code status\n\n    CLI-&gt;&gt;User: Display table</code></pre>"},{"location":"architecture/system-design/#status-information-sources","title":"Status Information Sources","text":"<ol> <li>Lambda API (required)</li> <li>Instance ID, name, status</li> <li>IP address</li> <li>GPU type, region</li> <li>Created timestamp</li> <li> <p>Lease expiration</p> </li> <li> <p>Pricing API (optional, cached)</p> </li> <li>Hourly rate for GPU type</li> <li> <p>Cost calculations</p> </li> <li> <p>Status Daemon (optional)</p> </li> <li>Current uptime</li> <li>Loaded model</li> <li>Last activity timestamp</li> <li>Shutdown time</li> </ol>"},{"location":"architecture/system-design/#cost-calculations","title":"Cost Calculations","text":"<pre><code># Current cost\nuptime_hours = (now - created_at).total_seconds() / 3600\ncurrent_cost = hourly_rate * uptime_hours\n\n# Estimated total\nlease_hours = (lease_expires_at - created_at).total_seconds() / 3600\ntotal_cost = hourly_rate * lease_hours\n</code></pre>"},{"location":"architecture/system-design/#security-model","title":"Security Model","text":""},{"location":"architecture/system-design/#authentication-layers","title":"Authentication Layers","text":"<ol> <li>Lambda API: Bearer token authentication</li> <li>API key stored in <code>~/.config/gpu-dashboard/config.yaml</code> (mode 0600)</li> <li>Never logged or displayed</li> <li> <p>Passed in <code>Authorization</code> header</p> </li> <li> <p>Status Daemon: Shared secret authentication</p> </li> <li>Token generated during configuration (32-byte URL-safe)</li> <li>Stored in config and deployed to instance</li> <li> <p>Required for all daemon endpoints</p> </li> <li> <p>SSH Access: Key-based authentication</p> </li> <li>Private key path configurable</li> <li>SSH keys managed in Lambda account</li> <li>Tunnel uses <code>-o StrictHostKeyChecking=no</code> for automation</li> </ol>"},{"location":"architecture/system-design/#secret-management","title":"Secret Management","text":"<p>Local secrets:</p> <ul> <li><code>~/.config/gpu-dashboard/config.yaml</code> (mode 0600)</li> <li>Contains API keys, tokens, SSH key paths</li> </ul> <p>Instance secrets:</p> <ul> <li><code>/lambda/nfs/coding-stack/secrets/env.sh</code></li> <li>Sourced by services during startup</li> <li>Contains Anthropic API keys, service tokens</li> </ul>"},{"location":"architecture/system-design/#network-security","title":"Network Security","text":"<p>Inbound:</p> <ul> <li>Lambda instances have public IPs</li> <li>SSH (port 22) open for management</li> <li>Services (8000, 5678, 8080) open for development</li> <li>Production deployments should use VPC</li> </ul> <p>Outbound:</p> <ul> <li>No restrictions</li> <li>Services make API calls (OpenAI, Anthropic, etc.)</li> <li>Metrics/monitoring can send to external services</li> </ul>"},{"location":"architecture/system-design/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/system-design/#api-rate-limiting","title":"API Rate Limiting","text":"<p>Lambda API has rate limits (undocumented). The client implements:</p> <ul> <li>Exponential backoff on errors</li> <li>Caching of instance types (pricing)</li> <li>Efficient polling intervals (10s)</li> </ul>"},{"location":"architecture/system-design/#instance-boot-time","title":"Instance Boot Time","text":"<p>Typical boot sequence:</p> <ol> <li>Instance provisioning: 30-60 seconds</li> <li>Cloud-init execution: 2-3 minutes</li> <li>Service deployment: 3-5 minutes</li> <li>Model loading (SGLang): 2-5 minutes</li> </ol> <p>Total: 7-14 minutes for ready state</p>"},{"location":"architecture/system-design/#filesystem-mounting","title":"Filesystem Mounting","text":"<p>Persistent filesystem mount is fast (1-2 seconds) because Lambda uses high-performance NFS.</p>"},{"location":"architecture/system-design/#error-handling","title":"Error Handling","text":""},{"location":"architecture/system-design/#api-errors","title":"API Errors","text":"<p>Transient errors (retry with backoff):</p> <ul> <li>Network timeouts</li> <li>5xx server errors</li> <li>Rate limiting (429)</li> </ul> <p>Permanent errors (fail immediately):</p> <ul> <li>Invalid API key (401)</li> <li>Resource not found (404)</li> <li>Insufficient quota (403)</li> </ul>"},{"location":"architecture/system-design/#instance-states","title":"Instance States","text":"<p>Terminal states (stop polling):</p> <ul> <li><code>terminated</code></li> <li><code>unhealthy</code></li> </ul> <p>Transient states (keep polling):</p> <ul> <li><code>booting</code></li> <li><code>pending</code></li> <li><code>active</code> (without IP yet)</li> </ul>"},{"location":"architecture/system-design/#ssh-tunnel-errors","title":"SSH Tunnel Errors","text":"<p>Common failures:</p> <ul> <li>Port already in use (tunnel exists)</li> <li>Connection refused (instance not ready)</li> <li>Authentication failure (wrong key)</li> </ul> <p>Recovery:</p> <ul> <li>Check tunnel status before starting</li> <li>Stop existing tunnel automatically</li> <li>Verify instance has IP before connecting</li> </ul>"},{"location":"architecture/system-design/#deployment-considerations","title":"Deployment Considerations","text":""},{"location":"architecture/system-design/#local-development","title":"Local Development","text":"<pre><code>cd cli\npip install -e \".[test]\"\nsoong configure\n</code></pre>"},{"location":"architecture/system-design/#production-usage","title":"Production Usage","text":"<p>For production deployments, consider:</p> <ol> <li>VPC networking for private instance access</li> <li>Cloudflare Workers for watchdog monitoring</li> <li>Secrets management via environment variables</li> <li>Log aggregation for debugging</li> <li>Cost alerts for budget monitoring</li> </ol>"},{"location":"architecture/system-design/#multi-user-setup","title":"Multi-User Setup","text":"<p>The CLI is designed for single-user workstations. For multi-user scenarios:</p> <ul> <li>Each user needs their own <code>~/.config/gpu-dashboard/</code></li> <li>Shared Lambda account (API key) or separate accounts</li> <li>Filesystem naming convention (e.g., <code>coding-stack-{username}</code>)</li> <li>Cost tracking per user via instance naming</li> </ul>"},{"location":"architecture/system-design/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"architecture/system-design/#cli-logging","title":"CLI Logging","text":"<p>The CLI uses Rich console for output:</p> <ul> <li>Color-coded status (green=success, red=error, yellow=warning)</li> <li>Progress spinners for long operations</li> <li>Tables for structured data</li> </ul>"},{"location":"architecture/system-design/#instance-logs","title":"Instance Logs","text":"<p>On the GPU instance:</p> <ul> <li><code>/var/log/gpu-stack-boot.log</code> - Cloud-init execution</li> <li><code>journalctl -u sglang</code> - SGLang service logs</li> <li><code>journalctl -u n8n</code> - n8n workflow logs</li> <li><code>journalctl -u status-daemon</code> - Status daemon logs</li> </ul>"},{"location":"architecture/system-design/#health-checks","title":"Health Checks","text":"<p>Status daemon health:</p> <pre><code>curl -H \"Authorization: Bearer &lt;token&gt;\" \\\n  http://&lt;instance-ip&gt;:8080/health\n</code></pre> <p>Service health:</p> <ul> <li>SGLang: <code>curl http://localhost:8000/health</code></li> <li>n8n: <code>curl http://localhost:5678/healthz</code></li> </ul>"},{"location":"architecture/system-design/#future-enhancements","title":"Future Enhancements","text":""},{"location":"architecture/system-design/#planned-features","title":"Planned Features","text":"<ol> <li>Instance pools: Manage multiple instances</li> <li>Snapshot support: Save instance state</li> <li>GPU sharing: Multi-tenant GPU access</li> <li>Cost analytics: Historical spending reports</li> <li>Automatic model selection: Based on task requirements</li> </ol>"},{"location":"architecture/system-design/#scalability","title":"Scalability","text":"<p>The current design supports:</p> <ul> <li>Single user managing 1-10 instances</li> <li>Manual orchestration of services</li> </ul> <p>For larger deployments:</p> <ul> <li>Consider Kubernetes for orchestration</li> <li>Implement instance pool management</li> <li>Add cost allocation and chargebacks</li> <li>Integrate with existing DevOps tooling</li> </ul>"},{"location":"contributing/","title":"Contributing Guide","text":"<p>Thank you for your interest in contributing to soong CLI! This guide will help you get started with development, testing, and submitting contributions.</p>"},{"location":"contributing/#contents","title":"Contents","text":"<ul> <li>Development Setup - Set up your local development environment</li> <li>Testing - Run tests and add new test coverage</li> </ul>"},{"location":"contributing/#quick-start","title":"Quick Start","text":"<pre><code># Clone the repository\ngit clone https://github.com/yourusername/my-ai.git\ncd my-ai/cli\n\n# Create virtual environment\npython3 -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install in development mode with test dependencies\npip install -e \".[test]\"\n\n# Run tests\npytest\n\n# Run the CLI in development mode\nsoong --help\n</code></pre>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>cli/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 soong/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 cli.py              # CLI commands and interface\n\u2502       \u251c\u2500\u2500 config.py           # Configuration management\n\u2502       \u251c\u2500\u2500 instance.py         # Instance lifecycle management\n\u2502       \u251c\u2500\u2500 lambda_api.py       # Lambda Labs API client\n\u2502       \u251c\u2500\u2500 ssh.py              # SSH tunnel management\n\u2502       \u251c\u2500\u2500 models.py           # Model definitions and GPU mapping\n\u2502       \u2514\u2500\u2500 history.py          # Termination history tracking\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 conftest.py            # Shared test fixtures\n\u2502   \u251c\u2500\u2500 test_*.py              # Test modules\n\u2502   \u2514\u2500\u2500 helpers/\n\u2502       \u2514\u2500\u2500 assertions.py      # Custom test assertions\n\u251c\u2500\u2500 pyproject.toml             # Project metadata and dependencies\n\u2514\u2500\u2500 README.md                  # CLI documentation\n</code></pre>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#1-create-a-feature-branch","title":"1. Create a Feature Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"contributing/#2-make-your-changes","title":"2. Make Your Changes","text":"<p>Follow the coding standards outlined below.</p>"},{"location":"contributing/#3-write-tests","title":"3. Write Tests","text":"<p>All new features and bug fixes should include tests. See Testing for details.</p>"},{"location":"contributing/#4-run-tests-locally","title":"4. Run Tests Locally","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage report\npytest --cov=soong --cov-report=term-missing\n\n# Run specific test file\npytest tests/test_models.py\n\n# Run specific test\npytest tests/test_models.py::test_estimate_vram_llama_70b_int4\n</code></pre>"},{"location":"contributing/#5-submit-a-pull-request","title":"5. Submit a Pull Request","text":"<ul> <li>Push your branch to GitHub</li> <li>Create a pull request with a clear description</li> <li>Ensure all CI checks pass</li> <li>Request review from maintainers</li> </ul>"},{"location":"contributing/#coding-standards","title":"Coding Standards","text":""},{"location":"contributing/#python-style","title":"Python Style","text":"<p>We follow PEP 8 with some modifications:</p> <ul> <li>Line length: 88 characters (Black default)</li> <li>Indentation: 4 spaces</li> <li>Quotes: Double quotes for strings</li> <li>Imports: Grouped by standard library, third-party, local</li> </ul>"},{"location":"contributing/#type-hints","title":"Type Hints","text":"<p>All public functions should have type hints:</p> <pre><code>def launch_instance(\n    region: str,\n    instance_type: str,\n    ssh_key_names: List[str],\n    filesystem_names: Optional[List[str]] = None,\n) -&gt; str:\n    \"\"\"Launch a new GPU instance.\"\"\"\n    ...\n</code></pre>"},{"location":"contributing/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings:</p> <pre><code>def estimate_vram(\n    params_billions: float,\n    quantization: Quantization = Quantization.FP16,\n    context_length: int = 8192,\n) -&gt; dict:\n    \"\"\"\n    Estimate VRAM requirements for any model.\n\n    Args:\n        params_billions: Parameter count in billions\n        quantization: Quantization level\n        context_length: Context window size\n\n    Returns:\n        Dict with VRAM estimates and recommended GPU\n\n    Example:\n        &gt;&gt;&gt; estimate_vram(70, Quantization.INT4, 8192)\n        {'base_vram_gb': 35.0, 'total_estimated_gb': 41.1, ...}\n    \"\"\"\n    ...\n</code></pre>"},{"location":"contributing/#error-handling","title":"Error Handling","text":"<p>Use specific exception types and provide helpful error messages:</p> <pre><code># Good\nif not instance:\n    console.print(\"[red]Error: Instance not found[/red]\")\n    raise typer.Exit(1)\n\n# Bad\nif not instance:\n    raise Exception(\"error\")\n</code></pre>"},{"location":"contributing/#cli-output","title":"CLI Output","text":"<p>Use Rich console for all output:</p> <pre><code>from rich.console import Console\nconsole = Console()\n\n# Success\nconsole.print(\"[green]Instance launched successfully[/green]\")\n\n# Error\nconsole.print(\"[red]Error: API key invalid[/red]\")\n\n# Warning\nconsole.print(\"[yellow]Warning: Lease will expire in 10 minutes[/yellow]\")\n\n# Info\nconsole.print(\"[cyan]Checking instance status...[/cyan]\")\n</code></pre>"},{"location":"contributing/#code-organization","title":"Code Organization","text":""},{"location":"contributing/#adding-new-commands","title":"Adding New Commands","text":"<p>New CLI commands go in <code>cli.py</code>:</p> <pre><code>@app.command()\ndef your_command(\n    arg1: str = typer.Argument(..., help=\"Required argument\"),\n    option1: Optional[str] = typer.Option(None, help=\"Optional flag\"),\n):\n    \"\"\"Brief description of what this command does.\"\"\"\n    config = get_config()\n    # Implementation\n</code></pre>"},{"location":"contributing/#adding-new-models","title":"Adding New Models","text":"<p>Add model definitions to <code>models.py</code>:</p> <pre><code>_register_model(\n    model_id=\"your-model\",\n    name=\"Your Model Name\",\n    hf_path=\"org/model-name\",\n    params_billions=13,\n    quantization=Quantization.FP16,\n    context_length=8192,\n    description=\"Brief description\",\n    good_for=[\"Use case 1\", \"Use case 2\"],\n    not_good_for=[\"Anti-pattern 1\"],\n    notes=\"Additional information\",\n)\n</code></pre>"},{"location":"contributing/#adding-api-endpoints","title":"Adding API Endpoints","text":"<p>Extend <code>LambdaAPI</code> class in <code>lambda_api.py</code>:</p> <pre><code>def new_endpoint(self, param: str) -&gt; ResultType:\n    \"\"\"\n    Call new Lambda API endpoint.\n\n    Args:\n        param: Description\n\n    Returns:\n        Parsed response\n\n    Raises:\n        LambdaAPIError: On API failure\n    \"\"\"\n    resp = self._request_with_retry(\n        \"GET\",\n        f\"endpoint/{param}\",\n    )\n    data = resp.json()\n    return ResultType.from_api_response(data)\n</code></pre>"},{"location":"contributing/#testing-philosophy","title":"Testing Philosophy","text":"<p>We aim for high test coverage while avoiding brittle tests. See Testing for detailed guidelines.</p>"},{"location":"contributing/#key-principles","title":"Key Principles","text":"<ol> <li>Test behavior, not implementation: Tests should verify what the code does, not how it does it</li> <li>Use realistic test data: Avoid hardcoded values that would hide bugs</li> <li>Test error cases: Verify error handling and edge cases</li> <li>Keep tests isolated: Each test should be independent</li> <li>Make tests readable: Use descriptive names and clear assertions</li> </ol>"},{"location":"contributing/#documentation","title":"Documentation","text":""},{"location":"contributing/#code-comments","title":"Code Comments","text":"<p>Use comments sparingly for complex logic:</p> <pre><code># Calculate VRAM with 10% headroom for safety\nmin_gpu = total_vram * 1.1\n</code></pre> <p>Most code should be self-documenting through clear naming.</p>"},{"location":"contributing/#user-documentation","title":"User Documentation","text":"<p>User-facing documentation lives in <code>docs/</code>:</p> <ul> <li>Update <code>docs/</code> when adding features</li> <li>Include examples for new commands</li> <li>Document all CLI flags and options</li> <li>Add troubleshooting for common issues</li> </ul>"},{"location":"contributing/#common-tasks","title":"Common Tasks","text":""},{"location":"contributing/#adding-a-new-gpu-type","title":"Adding a New GPU Type","text":"<ol> <li>Add to <code>KNOWN_GPUS</code> in <code>models.py</code>:</li> </ol> <pre><code>KNOWN_GPUS = {\n    \"gpu_1x_new_gpu\": {\n        \"vram_gb\": 96,\n        \"description\": \"1x New GPU (96 GB)\"\n    },\n    ...\n}\n</code></pre> <ol> <li>Update tests in <code>test_models.py</code></li> <li>Update documentation in <code>docs/reference/gpu-types.md</code></li> </ol>"},{"location":"contributing/#adding-a-new-configuration-option","title":"Adding a New Configuration Option","text":"<ol> <li>Add to dataclass in <code>config.py</code>:</li> </ol> <pre><code>@dataclass\nclass DefaultsConfig:\n    model: str = \"deepseek-r1-70b\"\n    gpu: str = \"gpu_1x_a100_sxm4_80gb\"\n    lease_hours: int = 4\n    new_option: str = \"default_value\"  # New option\n</code></pre> <ol> <li>Update <code>configure</code> command in <code>cli.py</code></li> <li>Add validation if needed</li> <li>Update documentation</li> </ol>"},{"location":"contributing/#debugging-tips","title":"Debugging Tips","text":"<p>Enable verbose output:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre> <p>Test API calls without launching:</p> <pre><code># In cli.py\nif dry_run:\n    console.print(f\"[dim]Would launch: {gpu} in {region}[/dim]\")\n    return\n</code></pre> <p>Mock expensive operations in tests:</p> <pre><code>def test_launch(mocker):\n    mock_api = mocker.Mock()\n    mock_api.launch_instance.return_value = \"inst_test123\"\n    # Test without actual API call\n</code></pre>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Issues: Check GitHub Issues</li> <li>Discussions: Join GitHub Discussions</li> <li>Documentation: Read the full documentation</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and constructive</li> <li>Welcome newcomers and help them learn</li> <li>Focus on what is best for the project</li> <li>Show empathy towards other contributors</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing to this project, you agree that your contributions will be licensed under the same license as the project (check LICENSE file).</p>"},{"location":"contributing/development-setup/","title":"Development Setup","text":"<p>This guide walks you through setting up a local development environment for soong CLI.</p>"},{"location":"contributing/development-setup/#prerequisites","title":"Prerequisites","text":""},{"location":"contributing/development-setup/#required","title":"Required","text":"<ul> <li> <p>Python 3.10 or higher <pre><code>python3 --version  # Should be 3.10+\n</code></pre></p> </li> <li> <p>Git <pre><code>git --version\n</code></pre></p> </li> <li> <p>pip (usually comes with Python)   <pre><code>pip --version\n</code></pre></p> </li> </ul>"},{"location":"contributing/development-setup/#optional","title":"Optional","text":"<ul> <li>Lambda Labs account (for testing against real API)</li> <li>SSH key configured in Lambda Labs</li> <li>Lambda filesystem named <code>coding-stack</code></li> </ul>"},{"location":"contributing/development-setup/#initial-setup","title":"Initial Setup","text":""},{"location":"contributing/development-setup/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/yourusername/my-ai.git\ncd my-ai/cli\n</code></pre> <p>Or if you're forking:</p> <pre><code>git clone https://github.com/YOUR_USERNAME/my-ai.git\ncd my-ai/cli\ngit remote add upstream https://github.com/yourusername/my-ai.git\n</code></pre>"},{"location":"contributing/development-setup/#2-create-virtual-environment","title":"2. Create Virtual Environment","text":"<p>Using <code>venv</code> (recommended):</p> <pre><code>python3 -m venv venv\nsource venv/bin/activate  # On macOS/Linux\n</code></pre> <p>On Windows:</p> <pre><code>python -m venv venv\nvenv\\Scripts\\activate\n</code></pre> <p>On Windows PowerShell:</p> <pre><code>python -m venv venv\nvenv\\Scripts\\Activate.ps1\n</code></pre>"},{"location":"contributing/development-setup/#3-install-dependencies","title":"3. Install Dependencies","text":"<p>Install the package in editable mode with test dependencies:</p> <pre><code>pip install -e \".[test]\"\n</code></pre> <p>This installs:</p> <ul> <li>Runtime dependencies: typer, rich, requests, pyyaml, questionary</li> <li>Test dependencies: pytest, pytest-cov, pytest-mock, responses</li> </ul>"},{"location":"contributing/development-setup/#4-verify-installation","title":"4. Verify Installation","text":"<pre><code># Check that CLI is available\nsoong --help\n\n# Should show:\n# Usage: soong [OPTIONS] COMMAND [ARGS]...\n</code></pre> <pre><code># Check Python imports\npython -c \"from soong import cli, models, config; print('OK')\"\n\n# Should print: OK\n</code></pre>"},{"location":"contributing/development-setup/#5-configure-for-testing-optional","title":"5. Configure for Testing (Optional)","text":"<p>If you want to test against the real Lambda API:</p> <pre><code>soong configure\n</code></pre> <p>Enter:</p> <ul> <li>Lambda API key: Get from https://cloud.lambdalabs.com/api-keys</li> <li>Status daemon token: Any secure random string (or generate one)</li> <li>Default region: <code>us-west-1</code> or your preferred region</li> <li>Filesystem name: <code>coding-stack</code> (must exist in Lambda)</li> <li>Default model: <code>deepseek-r1-70b</code></li> <li>Default GPU: <code>gpu_1x_a100_sxm4_80gb</code></li> <li>Lease hours: <code>4</code></li> <li>SSH key path: <code>~/.ssh/id_rsa</code></li> </ul> <p>For development, you can use dummy values since tests use mocks:</p> <pre><code># ~/.config/gpu-dashboard/config.yaml\nlambda:\n  api_key: \"test_key_12345\"\n  default_region: \"us-west-1\"\n  filesystem_name: \"test-fs\"\n\nstatus_daemon:\n  token: \"test_token_67890\"\n  port: 8080\n\ndefaults:\n  model: \"deepseek-r1-70b\"\n  gpu: \"gpu_1x_a100_sxm4_80gb\"\n  lease_hours: 4\n\nssh:\n  key_path: \"~/.ssh/id_rsa\"\n</code></pre>"},{"location":"contributing/development-setup/#development-tools","title":"Development Tools","text":""},{"location":"contributing/development-setup/#code-editor-setup","title":"Code Editor Setup","text":""},{"location":"contributing/development-setup/#vs-code","title":"VS Code","text":"<p>Install recommended extensions:</p> <ul> <li>Python (ms-python.python)</li> <li>Pylance (ms-python.vscode-pylance)</li> <li>Python Test Explorer (littlefoxteam.vscode-python-test-adapter)</li> </ul> <p>Workspace settings (<code>.vscode/settings.json</code>):</p> <pre><code>{\n  \"python.defaultInterpreterPath\": \"${workspaceFolder}/venv/bin/python\",\n  \"python.testing.pytestEnabled\": true,\n  \"python.testing.unittestEnabled\": false,\n  \"python.testing.pytestArgs\": [\n    \"tests\"\n  ],\n  \"python.linting.enabled\": true,\n  \"python.linting.pylintEnabled\": false,\n  \"python.linting.flake8Enabled\": true,\n  \"python.formatting.provider\": \"black\",\n  \"editor.formatOnSave\": true,\n  \"editor.rulers\": [88]\n}\n</code></pre>"},{"location":"contributing/development-setup/#pycharm","title":"PyCharm","text":"<ol> <li>Open <code>cli/</code> directory as project</li> <li>Configure interpreter: Settings \u2192 Project \u2192 Python Interpreter \u2192 Add \u2192 Existing environment</li> <li>Select <code>cli/venv/bin/python</code></li> <li>Enable pytest: Settings \u2192 Tools \u2192 Python Integrated Tools \u2192 Testing \u2192 pytest</li> </ol>"},{"location":"contributing/development-setup/#linting-and-formatting-optional","title":"Linting and Formatting (Optional)","text":"<p>Install development tools:</p> <pre><code>pip install black flake8 mypy\n</code></pre> <p>Format code:</p> <pre><code>black src/soong tests\n</code></pre> <p>Check style:</p> <pre><code>flake8 src/soong tests --max-line-length=88 --extend-ignore=E203\n</code></pre> <p>Type checking:</p> <pre><code>mypy src/soong --ignore-missing-imports\n</code></pre>"},{"location":"contributing/development-setup/#running-the-cli-in-development-mode","title":"Running the CLI in Development Mode","text":"<p>Since you installed with <code>-e</code>, any changes to the source code are immediately reflected:</p> <pre><code># Edit src/soong/cli.py\n# Then immediately test:\nsoong --help\n</code></pre>"},{"location":"contributing/development-setup/#testing-changes","title":"Testing Changes","text":"<p>Method 1: Direct invocation</p> <pre><code>soong status\nsoong start --help\n</code></pre> <p>Method 2: Python module</p> <pre><code>python -m soong.cli status\n</code></pre> <p>Method 3: Testing individual functions</p> <pre><code># In Python REPL\nfrom soong.models import estimate_vram, Quantization\n\nresult = estimate_vram(70, Quantization.INT4, 8192)\nprint(result)\n</code></pre>"},{"location":"contributing/development-setup/#project-structure","title":"Project Structure","text":"<p>Understanding the codebase:</p> <pre><code>cli/\n\u251c\u2500\u2500 src/soong/          # Main package\n\u2502   \u251c\u2500\u2500 __init__.py           # Package initialization\n\u2502   \u251c\u2500\u2500 cli.py                # CLI commands (Typer app)\n\u2502   \u251c\u2500\u2500 config.py             # Configuration management\n\u2502   \u251c\u2500\u2500 instance.py           # Instance lifecycle\n\u2502   \u251c\u2500\u2500 lambda_api.py         # API client\n\u2502   \u251c\u2500\u2500 ssh.py                # SSH tunnels\n\u2502   \u251c\u2500\u2500 models.py             # Model definitions\n\u2502   \u2514\u2500\u2500 history.py            # Termination history\n\u2502\n\u251c\u2500\u2500 tests/                    # Test suite\n\u2502   \u251c\u2500\u2500 conftest.py           # Pytest fixtures\n\u2502   \u251c\u2500\u2500 helpers/              # Test utilities\n\u2502   \u2514\u2500\u2500 test_*.py             # Test modules\n\u2502\n\u251c\u2500\u2500 pyproject.toml            # Project metadata\n\u2514\u2500\u2500 README.md                 # Package documentation\n</code></pre>"},{"location":"contributing/development-setup/#key-files","title":"Key Files","text":"<p><code>cli.py</code> - CLI interface</p> <ul> <li>Typer commands (<code>@app.command()</code>)</li> <li>User interaction (prompts, confirmations)</li> <li>Output formatting (tables, panels)</li> </ul> <p><code>lambda_api.py</code> - Lambda Labs API</p> <ul> <li>HTTP client with retry logic</li> <li>Dataclasses for API responses (<code>Instance</code>, <code>InstanceType</code>)</li> <li>Error handling (<code>LambdaAPIError</code>)</li> </ul> <p><code>models.py</code> - Model registry</p> <ul> <li>Model configurations (<code>ModelConfig</code>)</li> <li>VRAM estimation</li> <li>GPU recommendations</li> <li>Known GPUs and models</li> </ul> <p><code>config.py</code> - Configuration</p> <ul> <li>YAML config loading/saving</li> <li>Validation for custom models</li> <li>Secure file permissions</li> </ul> <p><code>instance.py</code> - Instance management</p> <ul> <li>Waiting for instances to become ready</li> <li>Status polling</li> <li>Active instance detection</li> </ul> <p><code>ssh.py</code> - SSH tunnels</p> <ul> <li>Background SSH tunnel creation</li> <li>Port forwarding</li> <li>PID tracking for cleanup</li> </ul>"},{"location":"contributing/development-setup/#common-development-tasks","title":"Common Development Tasks","text":""},{"location":"contributing/development-setup/#running-tests","title":"Running Tests","text":"<pre><code># All tests\npytest\n\n# With coverage\npytest --cov=soong --cov-report=html\n\n# Specific test file\npytest tests/test_models.py\n\n# Specific test\npytest tests/test_models.py::test_estimate_vram_llama_70b_int4\n\n# Verbose output\npytest -v\n\n# Show print statements\npytest -s\n</code></pre>"},{"location":"contributing/development-setup/#adding-a-new-command","title":"Adding a New Command","text":"<ol> <li>Add command function in <code>cli.py</code>:</li> </ol> <pre><code>@app.command()\ndef my_command(\n    arg: str = typer.Argument(..., help=\"Required argument\"),\n    option: bool = typer.Option(False, help=\"Optional flag\"),\n):\n    \"\"\"Brief description of the command.\"\"\"\n    config = get_config()\n    # Implementation\n    console.print(\"[green]Success![/green]\")\n</code></pre> <ol> <li>Add tests in <code>tests/test_cli_commands.py</code>:</li> </ol> <pre><code>def test_my_command_success(cli_runner, sample_config, mocker):\n    \"\"\"Test my_command with valid input.\"\"\"\n    mocker.patch(\"soong.cli.get_config\", return_value=sample_config)\n    result = cli_runner.invoke(app, [\"my-command\", \"test-arg\"])\n    assert result.exit_code == 0\n    assert \"Success!\" in result.output\n</code></pre> <ol> <li>Update documentation in <code>docs/</code></li> </ol>"},{"location":"contributing/development-setup/#debugging-tests","title":"Debugging Tests","text":"<p>Run test with debugger:</p> <pre><code>pytest tests/test_models.py::test_estimate_vram_llama_70b_int4 --pdb\n</code></pre> <p>Add breakpoint in code:</p> <pre><code>def estimate_vram(params_billions, quantization):\n    import pdb; pdb.set_trace()  # Debugger will stop here\n    base = params_billions * quantization.bytes_per_param\n    ...\n</code></pre> <p>See test output:</p> <pre><code># Show print statements and logging\npytest -s --log-cli-level=DEBUG\n</code></pre>"},{"location":"contributing/development-setup/#working-with-mocks","title":"Working with Mocks","text":"<p>Tests use <code>pytest-mock</code> and <code>responses</code> for mocking:</p> <pre><code>def test_api_call(mock_http, lambda_api_base_url):\n    \"\"\"Test API call with mocked HTTP.\"\"\"\n    # Mock HTTP response\n    mock_http.add(\n        responses.GET,\n        f\"{lambda_api_base_url}/instances\",\n        json={\"data\": []},\n        status=200,\n    )\n\n    # Call code that makes request\n    api = LambdaAPI(\"test_key\")\n    instances = api.list_instances()\n\n    # Verify\n    assert len(instances) == 0\n    assert len(mock_http.calls) == 1\n</code></pre>"},{"location":"contributing/development-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"contributing/development-setup/#import-errors","title":"Import Errors","text":"<p>Problem: <code>ModuleNotFoundError: No module named 'soong'</code></p> <p>Solution:</p> <pre><code># Make sure you're in the venv\nsource venv/bin/activate\n\n# Reinstall in editable mode\npip install -e \".[test]\"\n</code></pre>"},{"location":"contributing/development-setup/#test-failures","title":"Test Failures","text":"<p>Problem: Tests fail with import errors</p> <p>Solution:</p> <pre><code># Install test dependencies\npip install -e \".[test]\"\n\n# Or explicitly\npip install pytest pytest-cov pytest-mock responses\n</code></pre>"},{"location":"contributing/development-setup/#virtual-environment-issues","title":"Virtual Environment Issues","text":"<p>Problem: Can't activate venv</p> <p>Solution:</p> <pre><code># Delete and recreate\nrm -rf venv\npython3 -m venv venv\nsource venv/bin/activate\npip install -e \".[test]\"\n</code></pre>"},{"location":"contributing/development-setup/#config-file-permission-errors","title":"Config File Permission Errors","text":"<p>Problem: <code>PermissionError</code> when running tests</p> <p>Solution:</p> <pre><code># Fix permissions on config dir\nchmod 755 ~/.config/gpu-dashboard\nchmod 600 ~/.config/gpu-dashboard/config.yaml\n</code></pre>"},{"location":"contributing/development-setup/#next-steps","title":"Next Steps","text":"<ul> <li>Read Testing Guide to learn about writing tests</li> <li>Check Contributing Guide for coding standards</li> <li>Explore the codebase and experiment with changes!</li> </ul>"},{"location":"contributing/development-setup/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check GitHub Issues</li> <li>Search Discussions</li> <li>Ask in project chat/Slack (if available)</li> <li>Create a new issue with:</li> <li>Python version (<code>python --version</code>)</li> <li>OS and version</li> <li>Steps to reproduce</li> <li>Error messages</li> </ol>"},{"location":"contributing/testing/","title":"Testing Guide","text":"<p>This guide covers testing practices, running tests, and writing new tests for the soong CLI.</p>"},{"location":"contributing/testing/#overview","title":"Overview","text":"<p>The test suite uses pytest with coverage reporting, mocking, and HTTP request interception. Our goal is high coverage (95%+) with tests that verify actual behavior, not implementation details.</p>"},{"location":"contributing/testing/#running-tests","title":"Running Tests","text":""},{"location":"contributing/testing/#basic-test-execution","title":"Basic Test Execution","text":"<pre><code># Run all tests\npytest\n\n# Verbose output\npytest -v\n\n# Run specific test file\npytest tests/test_models.py\n\n# Run specific test\npytest tests/test_models.py::test_estimate_vram_llama_70b_int4\n\n# Run tests matching pattern\npytest -k \"vram\"  # Runs all tests with \"vram\" in name\n</code></pre>"},{"location":"contributing/testing/#coverage-reporting","title":"Coverage Reporting","text":"<pre><code># Run with coverage\npytest --cov=soong\n\n# Coverage with missing lines\npytest --cov=soong --cov-report=term-missing\n\n# Generate HTML coverage report\npytest --cov=soong --cov-report=html\n# Open htmlcov/index.html in browser\n\n# Coverage for specific module\npytest --cov=soong.models tests/test_models.py\n</code></pre>"},{"location":"contributing/testing/#test-output-options","title":"Test Output Options","text":"<pre><code># Show print statements\npytest -s\n\n# Show logging output\npytest --log-cli-level=DEBUG\n\n# Fail fast (stop on first failure)\npytest -x\n\n# Run last failed tests\npytest --lf\n\n# Run failed tests first, then all\npytest --ff\n</code></pre>"},{"location":"contributing/testing/#test-organization","title":"Test Organization","text":""},{"location":"contributing/testing/#directory-structure","title":"Directory Structure","text":"<pre><code>cli/tests/\n\u251c\u2500\u2500 conftest.py                    # Shared fixtures\n\u251c\u2500\u2500 helpers/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 assertions.py              # Custom assertions\n\u251c\u2500\u2500 test_config.py                 # Configuration tests\n\u251c\u2500\u2500 test_models.py                 # Model registry tests\n\u251c\u2500\u2500 test_vram.py                   # VRAM estimation tests\n\u251c\u2500\u2500 test_gpu_recommendation.py     # GPU recommendation tests\n\u251c\u2500\u2500 test_lambda_api.py             # API client tests\n\u251c\u2500\u2500 test_instance.py               # Instance manager tests\n\u251c\u2500\u2500 test_ssh.py                    # SSH tunnel tests\n\u251c\u2500\u2500 test_history.py                # History tracking tests\n\u251c\u2500\u2500 test_cli_configure.py          # Configure command tests\n\u251c\u2500\u2500 test_cli_start.py              # Start command tests\n\u251c\u2500\u2500 test_cli_status.py             # Status command tests\n\u251c\u2500\u2500 test_cli_commands.py           # Other CLI commands\n\u251c\u2500\u2500 test_cli_models.py             # Models subcommand tests\n\u251c\u2500\u2500 test_cli_models_add.py         # Models add tests\n\u251c\u2500\u2500 test_cli_models_remove.py      # Models remove tests\n\u2514\u2500\u2500 test_cli_tunnel.py             # Tunnel subcommand tests\n</code></pre>"},{"location":"contributing/testing/#test-naming-conventions","title":"Test Naming Conventions","text":"<p>From <code>conftest.py</code>:</p> <pre><code>\"\"\"\nTest Naming Conventions:\n\n- test_&lt;function_name&gt;_&lt;scenario&gt;() for unit tests\n  Example: test_estimate_vram_llama_70b_int4()\n\n- test_&lt;command&gt;_&lt;behavior&gt;() for CLI command tests\n  Example: test_models_list_displays_all_known_models()\n\n- test_&lt;class_name&gt;_&lt;method&gt;_&lt;scenario&gt;() for class method tests\n  Example: test_model_config_from_dict_valid()\n\n- test_&lt;integration_workflow&gt;() for end-to-end tests\n  Example: test_full_workflow_configure_add_model_start()\n\nUse descriptive scenario suffixes:\n- _valid, _invalid for validation tests\n- _missing_field, _negative_params for error cases\n- _happy_path, _edge_case for behavior tests\n\"\"\"\n</code></pre>"},{"location":"contributing/testing/#writing-tests","title":"Writing Tests","text":""},{"location":"contributing/testing/#unit-tests","title":"Unit Tests","text":"<p>Test individual functions in isolation:</p> <pre><code>def test_estimate_vram_llama_70b_int4():\n    \"\"\"Test VRAM estimation for Llama 3.1 70B with INT4 quantization.\"\"\"\n    from soong.models import estimate_vram, Quantization\n\n    result = estimate_vram(\n        params_billions=70,\n        quantization=Quantization.INT4,\n        context_length=8192,\n    )\n\n    # Verify structure\n    assert \"base_vram_gb\" in result\n    assert \"kv_cache_gb\" in result\n    assert \"total_estimated_gb\" in result\n    assert \"min_vram_gb\" in result\n\n    # Verify calculations\n    # 70B * 0.5 bytes (INT4) = 35GB base\n    assert result[\"base_vram_gb\"] == 35.0\n\n    # Total should be base + KV cache + overhead + activations\n    assert result[\"total_estimated_gb\"] &gt; 35.0\n    assert result[\"total_estimated_gb\"] &lt; 50.0  # Reasonable upper bound\n\n    # Should fit on 48GB GPU\n    assert result[\"min_vram_gb\"] in [40, 48, 80]\n</code></pre>"},{"location":"contributing/testing/#cli-command-tests","title":"CLI Command Tests","text":"<p>Test CLI commands using <code>CliRunner</code>:</p> <pre><code>def test_start_command_shows_cost_estimate(cli_runner, sample_config, mocker):\n    \"\"\"Test that start command shows cost estimate before launching.\"\"\"\n    # Setup mocks\n    mocker.patch(\"soong.cli.get_config\", return_value=sample_config)\n\n    mock_api = mocker.Mock()\n    mock_api.get_instance_type.return_value = mocker.Mock(\n        description=\"1x A100 SXM4 (80 GB)\",\n        price_per_hour=1.29,\n        estimate_cost=lambda h: 1.29 * h,\n    )\n    mocker.patch(\"soong.cli.LambdaAPI\", return_value=mock_api)\n\n    # Mock user declining\n    mocker.patch(\"questionary.confirm\", return_value=mocker.Mock(ask=lambda: False))\n\n    # Run command\n    result = cli_runner.invoke(app, [\"start\"])\n\n    # Verify cost estimate shown\n    assert \"Cost Estimate\" in result.output\n    assert \"$1.29/hr\" in result.output\n    assert \"4 hours\" in result.output  # Default lease\n\n    # Verify launch cancelled\n    assert \"cancelled\" in result.output.lower()\n    assert result.exit_code == 0\n</code></pre>"},{"location":"contributing/testing/#api-client-tests","title":"API Client Tests","text":"<p>Test API calls with <code>responses</code> library (not mocking <code>requests</code> directly):</p> <pre><code>def test_list_instances_success(mock_http, lambda_api_base_url):\n    \"\"\"Test listing instances with successful API response.\"\"\"\n    # Setup mock HTTP response\n    mock_http.add(\n        responses.GET,\n        f\"{lambda_api_base_url}/instances\",\n        json={\n            \"data\": [\n                {\n                    \"id\": \"inst_abc123\",\n                    \"name\": \"test-instance\",\n                    \"ip\": \"1.2.3.4\",\n                    \"status\": \"active\",\n                    \"instance_type\": {\"name\": \"gpu_1x_a100_sxm4_80gb\"},\n                    \"region\": {\"name\": \"us-west-1\"},\n                    \"created_at\": \"2025-01-01T12:00:00Z\",\n                }\n            ]\n        },\n        status=200,\n    )\n\n    # Call API\n    from soong.lambda_api import LambdaAPI\n    api = LambdaAPI(\"test_key\")\n    instances = api.list_instances()\n\n    # Verify results\n    assert len(instances) == 1\n    assert instances[0].id == \"inst_abc123\"\n    assert instances[0].status == \"active\"\n\n    # Verify HTTP call made correctly\n    assert len(mock_http.calls) == 1\n    assert mock_http.calls[0].request.headers[\"Authorization\"] == \"Bearer test_key\"\n</code></pre>"},{"location":"contributing/testing/#testing-error-handling","title":"Testing Error Handling","text":"<pre><code>def test_launch_instance_api_error(mock_http, lambda_api_base_url):\n    \"\"\"Test launch_instance handles API errors gracefully.\"\"\"\n    # Mock API error response\n    mock_http.add(\n        responses.POST,\n        f\"{lambda_api_base_url}/instance-operations/launch\",\n        json={\"error\": \"Insufficient quota\"},\n        status=403,\n    )\n\n    # Call API and expect error\n    from soong.lambda_api import LambdaAPI, LambdaAPIError\n    api = LambdaAPI(\"test_key\")\n\n    with pytest.raises(LambdaAPIError, match=\"API request failed\"):\n        api.launch_instance(\n            region=\"us-west-1\",\n            instance_type=\"gpu_1x_a100_sxm4_80gb\",\n            ssh_key_names=[\"my-key\"],\n        )\n</code></pre>"},{"location":"contributing/testing/#test-fixtures","title":"Test Fixtures","text":""},{"location":"contributing/testing/#common-fixtures-from-conftestpy","title":"Common Fixtures (from <code>conftest.py</code>)","text":"<p>Model configurations:</p> <pre><code>def test_using_sample_model(sample_model_config):\n    \"\"\"Use the sample 70B model fixture.\"\"\"\n    assert sample_model_config.params_billions == 70.0\n    assert sample_model_config.default_quantization == Quantization.INT4\n\ndef test_using_small_model(small_model_config):\n    \"\"\"Use the sample 7B model fixture.\"\"\"\n    assert small_model_config.params_billions == 7.0\n</code></pre> <p>Configuration:</p> <pre><code>def test_using_config(sample_config):\n    \"\"\"Use the sample configuration fixture.\"\"\"\n    assert sample_config.lambda_config.api_key == \"test_key_12345\"\n    assert sample_config.defaults.lease_hours == 4\n</code></pre> <p>HTTP mocking:</p> <pre><code>def test_api_call(mock_http, lambda_api_base_url):\n    \"\"\"Use HTTP mocking fixture.\"\"\"\n    mock_http.add(\n        responses.GET,\n        f\"{lambda_api_base_url}/endpoint\",\n        json={\"status\": \"ok\"},\n    )\n    # Make request and verify\n</code></pre> <p>CLI runner:</p> <pre><code>def test_command(cli_runner):\n    \"\"\"Use CLI runner fixture.\"\"\"\n    from soong.cli import app\n    result = cli_runner.invoke(app, [\"command\", \"args\"])\n    assert result.exit_code == 0\n</code></pre>"},{"location":"contributing/testing/#creating-custom-fixtures","title":"Creating Custom Fixtures","text":"<pre><code># In test file or conftest.py\nimport pytest\n\n@pytest.fixture\ndef mock_instance_ready(mocker):\n    \"\"\"Mock an instance that is ready.\"\"\"\n    instance = mocker.Mock()\n    instance.id = \"inst_ready123\"\n    instance.status = \"active\"\n    instance.ip = \"5.6.7.8\"\n    return instance\n\ndef test_with_custom_fixture(mock_instance_ready):\n    \"\"\"Use custom fixture.\"\"\"\n    assert mock_instance_ready.status == \"active\"\n</code></pre>"},{"location":"contributing/testing/#mocking-best-practices","title":"Mocking Best Practices","text":""},{"location":"contributing/testing/#mock-external-services-not-internal-logic","title":"Mock External Services, Not Internal Logic","text":"<p>Good:</p> <pre><code>def test_instance_launch(mocker):\n    \"\"\"Mock the Lambda API, not internal logic.\"\"\"\n    mock_api = mocker.Mock()\n    mock_api.launch_instance.return_value = \"inst_123\"\n\n    manager = InstanceManager(mock_api)\n    result = manager.launch_with_config(...)\n\n    # Verify API was called correctly\n    mock_api.launch_instance.assert_called_once_with(\n        region=\"us-west-1\",\n        instance_type=\"gpu_1x_a100_sxm4_80gb\",\n    )\n</code></pre> <p>Bad:</p> <pre><code>def test_instance_launch_bad(mocker):\n    \"\"\"Don't mock internal helper functions.\"\"\"\n    mocker.patch(\"soong.instance._validate_config\")  # Internal detail\n    # This test is brittle and doesn't verify behavior\n</code></pre>"},{"location":"contributing/testing/#use-responses-for-http-not-mockerpatchrequests","title":"Use <code>responses</code> for HTTP, Not <code>mocker.patch(\"requests\")</code>","text":"<p>Good:</p> <pre><code>def test_http_request(mock_http):\n    \"\"\"Mock HTTP responses with responses library.\"\"\"\n    mock_http.add(responses.GET, \"https://api.example.com\", json={\"ok\": True})\n\n    result = requests.get(\"https://api.example.com\")\n    assert result.json()[\"ok\"] is True\n</code></pre> <p>Bad:</p> <pre><code>def test_http_request_bad(mocker):\n    \"\"\"Don't mock requests directly.\"\"\"\n    mocker.patch(\"requests.get\", return_value=mocker.Mock(json=lambda: {\"ok\": True}))\n    # Doesn't test actual HTTP logic\n</code></pre>"},{"location":"contributing/testing/#use-realistic-test-data","title":"Use Realistic Test Data","text":"<p>Good:</p> <pre><code>def test_instance_parsing(mock_http):\n    \"\"\"Use realistic instance data from Lambda API.\"\"\"\n    mock_http.add(\n        responses.GET,\n        url,\n        json={\n            \"data\": [\n                {\n                    \"id\": \"inst_unique_789xyz\",  # Unique ID\n                    \"name\": \"my-test-instance\",\n                    \"ip\": \"192.168.99.42\",  # Unique IP\n                    \"status\": \"active\",\n                    # ... full realistic data\n                }\n            ]\n        },\n    )\n</code></pre> <p>Bad:</p> <pre><code>def test_instance_parsing_bad(mock_http):\n    \"\"\"Don't use minimal data that hides bugs.\"\"\"\n    mock_http.add(responses.GET, url, json={\"data\": [{\"id\": \"123\"}]})\n    # Missing fields would cause AttributeError in real usage\n</code></pre>"},{"location":"contributing/testing/#coverage-goals","title":"Coverage Goals","text":""},{"location":"contributing/testing/#current-coverage","title":"Current Coverage","text":"<p>As of recent updates, the test suite achieves:</p> <ul> <li>Overall coverage: 95%+</li> <li>Core modules: 95-100%</li> <li>CLI commands: 90-95%</li> <li>API client: 95%+</li> </ul>"},{"location":"contributing/testing/#measuring-coverage","title":"Measuring Coverage","text":"<pre><code># Generate coverage report\npytest --cov=soong --cov-report=term-missing\n\n# See which lines are not covered\npytest --cov=soong --cov-report=html\nopen htmlcov/index.html\n</code></pre>"},{"location":"contributing/testing/#improving-coverage","title":"Improving Coverage","text":"<p>Find untested code:</p> <pre><code>pytest --cov=soong --cov-report=term-missing | grep -A 5 \"TOTAL\"\n</code></pre> <p>Add tests for uncovered lines:</p> <pre><code># Example: Testing error path\ndef test_config_validation_invalid_quantization():\n    \"\"\"Test that invalid quantization raises ValueError.\"\"\"\n    from soong.config import validate_custom_model\n\n    invalid_model = {\n        \"hf_path\": \"org/model\",\n        \"params_billions\": 7,\n        \"quantization\": \"invalid\",  # Invalid value\n        \"context_length\": 8192,\n    }\n\n    with pytest.raises(ValueError, match=\"Invalid quantization\"):\n        validate_custom_model(invalid_model)\n</code></pre>"},{"location":"contributing/testing/#continuous-integration","title":"Continuous Integration","text":"<p>Tests run automatically on:</p> <ul> <li>Every push to feature branches</li> <li>Every pull request</li> <li>Merges to main branch</li> </ul>"},{"location":"contributing/testing/#ci-configuration","title":"CI Configuration","text":"<p>GitHub Actions workflow (<code>.github/workflows/test.yml</code>):</p> <pre><code>name: Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [\"3.10\", \"3.11\", \"3.12\"]\n\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Install dependencies\n        run: |\n          cd cli\n          pip install -e \".[test]\"\n\n      - name: Run tests with coverage\n        run: |\n          cd cli\n          pytest --cov=soong --cov-report=xml\n\n      - name: Upload coverage\n        uses: codecov/codecov-action@v3\n        with:\n          file: ./cli/coverage.xml\n</code></pre>"},{"location":"contributing/testing/#debugging-tests","title":"Debugging Tests","text":""},{"location":"contributing/testing/#using-pytest-pdb","title":"Using <code>pytest --pdb</code>","text":"<pre><code># Drop into debugger on failure\npytest --pdb tests/test_models.py\n\n# Drop into debugger on first failure\npytest -x --pdb\n</code></pre>"},{"location":"contributing/testing/#adding-breakpoints","title":"Adding Breakpoints","text":"<pre><code>def test_complex_logic():\n    \"\"\"Test with debugging.\"\"\"\n    result = complex_function()\n\n    import pdb; pdb.set_trace()  # Debugger stops here\n\n    assert result == expected\n</code></pre>"},{"location":"contributing/testing/#viewing-test-output","title":"Viewing Test Output","text":"<pre><code># Show all print statements\npytest -s\n\n# Show logging\npytest --log-cli-level=DEBUG\n\n# Capture turned off (shows all output)\npytest --capture=no\n</code></pre>"},{"location":"contributing/testing/#test-performance","title":"Test Performance","text":""},{"location":"contributing/testing/#running-tests-quickly","title":"Running Tests Quickly","text":"<pre><code># Run tests in parallel (requires pytest-xdist)\npip install pytest-xdist\npytest -n auto\n\n# Run only fast tests (mark slow tests with @pytest.mark.slow)\npytest -m \"not slow\"\n\n# Run only changed tests\npytest --testmon\n</code></pre>"},{"location":"contributing/testing/#marking-slow-tests","title":"Marking Slow Tests","text":"<pre><code>import pytest\n\n@pytest.mark.slow\ndef test_full_integration_workflow():\n    \"\"\"This test takes &gt;5 seconds.\"\"\"\n    # Slow integration test\n</code></pre> <p>Then skip slow tests:</p> <pre><code>pytest -m \"not slow\"\n</code></pre>"},{"location":"contributing/testing/#testing-best-practices","title":"Testing Best Practices","text":""},{"location":"contributing/testing/#1-test-behavior-not-implementation","title":"1. Test Behavior, Not Implementation","text":"<p>Good:</p> <pre><code>def test_launch_instance_creates_active_instance():\n    \"\"\"Verify instance becomes active after launch.\"\"\"\n    # Test the outcome\n    assert instance.status == \"active\"\n</code></pre> <p>Bad:</p> <pre><code>def test_launch_instance_calls_api_exactly_once():\n    \"\"\"Don't test implementation details.\"\"\"\n    # Too coupled to implementation\n    mock_api.launch_instance.assert_called_once()\n</code></pre>"},{"location":"contributing/testing/#2-use-descriptive-test-names","title":"2. Use Descriptive Test Names","text":"<p>Good:</p> <pre><code>def test_estimate_vram_70b_int4_fits_on_a100_80gb():\n    \"\"\"Clear what is being tested and expected outcome.\"\"\"\n</code></pre> <p>Bad:</p> <pre><code>def test_vram():\n    \"\"\"Vague and unhelpful.\"\"\"\n</code></pre>"},{"location":"contributing/testing/#3-arrange-act-assert-pattern","title":"3. Arrange-Act-Assert Pattern","text":"<pre><code>def test_something():\n    # Arrange: Set up test data\n    model = ModelConfig(...)\n\n    # Act: Execute the code being tested\n    result = model.estimated_vram_gb\n\n    # Assert: Verify the outcome\n    assert result &gt; 0\n    assert result &lt; 100\n</code></pre>"},{"location":"contributing/testing/#4-one-logical-assertion-per-test","title":"4. One Logical Assertion Per Test","text":"<p>Good:</p> <pre><code>def test_vram_estimate_includes_base():\n    \"\"\"Test that base VRAM is calculated.\"\"\"\n    assert result[\"base_vram_gb\"] == 35.0\n\ndef test_vram_estimate_includes_overhead():\n    \"\"\"Test that overhead is included.\"\"\"\n    assert result[\"total_estimated_gb\"] &gt; result[\"base_vram_gb\"]\n</code></pre> <p>Bad:</p> <pre><code>def test_vram_estimate():\n    \"\"\"Test everything at once.\"\"\"\n    assert result[\"base_vram_gb\"] == 35.0\n    assert result[\"kv_cache_gb\"] &gt; 0\n    assert result[\"overhead_gb\"] == 2.0\n    assert result[\"total_estimated_gb\"] &gt; 35.0\n    # Hard to debug when one assertion fails\n</code></pre>"},{"location":"contributing/testing/#5-test-edge-cases","title":"5. Test Edge Cases","text":"<pre><code>def test_estimate_vram_zero_params():\n    \"\"\"Test VRAM estimation with zero parameters.\"\"\"\n    result = estimate_vram(0, Quantization.FP16, 8192)\n    assert result[\"base_vram_gb\"] == 0\n\ndef test_estimate_vram_huge_params():\n    \"\"\"Test VRAM estimation with very large model.\"\"\"\n    result = estimate_vram(1000, Quantization.FP32, 32768)\n    assert result[\"min_vram_gb\"] == 160  # Multi-GPU\n</code></pre>"},{"location":"contributing/testing/#next-steps","title":"Next Steps","text":"<ul> <li>Read Development Setup to set up your environment</li> <li>Explore existing tests in <code>cli/tests/</code></li> <li>Run tests and experiment with changes</li> <li>Write tests for new features before implementing them (TDD)</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome to Soong CLI! This guide will walk you through everything you need to launch and manage Lambda Labs GPU instances.</p>"},{"location":"getting-started/#overview","title":"Overview","text":"<p>Getting started with Soong CLI involves four simple steps:</p> <ol> <li>Prerequisites: Set up your Lambda Labs account and gather required credentials</li> <li>Installation: Install the CLI tool on your local machine</li> <li>Configuration: Configure the CLI with your Lambda Labs credentials</li> <li>Quick Start: Launch your first GPU instance</li> </ol>"},{"location":"getting-started/#your-journey","title":"Your Journey","text":"<ul> <li> <p> Prerequisites</p> <p>Set up your Lambda Labs account, create API keys, and configure SSH access.</p> <p>Time: 10-15 minutes</p> </li> <li> <p> Installation</p> <p>Install Soong CLI and verify your installation.</p> <p>Time: 2-3 minutes</p> </li> <li> <p> Configuration</p> <p>Run the configuration wizard and set up your preferences.</p> <p>Time: 3-5 minutes</p> </li> <li> <p> Quick Start</p> <p>Launch your first GPU instance and connect via SSH.</p> <p>Time: 5 minutes</p> </li> </ul>"},{"location":"getting-started/#what-youll-need","title":"What You'll Need","text":"<p>Before you begin, make sure you have:</p> <ul> <li> A Lambda Labs account (sign up here)</li> <li> A Lambda Labs API key</li> <li> An SSH key pair uploaded to Lambda Labs</li> <li> Python 3.10 or later installed on your machine</li> <li> Basic familiarity with the command line</li> </ul>"},{"location":"getting-started/#estimated-time","title":"Estimated Time","text":"<p>Total Setup Time: 20-30 minutes</p> <p>Once configured, launching instances takes less than a minute.</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Start with the Prerequisites Guide to set up your Lambda Labs account.</p> <p>Already have Lambda Labs configured? Skip to Installation.</p> <p>Need Help?</p> <p>If you encounter any issues during setup, review the Prerequisites section carefully or check the FAQ in the Quick Start guide.</p>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>After installing Soong CLI, you'll need to configure it with your Lambda Labs credentials and preferences.</p>"},{"location":"getting-started/configuration/#running-the-configuration-wizard","title":"Running the Configuration Wizard","text":"<p>Soong CLI includes an interactive configuration wizard that guides you through setup:</p> <pre><code>soong configure\n</code></pre> <p>The wizard will prompt you for the following information:</p> <ol> <li>Lambda Labs API key</li> <li>SSH private key path</li> <li>Persistent filesystem name (optional)</li> <li>Default instance type</li> <li>Default region</li> <li>Default maximum runtime hours</li> </ol>"},{"location":"getting-started/configuration/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"getting-started/configuration/#lambda-labs-api-key","title":"Lambda Labs API Key","text":"<p>Prompt: <code>Enter your Lambda Labs API key:</code></p> <p>Description: Your Lambda Labs API key for authentication.</p> <p>Example: <code>lam_1234567890abcdefghijklmnopqrstuvwxyz</code></p> <p>Where to find it: cloud.lambdalabs.com \u2192 Account Settings \u2192 API Keys</p> <p>Security Warning</p> <p>Your API key will be stored in <code>~/.config/gpu-dashboard/config.yaml</code> with restricted permissions (0600). Never share this file or commit it to version control.</p>"},{"location":"getting-started/configuration/#ssh-private-key-path","title":"SSH Private Key Path","text":"<p>Prompt: <code>Enter path to your SSH private key:</code></p> <p>Description: Path to your SSH private key file (the key pair you uploaded to Lambda Labs).</p> <p>Default: <code>~/.ssh/id_ed25519</code></p> <p>Example: <code>/home/user/.ssh/id_ed25519</code></p> <p>Validation: The wizard verifies that: - The file exists - The file has correct permissions (0600) - The file is a valid private key</p> <p>SSH Key Permissions</p> <p>SSH requires private keys to have restrictive permissions. The wizard will warn you if permissions are too open and offer to fix them automatically.</p>"},{"location":"getting-started/configuration/#persistent-filesystem-name","title":"Persistent Filesystem Name","text":"<p>Prompt: <code>Enter persistent filesystem name (optional):</code></p> <p>Description: Name of your Lambda Labs persistent filesystem. If provided, this filesystem will be automatically attached to launched instances.</p> <p>Example: <code>ml-workspace</code></p> <p>Optional: Press Enter to skip if you don't have a persistent filesystem.</p> <p>Where to find it: cloud.lambdalabs.com \u2192 Storage \u2192 Filesystems</p>"},{"location":"getting-started/configuration/#default-instance-type","title":"Default Instance Type","text":"<p>Prompt: <code>Enter default instance type (e.g., gpu_1x_a10):</code></p> <p>Description: The GPU instance type to use by default when launching instances.</p> <p>Default: <code>gpu_1x_a10</code></p> <p>Common Options: - <code>gpu_1x_a10</code> - Single A10 GPU (24GB VRAM) - <code>gpu_1x_a100</code> - Single A100 GPU (40GB VRAM) - <code>gpu_1x_h100_pcie</code> - Single H100 GPU (80GB VRAM)</p> <p>See all options: Run <code>soong available</code> after configuration</p> <p>Choosing an Instance Type</p> <p>Start with <code>gpu_1x_a10</code> for smaller models. You can override this when launching instances with the <code>--instance-type</code> flag.</p>"},{"location":"getting-started/configuration/#default-region","title":"Default Region","text":"<p>Prompt: <code>Enter default region (e.g., us-west-1):</code></p> <p>Description: The Lambda Labs region where instances will be launched by default.</p> <p>Default: <code>us-west-1</code></p> <p>Common Options: - <code>us-west-1</code> - US West (California) - <code>us-east-1</code> - US East (Virginia) - <code>us-south-1</code> - US South (Texas) - <code>europe-central-1</code> - Europe (Germany)</p> <p>Filesystem Region</p> <p>If using a persistent filesystem, make sure to select the same region where your filesystem is located.</p>"},{"location":"getting-started/configuration/#default-max-hours","title":"Default Max Hours","text":"<p>Prompt: <code>Enter default maximum runtime hours (0 for unlimited):</code></p> <p>Description: Maximum number of hours an instance can run before automatically stopping. This helps prevent unexpected costs.</p> <p>Default: <code>2</code></p> <p>Example: <code>4</code> (instance stops after 4 hours)</p> <p>Unlimited: Enter <code>0</code> for no time limit</p> <p>Cost Control</p> <p>Setting a default max hours helps prevent forgetting to stop instances. You can always extend the runtime with <code>soong extend</code>.</p>"},{"location":"getting-started/configuration/#configuration-file-location","title":"Configuration File Location","text":"<p>Your configuration is stored in:</p> <pre><code>~/.config/gpu-dashboard/config.yaml\n</code></pre> <p>Permissions: <code>0600</code> (read/write for owner only)</p>"},{"location":"getting-started/configuration/#example-configuration-file","title":"Example Configuration File","text":"<pre><code>lambda_api_key: lam_1234567890abcdefghijklmnopqrstuvwxyz\nssh_private_key_path: /home/user/.ssh/id_ed25519\npersistent_filesystem_name: ml-workspace\ndefault_instance_type: gpu_1x_a10\ndefault_region: us-west-1\ndefault_max_hours: 2\n</code></pre>"},{"location":"getting-started/configuration/#reconfiguring","title":"Reconfiguring","text":"<p>To update your configuration, run the wizard again:</p> <pre><code>soong configure\n</code></pre> <p>The wizard will display your current values and allow you to change them.</p>"},{"location":"getting-started/configuration/#manual-configuration","title":"Manual Configuration","text":"<p>Advanced users can edit the configuration file directly:</p> <pre><code># Open configuration in your default editor\nnano ~/.config/gpu-dashboard/config.yaml\n\n# Or use your preferred editor\nvim ~/.config/gpu-dashboard/config.yaml\ncode ~/.config/gpu-dashboard/config.yaml\n</code></pre> <p>Manual Editing</p> <p>If you edit the configuration manually, make sure the file permissions remain <code>0600</code>: <pre><code>chmod 0600 ~/.config/gpu-dashboard/config.yaml\n</code></pre></p>"},{"location":"getting-started/configuration/#verifying-configuration","title":"Verifying Configuration","text":"<p>After configuration, verify your settings:</p> <pre><code># Check that you can list available instance types\nsoong available\n\n# Check that you can list models\nsoong models --limit 5\n</code></pre> <p>Expected Output: <pre><code>Available instance types:\n- gpu_1x_a10 (24GB VRAM) - $0.60/hr\n- gpu_1x_a100 (40GB VRAM) - $1.10/hr\n...\n\nAvailable models:\n- meta-llama/Llama-2-7b-hf\n- mistralai/Mistral-7B-v0.1\n...\n</code></pre></p> <p>Configuration Complete</p> <p>If you see instance types and models, your configuration is working correctly!</p>"},{"location":"getting-started/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/configuration/#invalid-api-key","title":"\"Invalid API key\"","text":"<p>Cause: The API key is incorrect or has been revoked.</p> <p>Solution: 1. Go to cloud.lambdalabs.com \u2192 Account Settings \u2192 API Keys 2. Generate a new API key 3. Run <code>soong configure</code> and enter the new key</p>"},{"location":"getting-started/configuration/#ssh-key-not-found","title":"\"SSH key not found\"","text":"<p>Cause: The specified SSH key path doesn't exist.</p> <p>Solution: 1. Verify the path is correct: <code>ls -la ~/.ssh/</code> 2. If the key doesn't exist, generate one: <code>ssh-keygen -t ed25519</code> 3. Upload the public key to Lambda Labs 4. Run <code>soong configure</code> with the correct path</p>"},{"location":"getting-started/configuration/#permission-denied-when-accessing-config-file","title":"\"Permission denied\" when accessing config file","text":"<p>Cause: The configuration file has incorrect permissions.</p> <p>Solution: <pre><code># Fix permissions\nchmod 0600 ~/.config/gpu-dashboard/config.yaml\n</code></pre></p>"},{"location":"getting-started/configuration/#filesystem-not-found","title":"\"Filesystem not found\"","text":"<p>Cause: The specified filesystem name doesn't exist or is in a different region.</p> <p>Solution: 1. Check your filesystems: cloud.lambdalabs.com \u2192 Storage \u2192 Filesystems 2. Verify the name matches exactly 3. If using a filesystem, make sure your default region matches the filesystem region 4. Run <code>soong configure</code> to update the settings</p>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<p>Now that Soong CLI is configured, proceed to the Quick Start Guide to launch your first GPU instance!</p> <p>Configuration Best Practices</p> <ul> <li>Use a short max hours default (2-4 hours) to prevent unexpected costs</li> <li>Store your API key securely\u2014treat it like a password</li> <li>Use a persistent filesystem for any data you want to keep across sessions</li> <li>Choose a region close to you for better SSH latency</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide covers installing Soong CLI on your local machine.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"<p>Before installing, verify your system meets these requirements:</p> Requirement Minimum Version Python 3.10 or later pip 21.0 or later Operating System Linux, macOS, or Windows (WSL)"},{"location":"getting-started/installation/#check-python-version","title":"Check Python Version","text":"<p>Verify you have Python 3.10 or later installed:</p> <pre><code>python --version\n# or\npython3 --version\n</code></pre> <p>Expected Output: <pre><code>Python 3.10.0  # or later\n</code></pre></p> <p>Python Version</p> <p>Soong CLI requires Python 3.10 or later. If you have an older version, download the latest Python before proceeding.</p>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#option-1-install-from-source-recommended","title":"Option 1: Install from Source (Recommended)","text":"<p>Clone the repository and install in development mode:</p> <pre><code># Clone the repository\ngit clone https://github.com/yourusername/soong.git\ncd soong\n\n# Create a virtual environment (recommended)\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install in editable mode with all dependencies\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#option-2-install-from-pypi-coming-soon","title":"Option 2: Install from PyPI (Coming Soon)","text":"<p>Once published to PyPI, you'll be able to install with:</p> <pre><code>pip install soong\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>After installation, verify that Soong CLI is working:</p> <pre><code>soong --version\n</code></pre> <p>Expected Output: <pre><code>soong version 1.0.0\n</code></pre></p>"},{"location":"getting-started/installation/#run-the-help-command","title":"Run the Help Command","text":"<pre><code>soong --help\n</code></pre> <p>Expected Output: <pre><code>Usage: soong [OPTIONS] COMMAND [ARGS]...\n\n  Soong CLI - Manage Lambda Labs GPU instances\n\nOptions:\n  --version  Show the version and exit.\n  --help     Show this message and exit.\n\nCommands:\n  configure  Configure Soong CLI with Lambda Labs credentials\n  start      Start a new GPU instance\n  status     Show current instance status\n  extend     Extend instance runtime\n  stop       Stop the current instance\n  ssh        SSH into the running instance\n  tunnel     Create SSH tunnels to instance ports\n  available  List available instance types\n  models     List and search available models\n</code></pre></p> <p>Installation Complete</p> <p>If you see the help output, Soong CLI is installed correctly!</p>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#command-not-found-soong","title":"\"command not found: soong\"","text":"<p>Cause: The installation directory isn't in your PATH.</p> <p>Solution:</p> Linux/macOSWindows (PowerShell) <pre><code># If installed with --user flag\nexport PATH=\"$HOME/.local/bin:$PATH\"\n\n# Add to your shell profile (~/.bashrc or ~/.zshrc)\necho 'export PATH=\"$HOME/.local/bin:$PATH\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <pre><code># Check if Scripts directory is in PATH\n$env:Path\n\n# If not, add it to your user PATH through System Properties\n# or use a virtual environment\n</code></pre>"},{"location":"getting-started/installation/#pip-command-not-found","title":"\"pip: command not found\"","text":"<p>Solution: Install pip for your Python version:</p> <pre><code># Linux/macOS\npython3 -m ensurepip --upgrade\n\n# Or use your package manager\nsudo apt install python3-pip  # Debian/Ubuntu\nbrew install python3  # macOS with Homebrew\n</code></pre>"},{"location":"getting-started/installation/#installation-fails-with-permission-errors","title":"Installation fails with permission errors","text":"<p>Solution: Use a virtual environment or install with the <code>--user</code> flag:</p> <pre><code># Option 1: Virtual environment (recommended)\npython -m venv venv\nsource venv/bin/activate\npip install -e .\n\n# Option 2: User installation\npip install --user -e .\n</code></pre>"},{"location":"getting-started/installation/#dependencies-fail-to-install","title":"Dependencies fail to install","text":"<p>Solution: Make sure you have build tools installed:</p> Debian/UbuntumacOSWindows <pre><code>sudo apt update\nsudo apt install python3-dev build-essential\n</code></pre> <pre><code>xcode-select --install\n</code></pre> <p>Install Microsoft C++ Build Tools</p>"},{"location":"getting-started/installation/#uninstalling","title":"Uninstalling","text":"<p>To uninstall Soong CLI:</p> <pre><code>pip uninstall soong\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that Soong CLI is installed, proceed to Configuration to set up your Lambda Labs credentials.</p> <p>Using Virtual Environments</p> <p>We strongly recommend using Python virtual environments to avoid dependency conflicts. Learn more in the Python venv documentation.</p>"},{"location":"getting-started/prerequisites/","title":"Prerequisites","text":"<p>Before installing Soong CLI, you'll need to set up your Lambda Labs account and gather the required credentials.</p>"},{"location":"getting-started/prerequisites/#1-create-a-lambda-labs-account","title":"1. Create a Lambda Labs Account","text":"<p>If you don't already have a Lambda Labs account:</p> <ol> <li>Go to cloud.lambdalabs.com</li> <li>Click Sign Up and create an account</li> <li>Verify your email address</li> <li>Add a payment method to your account</li> </ol> <p>Payment Method Required</p> <p>Lambda Labs requires a valid payment method before you can launch GPU instances. Make sure to add your payment information in the account settings.</p>"},{"location":"getting-started/prerequisites/#2-generate-an-api-key","title":"2. Generate an API Key","text":"<p>Soong CLI uses the Lambda Labs API to manage instances. You'll need an API key to authenticate.</p>"},{"location":"getting-started/prerequisites/#steps-to-generate-an-api-key","title":"Steps to Generate an API Key","text":"<ol> <li>Log in to cloud.lambdalabs.com</li> <li>Navigate to Account Settings \u2192 API Keys</li> <li>Click Generate New API Key</li> <li>Copy the API key immediately\u2014you won't be able to see it again</li> <li>Store the API key securely (you'll need it during configuration)</li> </ol> <p>Keep Your API Key Secret</p> <p>Your API key provides full access to your Lambda Labs account. Never share it or commit it to version control.</p>"},{"location":"getting-started/prerequisites/#example-api-key-format","title":"Example API Key Format","text":"<pre><code>lam_1234567890abcdefghijklmnopqrstuvwxyz\n</code></pre>"},{"location":"getting-started/prerequisites/#3-create-and-upload-ssh-keys","title":"3. Create and Upload SSH Keys","text":"<p>Soong CLI uses SSH to connect to your instances. You'll need to upload your public SSH key to Lambda Labs.</p>"},{"location":"getting-started/prerequisites/#generate-an-ssh-key-pair-if-you-dont-have-one","title":"Generate an SSH Key Pair (if you don't have one)","text":"Linux/macOSWindows (PowerShell) <pre><code># Generate a new SSH key pair\nssh-keygen -t ed25519 -C \"your_email@example.com\"\n\n# Press Enter to save to the default location (~/.ssh/id_ed25519)\n# Optionally set a passphrase for extra security\n\n# Display your public key\ncat ~/.ssh/id_ed25519.pub\n</code></pre> <pre><code># Generate a new SSH key pair\nssh-keygen -t ed25519 -C \"your_email@example.com\"\n\n# Press Enter to save to the default location\n# Optionally set a passphrase for extra security\n\n# Display your public key\nGet-Content ~\\.ssh\\id_ed25519.pub\n</code></pre>"},{"location":"getting-started/prerequisites/#upload-ssh-key-to-lambda-labs","title":"Upload SSH Key to Lambda Labs","text":"<ol> <li>Log in to cloud.lambdalabs.com</li> <li>Navigate to Account Settings \u2192 SSH Keys</li> <li>Click Add SSH Key</li> <li>Paste your public key (the content of <code>id_ed25519.pub</code>)</li> <li>Give it a memorable name (e.g., \"My Laptop\")</li> <li>Click Save</li> </ol> <p>Public Key Only</p> <p>Upload your public key (<code>id_ed25519.pub</code>), NOT your private key (<code>id_ed25519</code>). Never share your private key with anyone.</p>"},{"location":"getting-started/prerequisites/#4-create-a-persistent-filesystem-recommended","title":"4. Create a Persistent Filesystem (Recommended)","text":"<p>Lambda Labs instances are ephemeral\u2014when stopped, all data is lost. A persistent filesystem preserves your data across instances.</p>"},{"location":"getting-started/prerequisites/#why-use-a-persistent-filesystem","title":"Why Use a Persistent Filesystem?","text":"<ul> <li>Preserve models: Downloaded models persist across sessions</li> <li>Save work: Code, datasets, and results aren't lost</li> <li>Faster startup: No need to re-download dependencies</li> </ul>"},{"location":"getting-started/prerequisites/#create-a-persistent-filesystem","title":"Create a Persistent Filesystem","text":"<ol> <li>Log in to cloud.lambdalabs.com</li> <li>Navigate to Storage \u2192 Filesystems</li> <li>Click Create Filesystem</li> <li>Choose a region (must match your instance region)</li> <li>Set the size (recommended: 512GB or larger for ML models)</li> <li>Name your filesystem (e.g., \"ml-workspace\")</li> <li>Click Create</li> </ol> <p>Filesystem Naming</p> <p>Use a descriptive name\u2014you'll reference it during Soong CLI configuration.</p>"},{"location":"getting-started/prerequisites/#filesystem-costs","title":"Filesystem Costs","text":"<p>Persistent filesystems are charged separately from instances:</p> <ul> <li>Cost: ~$0.20/GB/month</li> <li>Example: 512GB filesystem = ~$100/month</li> </ul>"},{"location":"getting-started/prerequisites/#5-verify-prerequisites","title":"5. Verify Prerequisites","text":"<p>Before proceeding, make sure you have:</p> <ul> <li> Lambda Labs account with verified email</li> <li> Payment method added to your account</li> <li> Lambda Labs API key generated and saved</li> <li> SSH key pair generated</li> <li> Public SSH key uploaded to Lambda Labs</li> <li> Persistent filesystem created (optional but recommended)</li> </ul>"},{"location":"getting-started/prerequisites/#next-steps","title":"Next Steps","text":"<p>Once you've completed all prerequisites, proceed to Installation to install Soong CLI.</p>"},{"location":"getting-started/prerequisites/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/prerequisites/#unable-to-generate-api-key","title":"\"Unable to generate API key\"","text":"<p>Solution: Make sure you've verified your email address and added a payment method to your account.</p>"},{"location":"getting-started/prerequisites/#ssh-key-upload-failed","title":"\"SSH key upload failed\"","text":"<p>Solution: Verify you're uploading your public key (<code>id_ed25519.pub</code>), not your private key. The public key should start with <code>ssh-ed25519</code> or <code>ssh-rsa</code>.</p>"},{"location":"getting-started/prerequisites/#filesystem-creation-unavailable-in-my-region","title":"\"Filesystem creation unavailable in my region\"","text":"<p>Solution: Persistent filesystems are only available in certain Lambda Labs regions. Choose a region that supports filesystems, then launch instances in that same region.</p> <p>Lambda Labs Documentation</p> <p>For more details on Lambda Labs features, see the official Lambda Labs documentation.</p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>This 5-minute tutorial will walk you through launching your first GPU instance, connecting via SSH, and stopping the instance.</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial, make sure you've completed:</p> <ul> <li> Prerequisites - Lambda Labs account setup</li> <li> Installation - Soong CLI installed</li> <li> Configuration - CLI configured with credentials</li> </ul>"},{"location":"getting-started/quick-start/#step-1-browse-available-models","title":"Step 1: Browse Available Models","text":"<p>First, let's see what models are available:</p> <pre><code>soong models --limit 10\n</code></pre> <p>Expected Output: <pre><code>Available models (showing 10 of 150):\n\nName                                    VRAM Required\n--------------------------------------  --------------\nmeta-llama/Llama-2-7b-hf               14 GB\nmistralai/Mistral-7B-v0.1              14 GB\nmeta-llama/Llama-2-13b-hf              26 GB\ndeepseek-ai/DeepSeek-R1                160 GB\nmeta-llama/Meta-Llama-3-70B            140 GB\n...\n</code></pre></p> <p>Model Recommendations</p> <p>Use <code>soong models --recommend &lt;model-name&gt;</code> to get instance type recommendations based on VRAM requirements.</p> <p>Example: <pre><code>soong models --recommend meta-llama/Llama-2-7b-hf\n</code></pre></p> <p>Output: <pre><code>Model: meta-llama/Llama-2-7b-hf\nEstimated VRAM: 14 GB\n\nRecommended instance types:\n\u2713 gpu_1x_a10 (24GB VRAM) - $0.60/hr\n\u2713 gpu_1x_a100 (40GB VRAM) - $1.10/hr\n\u2713 gpu_1x_h100_pcie (80GB VRAM) - $2.99/hr\n</code></pre></p>"},{"location":"getting-started/quick-start/#step-2-start-a-gpu-instance","title":"Step 2: Start a GPU Instance","text":"<p>Launch a GPU instance with your chosen model:</p> <pre><code>soong start --model meta-llama/Llama-2-7b-hf\n</code></pre> <p>What happens:</p> <ol> <li>Soong CLI requests an instance from Lambda Labs</li> <li>Lambda Labs provisions the GPU instance</li> <li>The instance boots and becomes accessible</li> <li>SSH tunnels are automatically configured</li> <li>Your persistent filesystem is mounted (if configured)</li> </ol> <p>Expected Output: <pre><code>Starting GPU instance...\nInstance type: gpu_1x_a10 (using default)\nRegion: us-west-1 (using default)\nMax runtime: 2 hours (using default)\n\n\u2713 Instance started successfully!\n\nInstance Details:\n  ID: i-abc123def456\n  Type: gpu_1x_a10\n  Status: running\n  IP: 203.0.113.42\n  Cost: $0.60/hour\n\nSSH Tunnels:\n  Local Port 8000 \u2192 Instance Port 8000 (SGLang)\n  Local Port 5678 \u2192 Instance Port 5678 (n8n)\n  Local Port 8080 \u2192 Instance Port 8080 (Status Daemon)\n\nTo connect: soong ssh\nTo check status: soong status\nTo stop: soong stop\n</code></pre></p> <p>Instance Startup Time</p> <p>Instances typically take 30-60 seconds to become fully ready. You'll see a \"running\" status once it's accessible.</p>"},{"location":"getting-started/quick-start/#customize-instance-launch","title":"Customize Instance Launch","text":"<p>You can override defaults with command-line flags:</p> <pre><code># Specify instance type\nsoong start --model meta-llama/Llama-2-7b-hf --instance-type gpu_1x_a100\n\n# Set max runtime to 4 hours\nsoong start --model meta-llama/Llama-2-7b-hf --max-hours 4\n\n# Combine multiple options\nsoong start \\\n  --model deepseek-ai/DeepSeek-R1 \\\n  --instance-type gpu_1x_h100_pcie \\\n  --max-hours 6 \\\n  --region us-east-1\n</code></pre>"},{"location":"getting-started/quick-start/#step-3-check-instance-status","title":"Step 3: Check Instance Status","text":"<p>View your running instance details:</p> <pre><code>soong status\n</code></pre> <p>Expected Output: <pre><code>Instance Status:\n  ID: i-abc123def456\n  Type: gpu_1x_a10\n  Status: running\n  Model: meta-llama/Llama-2-7b-hf\n\nRuntime:\n  Uptime: 5 minutes\n  Max runtime: 2 hours\n  Time remaining: 1 hour 55 minutes\n\nCost:\n  Hourly rate: $0.60/hr\n  Current cost: $0.05\n  Estimated total (at 2hr): $1.20\n\nNetwork:\n  IP: 203.0.113.42\n  SSH tunnels: Active\n\nPorts:\n  8000 (SGLang): Available at localhost:8000\n  5678 (n8n): Available at localhost:5678\n  8080 (Status): Available at localhost:8080\n</code></pre></p>"},{"location":"getting-started/quick-start/#step-4-connect-via-ssh","title":"Step 4: Connect via SSH","text":"<p>SSH into your running instance:</p> <pre><code>soong ssh\n</code></pre> <p>What happens: 1. Soong CLI connects using your configured SSH key 2. You're logged into the instance as the default user 3. Your persistent filesystem is mounted at <code>/home/ubuntu/workspace</code> (if configured)</p> <p>Expected Output: <pre><code>Connecting to instance i-abc123def456 (203.0.113.42)...\n\nWelcome to Ubuntu 22.04.3 LTS (GNU/Linux 5.15.0-1048-aws x86_64)\n\n * Documentation:  https://help.ubuntu.com\n * Management:     https://landscape.canonical.com\n * Support:        https://ubuntu.com/advantage\n\nubuntu@lambda-instance:~$\n</code></pre></p>"},{"location":"getting-started/quick-start/#verify-gpu-access","title":"Verify GPU Access","text":"<p>Once connected, verify GPU access:</p> <pre><code>nvidia-smi\n</code></pre> <p>Expected Output: <pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 535.54.03    Driver Version: 535.54.03    CUDA Version: 12.2   |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  NVIDIA A10          Off  | 00000000:00:1E.0 Off |                    0 |\n|  0%   32C    P0    52W / 150W |      0MiB / 24576MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n</code></pre></p>"},{"location":"getting-started/quick-start/#exit-ssh-session","title":"Exit SSH Session","text":"<p>To return to your local machine:</p> <pre><code>exit\n</code></pre>"},{"location":"getting-started/quick-start/#step-5-access-services-via-ssh-tunnels","title":"Step 5: Access Services via SSH Tunnels","text":"<p>Soong CLI automatically sets up SSH tunnels to common ports. You can access these services from your local machine:</p>"},{"location":"getting-started/quick-start/#sglang-inference-server-port-8000","title":"SGLang Inference Server (Port 8000)","text":"<pre><code># From your local machine\ncurl http://localhost:8000/health\n</code></pre>"},{"location":"getting-started/quick-start/#n8n-workflow-automation-port-5678","title":"n8n Workflow Automation (Port 5678)","text":"<p>Open in your browser: <pre><code>http://localhost:5678\n</code></pre></p>"},{"location":"getting-started/quick-start/#instance-status-daemon-port-8080","title":"Instance Status Daemon (Port 8080)","text":"<pre><code>curl http://localhost:8080/status\n</code></pre> <p>Tunnel Management</p> <p>SSH tunnels are automatically created when you start an instance. Use <code>soong tunnel</code> to manually manage tunnels if needed.</p>"},{"location":"getting-started/quick-start/#step-6-extend-runtime-optional","title":"Step 6: Extend Runtime (Optional)","text":"<p>If you need more time before the instance auto-stops:</p> <pre><code>soong extend --hours 2\n</code></pre> <p>Expected Output: <pre><code>Extended instance runtime by 2 hours.\n\nNew Details:\n  Current uptime: 45 minutes\n  New max runtime: 4 hours\n  Time remaining: 3 hours 15 minutes\n  New estimated cost (at 4hr): $2.40\n</code></pre></p>"},{"location":"getting-started/quick-start/#step-7-stop-the-instance","title":"Step 7: Stop the Instance","text":"<p>When you're done, stop the instance to avoid additional charges:</p> <pre><code>soong stop\n</code></pre> <p>Expected Output: <pre><code>Stopping instance i-abc123def456...\n\n\u2713 Instance stopped successfully!\n\nFinal Summary:\n  Total runtime: 1 hour 23 minutes\n  Total cost: $0.83\n\nInstance data has been saved to your persistent filesystem.\n</code></pre></p> <p>Data Loss Warning</p> <p>If you're NOT using a persistent filesystem, all data on the instance will be lost when stopped. Make sure to copy any important files before stopping.</p>"},{"location":"getting-started/quick-start/#complete-example-workflow","title":"Complete Example Workflow","text":"<p>Here's a complete workflow from start to finish:</p> <pre><code># 1. Check available models\nsoong models --limit 5\n\n# 2. Get recommendations for a specific model\nsoong models --recommend meta-llama/Llama-2-7b-hf\n\n# 3. Start instance with the model\nsoong start --model meta-llama/Llama-2-7b-hf\n\n# 4. Check status\nsoong status\n\n# 5. SSH into instance\nsoong ssh\n\n# (Inside instance) Verify GPU\nnvidia-smi\n\n# (Inside instance) Do your work...\npython train_model.py\n\n# (Inside instance) Exit SSH\nexit\n\n# 6. (Optional) Extend runtime if needed\nsoong extend --hours 1\n\n# 7. Stop instance when done\nsoong stop\n</code></pre>"},{"location":"getting-started/quick-start/#whats-next","title":"What's Next?","text":"<p>Now that you've launched your first instance, explore more features:</p> <ul> <li>Command Reference: Detailed documentation for all commands</li> <li>Configuration Reference: Advanced configuration options</li> <li>Model Management: Working with different models</li> <li>Cost Optimization: Tips for reducing GPU costs</li> </ul>"},{"location":"getting-started/quick-start/#common-questions","title":"Common Questions","text":""},{"location":"getting-started/quick-start/#how-much-does-this-cost","title":"How much does this cost?","text":"<p>Costs depend on the instance type and runtime:</p> Instance Type VRAM Cost/Hour 2hr Session gpu_1x_a10 24GB $0.60 $1.20 gpu_1x_a100 40GB $1.10 $2.20 gpu_1x_h100_pcie 80GB $2.99 $5.98 <p>See <code>soong available</code> for current pricing.</p>"},{"location":"getting-started/quick-start/#what-happens-if-i-forget-to-stop-an-instance","title":"What happens if I forget to stop an instance?","text":"<p>If you set <code>--max-hours</code>, the instance automatically stops when the time limit is reached. Without a time limit, the instance runs indefinitely until manually stopped.</p> <p>Set a Max Runtime</p> <p>Always use <code>--max-hours</code> to prevent unexpected costs from forgotten instances.</p>"},{"location":"getting-started/quick-start/#can-i-resume-a-stopped-instance","title":"Can I resume a stopped instance?","text":"<p>No, stopped instances cannot be resumed. You'll need to start a new instance. However, if you use a persistent filesystem, your data is preserved.</p>"},{"location":"getting-started/quick-start/#how-do-i-save-my-work-between-sessions","title":"How do I save my work between sessions?","text":"<p>Use a persistent filesystem (configured during setup). Any data stored in <code>/home/ubuntu/workspace</code> persists across instances.</p> <p>Congratulations!</p> <p>You've successfully launched and managed your first GPU instance with Soong CLI!</p>"},{"location":"guides/","title":"User Guides","text":"<p>Comprehensive guides for common workflows and tasks with soong CLI.</p>"},{"location":"guides/#getting-started","title":"Getting Started","text":"<p>If you haven't configured soong yet, start with the Quick Start guide.</p>"},{"location":"guides/#available-guides","title":"Available Guides","text":""},{"location":"guides/#core-workflows","title":"Core Workflows","text":"<ul> <li>Launching Instances - Start GPU instances with the right configuration</li> <li>Managing Leases - Monitor and extend your instance leases</li> <li>SSH Tunneling - Access remote services securely through SSH tunnels</li> </ul>"},{"location":"guides/#configuration-and-optimization","title":"Configuration and Optimization","text":"<ul> <li>Model Management - Choose, configure, and add custom models</li> <li>Cost Optimization - Strategies to minimize GPU costs</li> </ul>"},{"location":"guides/#quick-navigation","title":"Quick Navigation","text":"<p>I want to...</p> <ul> <li>Launch a new instance \u2192 Launching Instances</li> <li>Check how much time I have left \u2192 Managing Leases</li> <li>Access web services on my instance \u2192 SSH Tunneling</li> <li>Add a custom model \u2192 Model Management</li> <li>Save money on GPU costs \u2192 Cost Optimization</li> </ul>"},{"location":"guides/#common-use-cases","title":"Common Use Cases","text":""},{"location":"guides/#development-workflow","title":"Development Workflow","text":"<pre><code>graph LR\n    A[Start Instance] --&gt; B[SSH Tunnel]\n    B --&gt; C[Develop/Test]\n    C --&gt; D{Need More Time?}\n    D --&gt;|Yes| E[Extend Lease]\n    D --&gt;|No| F[Stop Instance]\n    E --&gt; C</code></pre> <ol> <li>Launch an instance with your preferred model and GPU</li> <li>Start SSH tunnel to access services</li> <li>Develop and test your application</li> <li>Monitor lease time periodically</li> <li>Extend if needed or stop when done</li> </ol>"},{"location":"guides/#model-selection-strategy","title":"Model Selection Strategy","text":"<pre><code>graph TD\n    A[Need GPU?] --&gt;|Complex Reasoning| B[DeepSeek-R1 70B]\n    A --&gt;|Fast Coding| C[Qwen2.5-Coder 32B]\n    A --&gt;|Budget| D[Qwen2.5-Coder 32B INT4]\n    A --&gt;|General Purpose| E[Llama 3.1 70B]\n    A --&gt;|Quick Tasks| F[Llama 3.1 8B]\n\n    B --&gt; G[A100 80GB]\n    C --&gt; H[A6000 48GB]\n    D --&gt; I[A10 24GB]\n    E --&gt; G\n    F --&gt; I</code></pre> <p>See Model Management for detailed model comparisons.</p>"},{"location":"guides/#cost-control-features","title":"Cost Control Features","text":"<p>soong includes several safety mechanisms to prevent runaway costs:</p> <ol> <li>Lease System - All instances have a maximum 8-hour lease</li> <li>Idle Detection - Automatic shutdown after 30 minutes of inactivity</li> <li>Cost Confirmations - Preview costs before launching or extending</li> <li>Lease Extensions - Explicit action required to keep instances running</li> </ol> <p>Learn more in Cost Optimization.</p>"},{"location":"guides/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and solutions:</p> <ul> <li>SSH tunnel won't start: Check SSH Tunneling Guide</li> <li>Instance won't launch: See Launching Instances</li> <li>Lease expired unexpectedly: Review Managing Leases</li> </ul>"},{"location":"guides/#next-steps","title":"Next Steps","text":"<ul> <li>Review the CLI Reference for all available commands</li> <li>Read about Architecture to understand how it works</li> </ul>"},{"location":"guides/cost-optimization/","title":"Cost Optimization","text":"<p>Strategies to minimize GPU costs while maintaining productivity.</p>"},{"location":"guides/cost-optimization/#overview","title":"Overview","text":"<p>GPU instances can be expensive (\\(0.60-\\)2.00+/hour). This guide shows how to optimize costs without sacrificing development quality.</p>"},{"location":"guides/cost-optimization/#cost-structure","title":"Cost Structure","text":"<p>Understanding the pricing model:</p> <pre><code>Total Cost = GPU Rate \u00d7 Uptime Hours\n</code></pre> <p>Key variables:</p> <ul> <li>GPU Rate: Determined by GPU type ($0.60/hr for A10, $1.29/hr for A100)</li> <li>Uptime Hours: How long the instance runs</li> </ul> <p>Optimization targets:</p> <ol> <li>Choose the cheapest GPU that fits your model</li> <li>Minimize unnecessary uptime</li> </ol>"},{"location":"guides/cost-optimization/#gpu-selection-strategies","title":"GPU Selection Strategies","text":""},{"location":"guides/cost-optimization/#rule-use-the-smallest-gpu-that-fits","title":"Rule: Use the Smallest GPU That Fits","text":"<pre><code>graph TD\n    A[Model needs 26 GB] --&gt; B{GPU Options}\n    B --&gt; C[A10: 24 GB \u274c]\n    B --&gt; D[A6000: 48 GB \u2705]\n    B --&gt; E[A100: 80 GB \u2705]\n\n    D --&gt; F[$0.80/hr - BEST]\n    E --&gt; G[$1.29/hr - Overpaying]\n\n    style F fill:#90EE90\n    style G fill:#FFB6C1</code></pre> <p>Example: Qwen2.5-Coder 32B INT4</p> <ul> <li>Needs: 26 GB VRAM</li> <li>\u274c A100 80GB ($1.29/hr): Overpaying by $0.49/hr</li> <li>\u2705 A10 24GB (too small)</li> <li>\u2705 A6000 48GB ($0.80/hr): Best choice</li> </ul> <p>Savings: \\(0.49/hr \u00d7 4 hours = **\\)1.96 saved per session**</p>"},{"location":"guides/cost-optimization/#use-int4-when-quality-allows","title":"Use INT4 When Quality Allows","text":"<p>Quantization reduces VRAM requirements, enabling cheaper GPUs:</p> Model Quantization VRAM GPU Cost/hr Quality Loss Qwen2.5-Coder 32B FP16 46 GB A6000 $0.80 0% (baseline) Qwen2.5-Coder 32B INT4 26 GB A10 $0.60 ~5% <p>Savings: \\(0.20/hr \u00d7 4 hours = **\\)0.80 saved per session**</p> <p>When to use INT4:</p> <ul> <li>\u2705 Code generation and completion</li> <li>\u2705 Simple refactoring</li> <li>\u2705 Prototyping</li> <li>\u274c Production code review</li> <li>\u274c Complex reasoning tasks</li> </ul>"},{"location":"guides/cost-optimization/#model-selection-by-budget","title":"Model Selection by Budget","text":"<pre><code>graph LR\n    A[Budget?] --&gt; B{Tier}\n\n    B --&gt;|Low $0.60/hr| C[Llama 3.1 8B]\n    B --&gt;|Medium $0.80/hr| D[Qwen2.5-Coder 32B]\n    B --&gt;|High $1.29/hr| E[DeepSeek-R1 70B]\n\n    C --&gt; F[A10 24GB]\n    D --&gt; G[A6000 48GB]\n    E --&gt; H[A100 80GB]</code></pre> <p>Cost comparison (4-hour session):</p> Tier Model GPU Session Cost Budget Llama 3.1 8B A10 $2.40 Medium Qwen2.5-Coder 32B INT4 A10 $2.40 Medium+ Qwen2.5-Coder 32B A6000 $3.20 Premium DeepSeek-R1 70B A100 $5.16 <p>Strategy: Start with medium tier (Qwen INT4), escalate to premium only when needed.</p>"},{"location":"guides/cost-optimization/#lease-management-strategies","title":"Lease Management Strategies","text":""},{"location":"guides/cost-optimization/#use-shorter-initial-leases","title":"Use Shorter Initial Leases","text":"<p>Start with 2 hours, extend as needed:</p> <pre><code># Start short\nsoong start --hours 2  # $1.60\n\n# Extend if needed (only if still working)\nsoong extend 2  # +$1.60 total $3.20\n</code></pre> <p>vs. starting with 4 hours:</p> <pre><code># Start long\nsoong start --hours 4  # $3.20\n\n# Finish early, waste time\n# (Stop after 2.5 hours, wasted $1.20)\n</code></pre> <p>Savings: Avoid paying for unused time.</p>"},{"location":"guides/cost-optimization/#monitor-lease-time","title":"Monitor Lease Time","text":"<p>Set reminders to check status:</p> <pre><code># Every hour\nwatch -n 3600 soong status\n</code></pre> <p>Or add to shell prompt:</p> <pre><code># Add to ~/.bashrc or ~/.zshrc\ngpu_status() {\n  soong status 2&gt;/dev/null | grep \"Time Left\" | awk '{print $NF}'\n}\n\nPS1='[GPU: $(gpu_status)] $ '\n</code></pre>"},{"location":"guides/cost-optimization/#extend-before-expiration","title":"Extend Before Expiration","text":"<p>Avoid unexpected termination:</p> <pre><code>Time Left: 45m  # Yellow warning\n</code></pre> <p>Action: Extend now, not at 5 minutes:</p> <pre><code>soong extend 2\n</code></pre>"},{"location":"guides/cost-optimization/#development-workflow-optimization","title":"Development Workflow Optimization","text":""},{"location":"guides/cost-optimization/#batch-work-sessions","title":"Batch Work Sessions","text":"<p>Instead of multiple small sessions, batch work:</p> <pre><code># \u274c Inefficient: 3 separate sessions\nSession 1: 1 hour ($0.80)\nSession 2: 1.5 hours ($1.20)\nSession 3: 0.5 hours ($0.40)\nTotal: 3 hours, 3 launches = $2.40\n\n# \u2705 Efficient: 1 combined session\nSession 1: 3 hours ($2.40)\nTotal: 3 hours, 1 launch = $2.40\n</code></pre> <p>Savings: Avoid instance startup overhead (5-10 min per launch).</p>"},{"location":"guides/cost-optimization/#use-local-development-first","title":"Use Local Development First","text":"<p>Develop and test locally, use GPU only for model-specific work:</p> <pre><code>graph LR\n    A[Write Code Locally] --&gt; B[Test Locally]\n    B --&gt; C{Need Model?}\n    C --&gt;|No| A\n    C --&gt;|Yes| D[Launch GPU]\n    D --&gt; E[Run Inference]\n    E --&gt; F[Stop Instance]</code></pre> <p>Example workflow:</p> <ol> <li>Write code on laptop (free)</li> <li>Write tests on laptop (free)</li> <li>Launch GPU instance for inference testing</li> <li>Stop instance immediately after</li> </ol> <p>Savings: Minimize GPU uptime to only essential tasks.</p>"},{"location":"guides/cost-optimization/#stop-instances-between-sessions","title":"Stop Instances Between Sessions","text":"<p>Don't leave instances running overnight or during breaks:</p> <pre><code># Before lunch\nsoong stop --yes\n\n# After lunch\nsoong start\n</code></pre> <p>Savings: \\(0.80/hr \u00d7 1 hour lunch = **\\)0.80 saved**</p>"},{"location":"guides/cost-optimization/#idle-detection-configuration","title":"Idle Detection Configuration","text":"<p>The status daemon automatically stops instances after 30 minutes of inactivity.</p>"},{"location":"guides/cost-optimization/#how-it-works","title":"How It Works","text":"<p>The daemon monitors:</p> <ul> <li>CPU usage</li> <li>GPU utilization</li> <li>Network activity</li> <li>Active SSH sessions</li> </ul> <p>Idle = all metrics below threshold for 30 minutes</p>"},{"location":"guides/cost-optimization/#benefits","title":"Benefits","text":"<ul> <li>Prevents forgetting to stop instances</li> <li>No manual monitoring required</li> <li>Automatic cost control</li> </ul>"},{"location":"guides/cost-optimization/#limitations","title":"Limitations","text":"<p>Not idle if:</p> <ul> <li>Long-running training jobs</li> <li>Background processes</li> <li>Active SSH tunnel (even if not using)</li> </ul> <p>Solution: Use lease limits (8 hours max) as hard stop.</p>"},{"location":"guides/cost-optimization/#cost-estimation","title":"Cost Estimation","text":""},{"location":"guides/cost-optimization/#before-launch","title":"Before Launch","text":"<p>Always review cost estimates:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Launch Instance                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                     \u2502\n\u2502 Cost Estimate                       \u2502\n\u2502                                     \u2502\n\u2502 GPU: 1x A6000 (48 GB)              \u2502\n\u2502 Rate: $0.80/hour                   \u2502\n\u2502 Duration: 4 hours                  \u2502\n\u2502                                     \u2502\n\u2502 Estimated cost: $3.20              \u2502\n\u2502                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Skip if confident:</p> <pre><code>soong start --yes\n</code></pre>"},{"location":"guides/cost-optimization/#during-session","title":"During Session","text":"<p>Monitor current and estimated total costs:</p> <pre><code>soong status\n</code></pre> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ID   \u2502 Uptime \u2502 Time Left \u2502 Cost Now \u2502 Est. Total         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 a1b2 \u2502 2h 30m \u2502 1h 30m    \u2502 $2.00    \u2502 $3.20              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ul> <li>Cost Now: Amount spent so far</li> <li>Est. Total: Projected cost if lease runs to completion</li> </ul>"},{"location":"guides/cost-optimization/#cost-comparison-scenarios","title":"Cost Comparison Scenarios","text":""},{"location":"guides/cost-optimization/#scenario-1-simple-code-changes","title":"Scenario 1: Simple Code Changes","text":"<p>Task: Fix a bug, add a feature</p> <p>Options:</p> Approach Model GPU Duration Cost Premium DeepSeek-R1 70B A100 1 hr $1.29 Medium Qwen2.5-Coder 32B A6000 1 hr $0.80 Budget Llama 3.1 8B A10 1 hr $0.60 <p>Recommendation: Budget (Llama 3.1 8B) Savings: $0.69 vs premium</p>"},{"location":"guides/cost-optimization/#scenario-2-large-codebase-refactoring","title":"Scenario 2: Large Codebase Refactoring","text":"<p>Task: Refactor multiple files, need long context</p> <p>Options:</p> Approach Model GPU Duration Cost High Quality Qwen2.5-Coder 32B (FP16) A6000 4 hr $3.20 Budget Qwen2.5-Coder 32B (INT4) A10 4 hr $2.40 <p>Recommendation: Budget (INT4) Savings: $0.80 (25% cheaper) Trade-off: ~5% quality loss (acceptable for refactoring)</p>"},{"location":"guides/cost-optimization/#scenario-3-complex-debugging","title":"Scenario 3: Complex Debugging","text":"<p>Task: Debug a difficult, multi-step issue</p> <p>Options:</p> Approach Model GPU Duration Cost Premium DeepSeek-R1 70B A100 3 hr $3.87 Medium Qwen2.5-Coder 32B A6000 4 hr $3.20 <p>Recommendation: Premium (DeepSeek-R1) Why: Better reasoning saves time (3 hrs vs 4 hrs) Actual savings: $3.87 vs $3.20 (spend $0.67 more but save 1 hour)</p>"},{"location":"guides/cost-optimization/#monthly-budget-planning","title":"Monthly Budget Planning","text":""},{"location":"guides/cost-optimization/#estimate-monthly-usage","title":"Estimate Monthly Usage","text":"<pre><code>Monthly Cost = Sessions/week \u00d7 Weeks \u00d7 Avg Cost/session\n</code></pre> <p>Example:</p> <ul> <li>3 sessions/week</li> <li>4 weeks/month</li> <li>$3.00/session average</li> </ul> <pre><code>Monthly Cost = 3 \u00d7 4 \u00d7 $3.00 = $36/month\n</code></pre>"},{"location":"guides/cost-optimization/#usage-patterns","title":"Usage Patterns","text":"Usage Level Sessions/Week Hours/Week Est. Monthly Cost Light 2-3 4-6 $20-40 Medium 5-7 10-14 $50-80 Heavy 10+ 20+ $100-200"},{"location":"guides/cost-optimization/#budget-allocation","title":"Budget Allocation","text":"<p>Strategy: Allocate 60/30/10</p> <ul> <li>60% - Budget tier (Llama 8B, Qwen INT4)</li> <li>30% - Medium tier (Qwen FP16)</li> <li>10% - Premium tier (DeepSeek-R1)</li> </ul> <p>Example ($80/month budget):</p> <ul> <li>$48 - Budget tier (20 sessions \u00d7 $2.40)</li> <li>$24 - Medium tier (7 sessions \u00d7 $3.20)</li> <li>$8 - Premium tier (2 sessions \u00d7 $4.00)</li> </ul> <p>Result: 29 sessions/month, average $2.76/session</p>"},{"location":"guides/cost-optimization/#advanced-strategies","title":"Advanced Strategies","text":""},{"location":"guides/cost-optimization/#use-persistent-filesystem","title":"Use Persistent Filesystem","text":"<p>Store work on persistent filesystem (<code>/home/ubuntu/workspace</code>) to avoid redoing work:</p> <pre><code># Lost work = wasted money\n\u274c /tmp/project  # Lost on termination\n\n# Persistent work\n\u2705 /home/ubuntu/workspace/project  # Survives termination\n</code></pre> <p>Benefit: Resume work in next session without starting over.</p>"},{"location":"guides/cost-optimization/#parallel-development","title":"Parallel Development","text":"<p>Run multiple instances for parallel tasks:</p> <pre><code># Terminal 1\nsoong start --name \"frontend\" --model llama-3.1-8b --hours 2\n\n# Terminal 2\nsoong start --name \"backend\" --model llama-3.1-8b --hours 2\n</code></pre> <p>Cost: $2.40 total (2 \u00d7 2 hours \u00d7 $0.60) Time savings: 2 hours \u2192 1 hour (50% faster) Effective rate: $2.40/hr for 2x productivity</p>"},{"location":"guides/cost-optimization/#automate-stop","title":"Automate Stop","text":"<p>Add a cron job or script to auto-stop instances:</p> <pre><code># Stop all instances at 6 PM\n0 18 * * * soong stop --yes\n</code></pre> <p>Benefit: Never forget to stop instances.</p>"},{"location":"guides/cost-optimization/#benchmarking-costs","title":"Benchmarking Costs","text":"<p>Track your actual costs over time:</p> <pre><code># View termination history\nsoong status --history --history-hours 168  # Last week\n</code></pre> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            Termination History (Last 168 Hours)          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Time   \u2502 Inst ID \u2502 Reason         \u2502 Uptime \u2502 GPU        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Jan 01 \u2502 a1b2c3  \u2502 Lease expired  \u2502 4h 5m  \u2502 a6000      \u2502\n\u2502 Dec 31 \u2502 x9y8z7  \u2502 Manual stop    \u2502 2h 12m \u2502 a10        \u2502\n\u2502 Dec 30 \u2502 m5n6o7  \u2502 Idle timeout   \u2502 1h 30m \u2502 a100       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nTotal uptime: 7h 47m\nEstimated cost: ~$5.50\n</code></pre> <p>Analysis: If over budget, adjust strategy:</p> <ul> <li>Use cheaper GPUs</li> <li>Shorter sessions</li> <li>More aggressive idle timeout</li> </ul>"},{"location":"guides/cost-optimization/#cost-avoidance-checklist","title":"Cost Avoidance Checklist","text":"<p>Before every launch:</p> <ul> <li> Do I need a GPU for this task?</li> <li> Can I use a smaller model?</li> <li> Can I use INT4 instead of FP16?</li> <li> Can I use a cheaper GPU?</li> <li> How long will this actually take? (start with shorter lease)</li> <li> Will I use all the time? (better to extend than pre-pay)</li> </ul> <p>During session:</p> <ul> <li> Am I actually using the instance?</li> <li> Can I finish in the current lease?</li> <li> Should I stop now and resume later?</li> </ul> <p>After session:</p> <ul> <li> Did I stop the instance?</li> <li> Is work saved to persistent filesystem?</li> <li> Did I get value for money spent?</li> </ul>"},{"location":"guides/cost-optimization/#best-practices-summary","title":"Best Practices Summary","text":"<ol> <li>GPU Selection: Always use the cheapest GPU that fits your model</li> <li>Quantization: Use INT4 when 5% quality loss is acceptable</li> <li>Lease Duration: Start short (2 hours), extend as needed</li> <li>Monitoring: Check status regularly, extend before expiration</li> <li>Development: Develop locally, use GPU only for model tasks</li> <li>Idle Detection: Leverage automatic shutdown for forgotten instances</li> <li>Budgeting: Track usage, allocate 60/30/10 across tiers</li> <li>Automation: Script stops, set reminders, use persistent storage</li> </ol>"},{"location":"guides/cost-optimization/#next-steps","title":"Next Steps","text":"<ul> <li>Choose the right model for your task</li> <li>Launch with optimal settings</li> <li>Monitor and extend leases efficiently</li> </ul>"},{"location":"guides/launching-instances/","title":"Launching Instances","text":"<p>Launch Lambda Labs GPU instances with the optimal configuration for your workload.</p>"},{"location":"guides/launching-instances/#quick-start","title":"Quick Start","text":"<p>Launch with default settings:</p> <pre><code>soong start\n</code></pre> <p>This uses your configured defaults for model, GPU type, region, and lease duration.</p>"},{"location":"guides/launching-instances/#customizing-launch-parameters","title":"Customizing Launch Parameters","text":""},{"location":"guides/launching-instances/#selecting-a-model","title":"Selecting a Model","text":"<p>Override the default model with <code>--model</code>:</p> <pre><code>soong start --model deepseek-r1-70b\n</code></pre> <p>Available models:</p> Model Parameters Use Case Min GPU <code>deepseek-r1-70b</code> 70B (INT4) Complex reasoning A100 80GB <code>qwen2.5-coder-32b</code> 32B (FP16) Fast coding, long context A6000 48GB <code>qwen2.5-coder-32b-int4</code> 32B (INT4) Budget coding A10 24GB <code>llama-3.1-70b</code> 70B (INT4) General purpose A100 80GB <code>llama-3.1-8b</code> 8B (FP16) Simple tasks A10 24GB <p>See Model Management for detailed comparisons.</p>"},{"location":"guides/launching-instances/#choosing-a-gpu-type","title":"Choosing a GPU Type","text":"<p>Override the default GPU with <code>--gpu</code>:</p> <pre><code>soong start --model qwen2.5-coder-32b --gpu gpu_1x_a6000\n</code></pre> <p>GPU Selection</p> <p>The configure wizard recommends the cheapest GPU that fits your model. You can always select a larger GPU, but it will cost more.</p> <p>Common GPU types:</p> <pre><code># Budget\ngpu_1x_a10          # 24GB VRAM - cheapest\ngpu_1x_rtx6000      # 48GB VRAM - mid-range\n\n# High Performance\ngpu_1x_a100_sxm4_80gb  # 80GB VRAM - large models\ngpu_1x_h100_pcie       # 80GB VRAM - fastest\n</code></pre>"},{"location":"guides/launching-instances/#selecting-a-region","title":"Selecting a Region","text":"<p>Override the default region with <code>--region</code>:</p> <pre><code>soong start --region us-west-2\n</code></pre> <p>Available regions depend on GPU availability. Common regions:</p> <ul> <li><code>us-west-1</code> (Northern California)</li> <li><code>us-west-2</code> (Oregon)</li> <li><code>us-east-1</code> (Virginia)</li> <li><code>us-south-1</code> (Texas)</li> </ul> <p>Region Availability</p> <p>GPU availability varies by region. Use <code>soong available</code> to see current capacity.</p>"},{"location":"guides/launching-instances/#setting-lease-duration","title":"Setting Lease Duration","text":"<p>Set lease duration (1-8 hours) with <code>--hours</code>:</p> <pre><code>soong start --hours 6\n</code></pre> <p>Lease Limits</p> <p>Maximum lease duration is 8 hours. After that, you must explicitly extend the lease or the instance will auto-terminate.</p>"},{"location":"guides/launching-instances/#naming-your-instance","title":"Naming Your Instance","text":"<p>Assign a custom name with <code>--name</code>:</p> <pre><code>soong start --name \"feature-branch-testing\"\n</code></pre> <p>Names help identify instances when running multiple sessions.</p>"},{"location":"guides/launching-instances/#launch-workflow","title":"Launch Workflow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant CLI\n    participant Lambda API\n    participant Instance\n\n    User-&gt;&gt;CLI: soong start\n    CLI-&gt;&gt;CLI: Load configuration\n    CLI-&gt;&gt;Lambda API: Get instance type pricing\n    CLI-&gt;&gt;User: Show cost estimate\n    User-&gt;&gt;CLI: Confirm (or --yes flag)\n    CLI-&gt;&gt;Lambda API: Launch instance request\n    Lambda API--&gt;&gt;CLI: Instance ID\n    CLI-&gt;&gt;Lambda API: Poll for status (if --wait)\n    Lambda API--&gt;&gt;CLI: Instance ready\n    CLI-&gt;&gt;User: Instance IP and commands</code></pre>"},{"location":"guides/launching-instances/#cost-confirmation","title":"Cost Confirmation","text":"<p>Before launching, soong shows an estimate:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Launch Instance                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                     \u2502\n\u2502 Cost Estimate                       \u2502\n\u2502                                     \u2502\n\u2502 GPU: 1x A6000 (48 GB)              \u2502\n\u2502 Rate: $0.80/hour                   \u2502\n\u2502 Duration: 4 hours                  \u2502\n\u2502                                     \u2502\n\u2502 Estimated cost: $3.20              \u2502\n\u2502                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n? Proceed with launch? (Y/n)\n</code></pre> <p>Skip confirmation with <code>--yes</code>:</p> <pre><code>soong start --yes\n</code></pre>"},{"location":"guides/launching-instances/#waiting-for-instance-ready","title":"Waiting for Instance Ready","text":"<p>By default, the CLI waits for the instance to be ready:</p> <pre><code>soong start --wait  # default behavior\n</code></pre> <p>Skip waiting to return immediately:</p> <pre><code>soong start --no-wait\n</code></pre> <p>When waiting is disabled, check status manually:</p> <pre><code>soong status\n</code></pre>"},{"location":"guides/launching-instances/#examples","title":"Examples","text":""},{"location":"guides/launching-instances/#scenario-deep-reasoning-task","title":"Scenario: Deep Reasoning Task","text":"<p>Launch the most capable reasoning model:</p> <pre><code>soong start \\\n  --model deepseek-r1-70b \\\n  --gpu gpu_1x_a100_sxm4_80gb \\\n  --hours 4 \\\n  --name \"debug-complex-issue\"\n</code></pre> <p>Cost: ~$12-16 for 4 hours (depending on region pricing)</p>"},{"location":"guides/launching-instances/#scenario-fast-iteration-coding","title":"Scenario: Fast Iteration Coding","text":"<p>Use a fast, budget-friendly coding model:</p> <pre><code>soong start \\\n  --model qwen2.5-coder-32b-int4 \\\n  --gpu gpu_1x_a10 \\\n  --hours 2 \\\n  --name \"rapid-prototyping\"\n</code></pre> <p>Cost: ~$1.20-1.60 for 2 hours</p>"},{"location":"guides/launching-instances/#scenario-long-context-refactoring","title":"Scenario: Long Context Refactoring","text":"<p>Use Qwen with 32K context window:</p> <pre><code>soong start \\\n  --model qwen2.5-coder-32b \\\n  --gpu gpu_1x_a6000 \\\n  --hours 6 \\\n  --name \"large-file-refactor\"\n</code></pre> <p>Cost: ~$4.80-6.00 for 6 hours</p>"},{"location":"guides/launching-instances/#after-launch","title":"After Launch","text":"<p>Once launched, the CLI displays:</p> <pre><code>Instance launched: a1b2c3d4\nInstance ready at 123.45.67.89\n\nSSH: soong ssh\nStatus: soong status\n</code></pre> <p>Next steps:</p> <ol> <li>Start SSH tunnel to access services</li> <li>Check status to monitor lease time</li> <li>Extend lease if you need more time</li> </ol>"},{"location":"guides/launching-instances/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/launching-instances/#no-capacity-available","title":"No Capacity Available","text":"<pre><code>Error launching instance: No capacity available in us-west-1\n</code></pre> <p>Solutions:</p> <ol> <li> <p>Try a different region:    <pre><code>soong start --region us-west-2\n</code></pre></p> </li> <li> <p>Check current availability:    <pre><code>soong available\n</code></pre></p> </li> <li> <p>Try a different GPU type (if model allows):    <pre><code>soong start --gpu gpu_1x_rtx6000\n</code></pre></p> </li> </ol>"},{"location":"guides/launching-instances/#ssh-keys-not-found","title":"SSH Keys Not Found","text":"<pre><code>Error: No SSH keys found in Lambda account\nAdd an SSH key at: https://cloud.lambdalabs.com/ssh-keys\n</code></pre> <p>Solution: Add your SSH public key to Lambda Labs:</p> <ol> <li>Go to https://cloud.lambdalabs.com/ssh-keys</li> <li>Click \"Add SSH Key\"</li> <li>Paste your public key (<code>~/.ssh/id_rsa.pub</code>)</li> <li>Try launching again</li> </ol>"},{"location":"guides/launching-instances/#instance-launch-timeout","title":"Instance Launch Timeout","text":"<pre><code>Instance launch timed out\n</code></pre> <p>Solutions:</p> <ol> <li> <p>Check instance status manually:    <pre><code>soong status\n</code></pre></p> </li> <li> <p>The instance may still be initializing. Wait 1-2 minutes and check again.</p> </li> <li> <p>If stuck in \"booting\" state for &gt;5 minutes, terminate and relaunch:    <pre><code>soong stop --yes\nsoong start\n</code></pre></p> </li> </ol>"},{"location":"guides/launching-instances/#cost-estimate-shows-different-price","title":"Cost Estimate Shows Different Price","text":"<p>Cost estimates are based on current Lambda Labs pricing, which can change. The estimate shown is for the lease duration, not the hourly rate.</p> <p>Example: <pre><code>Rate: $1.20/hour\nDuration: 4 hours\nEstimated cost: $4.80  # 4 \u00d7 $1.20\n</code></pre></p> <p>Actual Costs</p> <p>If you extend your lease or the instance runs beyond the lease (before auto-termination), costs will increase. Always monitor with <code>soong status</code>.</p>"},{"location":"guides/launching-instances/#best-practices","title":"Best Practices","text":""},{"location":"guides/launching-instances/#choose-the-right-model","title":"Choose the Right Model","text":"<ul> <li>Complex reasoning: DeepSeek-R1 70B</li> <li>Fast coding: Qwen2.5-Coder 32B</li> <li>Budget coding: Qwen2.5-Coder 32B INT4</li> <li>Simple tasks: Llama 3.1 8B</li> </ul> <p>See Model Management for detailed guidance.</p>"},{"location":"guides/launching-instances/#start-with-shorter-leases","title":"Start with Shorter Leases","text":"<p>Begin with 2-4 hours and extend if needed. This prevents paying for unused time.</p> <pre><code># Start short\nsoong start --hours 2\n\n# Extend later if needed\nsoong extend 2\n</code></pre>"},{"location":"guides/launching-instances/#use-yes-for-automation","title":"Use --yes for Automation","text":"<p>When scripting or automating launches:</p> <pre><code>soong start --yes --no-wait\n</code></pre>"},{"location":"guides/launching-instances/#set-meaningful-names","title":"Set Meaningful Names","text":"<p>Use descriptive names for multi-instance workflows:</p> <pre><code>soong start --name \"frontend-dev-server\"\nsoong start --name \"backend-api-server\"\nsoong start --name \"ml-training-job\"\n</code></pre>"},{"location":"guides/launching-instances/#next-steps","title":"Next Steps","text":"<ul> <li>Set up SSH tunneling to access services</li> <li>Monitor your lease to avoid unexpected termination</li> <li>Optimize costs with smart GPU selection</li> </ul>"},{"location":"guides/managing-leases/","title":"Managing Leases","text":"<p>Monitor, extend, and understand the lease system that prevents runaway GPU costs.</p>"},{"location":"guides/managing-leases/#overview","title":"Overview","text":"<p>Every GPU instance has a lease that defines how long it can run. Leases have these key properties:</p> <ul> <li>Maximum duration: 8 hours</li> <li>Default duration: 4 hours (configurable)</li> <li>Extensions: Allowed up to 8-hour total</li> <li>Auto-termination: Instance stops when lease expires</li> </ul> <p>Why Leases?</p> <p>Leases prevent accidentally leaving expensive GPU instances running indefinitely. You must explicitly extend to keep working.</p>"},{"location":"guides/managing-leases/#checking-lease-status","title":"Checking Lease Status","text":"<p>View all running instances and their lease status:</p> <pre><code>soong status\n</code></pre>"},{"location":"guides/managing-leases/#status-output","title":"Status Output","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           GPU Instances                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 ID     \u2502 Name     \u2502 Status \u2502 IP       \u2502 GPU    \u2502 Uptime \u2502 Time Left\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 a1b2c3 \u2502 dev-work \u2502 active \u2502 1.2.3.4  \u2502 a6000  \u2502 2h 15m \u2502 1h 45m   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/managing-leases/#understanding-status-fields","title":"Understanding Status Fields","text":"Field Description ID First 8 chars of instance ID Name Custom name (if set) Status Instance state (active, booting, terminated) IP Public IP address GPU GPU type Uptime Time since instance started Time Left Time until lease expires"},{"location":"guides/managing-leases/#time-left-indicators","title":"Time Left Indicators","text":"<p>Time remaining is color-coded:</p> <ul> <li>Green: More than 1 hour remaining</li> <li>Yellow: Less than 1 hour remaining</li> <li>Red: Lease expired (instance may still be running briefly)</li> </ul> <pre><code># Examples\n1h 45m      # Green - plenty of time\n45m         # Yellow - less than 1 hour\nEXPIRED     # Red - lease expired\n</code></pre>"},{"location":"guides/managing-leases/#cost-tracking","title":"Cost Tracking","text":"<p>Status also shows cost information:</p> <pre><code>soong status\n</code></pre> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                             GPU Instances                                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 ID   \u2502Name \u2502 Status \u2502 IP      \u2502 GPU    \u2502 Uptime \u2502 Cost Now \u2502 Est. Total  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 a1b2 \u2502 dev \u2502 active \u2502 1.2.3.4 \u2502 a6000  \u2502 2h 15m \u2502 $1.80    \u2502 $3.20       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Field Description Cost Now Amount spent so far (based on uptime) Est. Total Estimated cost if lease runs to completion <p>Cost After Expiration</p> <p>If your lease expires but the instance hasn't auto-terminated yet, costs continue accruing. The \"Cost Now\" field turns red to indicate this.</p>"},{"location":"guides/managing-leases/#extending-leases","title":"Extending Leases","text":"<p>Extend an active instance to add more time:</p> <pre><code>soong extend 2  # Add 2 hours\n</code></pre>"},{"location":"guides/managing-leases/#extension-limits","title":"Extension Limits","text":"<ul> <li>Maximum total lease: 8 hours</li> <li>Minimum extension: 1 hour</li> <li>Maximum extension: Up to 8-hour total limit</li> </ul> <pre><code># Current lease: 4 hours\n# Uptime: 3 hours\n# Remaining: 1 hour\n# Max extension: 4 hours (to reach 8-hour total)\n\nsoong extend 4  # OK - brings total to 8 hours\nsoong extend 5  # Error - would exceed 8-hour limit\n</code></pre>"},{"location":"guides/managing-leases/#extension-cost-confirmation","title":"Extension Cost Confirmation","text":"<p>Before extending, you'll see a cost estimate:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Extend Lease                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                     \u2502\n\u2502 Extension Cost Estimate             \u2502\n\u2502                                     \u2502\n\u2502 Instance: a1b2c3d4                 \u2502\n\u2502 GPU: 1x A6000 (48 GB)              \u2502\n\u2502 Rate: $0.80/hour                   \u2502\n\u2502 Extension: 2 hours                 \u2502\n\u2502                                     \u2502\n\u2502 Additional cost: $1.60             \u2502\n\u2502                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n? Extend lease by 2 hours? (Y/n)\n</code></pre> <p>Skip confirmation with <code>--yes</code>:</p> <pre><code>soong extend 2 --yes\n</code></pre>"},{"location":"guides/managing-leases/#extension-workflow","title":"Extension Workflow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant CLI\n    participant Instance\n\n    User-&gt;&gt;CLI: soong extend 2\n    CLI-&gt;&gt;Instance: POST /extend\n    Note over Instance: Calculate new expiry\n    Instance--&gt;&gt;CLI: New shutdown time\n    CLI-&gt;&gt;User: Lease extended\n    Note over User,Instance: Continue working</code></pre>"},{"location":"guides/managing-leases/#auto-shutdown-behavior","title":"Auto-Shutdown Behavior","text":"<p>Instances automatically terminate when:</p> <ol> <li>Lease expires - 8-hour maximum reached</li> <li>Idle timeout - 30 minutes of no activity</li> <li>Manual stop - You run <code>soong stop</code></li> </ol>"},{"location":"guides/managing-leases/#idle-detection","title":"Idle Detection","text":"<p>The status daemon monitors system activity and will auto-terminate after 30 minutes of inactivity:</p> <ul> <li>Monitored metrics:</li> <li>CPU usage</li> <li>GPU utilization</li> <li>Network activity</li> <li> <p>Active SSH sessions</p> </li> <li> <p>Grace period: 30 minutes</p> </li> <li>Warning: None (automatic)</li> </ul> <p>Keep Alive</p> <p>If running long-running tasks (training, batch jobs), ensure they generate GPU activity to prevent idle shutdown.</p>"},{"location":"guides/managing-leases/#lease-expiration-process","title":"Lease Expiration Process","text":"<p>When a lease expires:</p> <ol> <li>T-0: Lease expires</li> <li>T+5min: Instance begins shutdown sequence</li> <li>T+10min: Instance terminates</li> </ol> <p>No Grace Period for Leases</p> <p>Unlike idle timeout, lease expiration has no grace period. Extend before the lease expires.</p>"},{"location":"guides/managing-leases/#stopping-instances","title":"Stopping Instances","text":"<p>Manually terminate an instance:</p> <pre><code>soong stop\n</code></pre> <p>With confirmation:</p> <pre><code>? Terminate instance a1b2c3d4e5f6? (y/N)\n</code></pre> <p>Skip confirmation with <code>--yes</code>:</p> <pre><code>soong stop --yes\n</code></pre> <p>Unsaved Work</p> <p>Stopping an instance terminates it immediately. Ensure all work is saved to the persistent filesystem (<code>/home/ubuntu/workspace</code>) before stopping.</p>"},{"location":"guides/managing-leases/#viewing-history","title":"Viewing History","text":""},{"location":"guides/managing-leases/#termination-history","title":"Termination History","text":"<p>View why instances were terminated:</p> <pre><code>soong status --history\n</code></pre> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  Termination History (Last 24 Hours)                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Time       \u2502 Inst ID \u2502 Reason             \u2502 Uptime \u2502 GPU    \u2502 Region \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2025-01-01 \u2502 a1b2c3  \u2502 Lease expired      \u2502 8h 5m  \u2502 a6000  \u2502 us-w-1 \u2502\n\u2502 2024-12-31 \u2502 x9y8z7  \u2502 Idle timeout       \u2502 2h 45m \u2502 a10    \u2502 us-w-1 \u2502\n\u2502 2024-12-31 \u2502 m5n6o7  \u2502 Manual termination \u2502 3h 12m \u2502 a100   \u2502 us-e-1 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Adjust time window:</p> <pre><code>soong status --history --history-hours 48  # Last 48 hours\n</code></pre>"},{"location":"guides/managing-leases/#stopped-instances","title":"Stopped Instances","text":"<p>View recently stopped instances:</p> <pre><code>soong status --stopped\n</code></pre> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         Stopped Instances                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Inst ID \u2502 Name   \u2502 Status   \u2502 GPU    \u2502 Region \u2502 Created At       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 a1b2c3  \u2502 dev-1  \u2502 termin'd \u2502 a6000  \u2502 us-w-1 \u2502 2025-01-01 10:30 \u2502\n\u2502 x9y8z7  \u2502 test-2 \u2502 stopped  \u2502 a10    \u2502 us-w-1 \u2502 2024-12-31 14:20 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/managing-leases/#best-practices","title":"Best Practices","text":""},{"location":"guides/managing-leases/#monitor-regularly","title":"Monitor Regularly","text":"<p>Check status every hour during active development:</p> <pre><code># Add to your workflow\nwatch -n 3600 soong status  # Check every hour\n</code></pre> <p>Or set up a terminal status bar:</p> <pre><code># Simple status check\nalias gpu-status='soong status'\n</code></pre>"},{"location":"guides/managing-leases/#extend-proactively","title":"Extend Proactively","text":"<p>Don't wait until the last minute:</p> <pre><code># When you see &lt; 1 hour remaining\nTime Left: 45m  # Yellow warning\n\n# Extend immediately\nsoong extend 2\n</code></pre>"},{"location":"guides/managing-leases/#save-work-to-persistent-storage","title":"Save Work to Persistent Storage","text":"<p>The persistent filesystem survives instance termination:</p> <pre><code># Always work in workspace\ncd /home/ubuntu/workspace\n\n# Or symlink projects\nln -s /home/ubuntu/workspace/my-project ~/my-project\n</code></pre> <p>Files outside <code>/home/ubuntu/workspace</code> are lost when the instance terminates.</p>"},{"location":"guides/managing-leases/#use-multiple-short-leases","title":"Use Multiple Short Leases","text":"<p>Instead of one 8-hour lease, use multiple shorter sessions:</p> <pre><code># Session 1: 2-4 hours\nsoong start --hours 2\n# ... work ...\nsoong stop\n\n# Session 2: 2-4 hours (later)\nsoong start --hours 2\n# ... more work ...\n</code></pre> <p>Benefits: - Pay only for actual usage - Fresh instance state - Lower risk of data loss</p>"},{"location":"guides/managing-leases/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/managing-leases/#lease-extended-but-still-shows-old-time","title":"Lease Extended but Still Shows Old Time","text":"<p>Status may be cached. Wait 10-30 seconds and check again:</p> <pre><code>sleep 30 &amp;&amp; soong status\n</code></pre>"},{"location":"guides/managing-leases/#cant-extend-already-at-8-hours","title":"Can't Extend (Already at 8 Hours)","text":"<pre><code>Error extending lease: Maximum lease duration reached (8 hours)\n</code></pre> <p>Solution: You must stop the current instance and start a new one:</p> <pre><code>soong stop --yes\nsoong start --hours 4\n</code></pre>"},{"location":"guides/managing-leases/#instance-terminated-unexpectedly","title":"Instance Terminated Unexpectedly","text":"<p>Check termination history to see why:</p> <pre><code>soong status --history\n</code></pre> <p>Common reasons:</p> <ul> <li>Lease expired: You didn't extend in time</li> <li>Idle timeout: No activity for 30 minutes</li> <li>Manual termination: You or someone else ran <code>stop</code></li> </ul>"},{"location":"guides/managing-leases/#cost-still-increasing-after-expiration","title":"Cost Still Increasing After Expiration","text":"<p>If the lease shows \"EXPIRED\" but cost is still increasing (red), the instance hasn't auto-terminated yet. Manually stop it:</p> <pre><code>soong stop --yes\n</code></pre>"},{"location":"guides/managing-leases/#next-steps","title":"Next Steps","text":"<ul> <li>Set up SSH tunneling to access services</li> <li>Optimize costs with smart lease management</li> <li>Add custom models for your workflow</li> </ul>"},{"location":"guides/model-management/","title":"Model Management","text":"<p>Choose, configure, and add custom AI models for your GPU instances.</p>"},{"location":"guides/model-management/#overview","title":"Overview","text":"<p>soong supports multiple AI models with different characteristics:</p> <ul> <li>Pre-configured models: Ready to use with known VRAM requirements</li> <li>Custom models: Add your own models with manual VRAM calculation</li> <li>GPU recommendations: Automatic matching of models to GPUs</li> </ul>"},{"location":"guides/model-management/#listing-models","title":"Listing Models","text":"<p>View all available models:</p> <pre><code>soong models\n</code></pre> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          Available Models                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 ID                   \u2502 Params \u2502 Quant \u2502 VRAM \u2502 Min GPU              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 llama-3.1-8b         \u2502 8B     \u2502 FP16  \u2502 22GB \u2502 1x A10 (24 GB)       \u2502\n\u2502 qwen2.5-coder-32b... \u2502 32B    \u2502 INT4  \u2502 26GB \u2502 1x A6000 (48 GB)     \u2502\n\u2502 qwen2.5-coder-32b    \u2502 32B    \u2502 FP16  \u2502 46GB \u2502 1x A6000 (48 GB)     \u2502\n\u2502 codellama-34b        \u2502 34B    \u2502 FP16  \u2502 48GB \u2502 1x A6000 (48 GB)     \u2502\n\u2502 deepseek-r1-70b      \u2502 70B    \u2502 INT4  \u2502 49GB \u2502 1x A100 SXM4 (80 GB) \u2502\n\u2502 llama-3.1-70b        \u2502 70B    \u2502 INT4  \u2502 49GB \u2502 1x A100 SXM4 (80 GB) \u2502\n\u2502 mistral-7b           \u2502 7B     \u2502 FP16  \u2502 21GB \u2502 1x A10 (24 GB)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Models are sorted by VRAM requirement (cheapest first).</p>"},{"location":"guides/model-management/#model-details","title":"Model Details","text":"<p>Get comprehensive information about a specific model:</p> <pre><code>soong models info deepseek-r1-70b\n</code></pre> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      DeepSeek-R1 70B              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nHuggingFace Path: deepseek-ai/DeepSeek-R1-Distill-Llama-70B\nParameters: 70B\nQuantization: INT4\nContext Length: 8,192 tokens\n\nVRAM Breakdown:\n  Base weights:       35.0 GB\n  KV cache:            4.0 GB\n  Overhead:            2.0 GB\n  Activations:         3.5 GB\n  Total estimated:    49.0 GB\n\nRecommended GPU: 1x A100 SXM4 (80 GB)\n  Price: $1.29/hour\n\nGood for:\n  \u2022 Complex multi-step reasoning\n  \u2022 Debugging difficult issues\n  \u2022 Architecture decisions\n  \u2022 Code review with explanations\n\nNot good for:\n  \u2022 Simple/quick tasks (overkill)\n  \u2022 Long context windows (8K limit)\n  \u2022 Speed-critical applications\n\nNotes: Chain-of-thought reasoning. Slower but more accurate.\n</code></pre>"},{"location":"guides/model-management/#model-comparison","title":"Model Comparison","text":""},{"location":"guides/model-management/#deepseek-r1-70b","title":"DeepSeek-R1 70B","text":"<p>Best for: Complex reasoning tasks</p> <pre><code>Parameters: 70B (INT4)\nVRAM: ~49 GB\nMin GPU: A100 80GB\nContext: 8K tokens\nCost: ~$1.29/hour\n</code></pre> <p>Strengths: - Chain-of-thought reasoning - Debugging complex issues - Architecture decisions - Code review with detailed explanations</p> <p>Weaknesses: - Overkill for simple tasks - Limited context window (8K) - Slower inference</p> <p>Use when: You need maximum reasoning capability and accuracy matters more than speed.</p>"},{"location":"guides/model-management/#qwen25-coder-32b","title":"Qwen2.5-Coder 32B","text":"<p>Best for: Fast coding with long context</p> <pre><code>Parameters: 32B (FP16)\nVRAM: ~46 GB\nMin GPU: A6000 48GB\nContext: 32K tokens\nCost: ~$0.80/hour\n</code></pre> <p>Strengths: - Purpose-built for code - 4x longer context (32K vs 8K) - Fast inference - Multi-language support</p> <p>Weaknesses: - Less reasoning capability than DeepSeek - Not ideal for non-coding tasks - Limited world knowledge</p> <p>Use when: You're refactoring large files or need long context for code generation.</p>"},{"location":"guides/model-management/#qwen25-coder-32b-int4","title":"Qwen2.5-Coder 32B INT4","text":"<p>Best for: Budget-friendly coding</p> <pre><code>Parameters: 32B (INT4)\nVRAM: ~26 GB\nMin GPU: A10 24GB\nContext: 32K tokens\nCost: ~$0.60/hour\n</code></pre> <p>Strengths: - 50% cheaper than FP16 version - Same 32K context window - Runs on cheaper GPUs - Good quality/cost ratio</p> <p>Weaknesses: - ~5% quality loss vs FP16 - Slightly less accurate for complex code</p> <p>Use when: Cost is a concern and you can tolerate minor quality reduction.</p>"},{"location":"guides/model-management/#llama-31-70b","title":"Llama 3.1 70B","text":"<p>Best for: General-purpose tasks</p> <pre><code>Parameters: 70B (INT4)\nVRAM: ~49 GB\nMin GPU: A100 80GB\nContext: 8K tokens\nCost: ~$1.29/hour\n</code></pre> <p>Strengths: - Broad task coverage - Excellent instruction following - Good for writing and documentation - Balanced code + general knowledge</p> <p>Weaknesses: - Not specialized for coding - Not specialized for reasoning - Jack of all trades, master of none</p> <p>Use when: You need a single model for diverse tasks (code, writing, analysis).</p>"},{"location":"guides/model-management/#llama-31-8b","title":"Llama 3.1 8B","text":"<p>Best for: Quick, simple tasks</p> <pre><code>Parameters: 8B (FP16)\nVRAM: ~22 GB\nMin GPU: A10 24GB\nContext: 8K tokens\nCost: ~$0.60/hour\n</code></pre> <p>Strengths: - Fastest inference - Cheapest option - Good for simple code changes - Rapid iteration</p> <p>Weaknesses: - Limited reasoning - Struggles with complex tasks - Smaller knowledge base</p> <p>Use when: Speed and cost matter more than quality for straightforward tasks.</p>"},{"location":"guides/model-management/#adding-custom-models","title":"Adding Custom Models","text":"<p>Add models not in the pre-configured list:</p>"},{"location":"guides/model-management/#interactive-mode","title":"Interactive Mode","text":"<pre><code>soong models add\n</code></pre> <p>The CLI prompts for details:</p> <pre><code>Add Custom Model\n\n? Model name/ID: mistral-nemo-12b\n? HuggingFace model path: mistralai/Mistral-Nemo-Instruct-2407\n? Parameter count (billions): 12\n? Quantization: FP16 (2 bytes/param)\n? Context length: 128000\n\nEstimated VRAM: 32.5 GB\nMinimum GPU: 40 GB\n\nModel 'mistral-nemo-12b' added successfully!\n</code></pre>"},{"location":"guides/model-management/#flag-mode","title":"Flag Mode","text":"<p>For scripting, use flags:</p> <pre><code>soong models add \\\n  --name mistral-nemo-12b \\\n  --hf-path mistralai/Mistral-Nemo-Instruct-2407 \\\n  --params 12 \\\n  --quantization fp16 \\\n  --context 128000\n</code></pre>"},{"location":"guides/model-management/#quantization-options","title":"Quantization Options","text":"Quantization Bytes/Param Use Case FP32 4.0 Research, maximum precision FP16 2.0 Production, good quality INT8 1.0 Efficient, minor quality loss INT4 0.5 Budget, ~5% quality loss"},{"location":"guides/model-management/#vram-calculation","title":"VRAM Calculation","text":"<p>Understanding how VRAM is estimated:</p> <pre><code>Total VRAM = Base Weights + KV Cache + Overhead + Activations\n</code></pre>"},{"location":"guides/model-management/#components","title":"Components","text":"<p>Base Weights: <pre><code>params_billions \u00d7 quantization_bytes_per_param\n</code></pre></p> <p>Example: 70B model at INT4 (0.5 bytes/param) <pre><code>70 \u00d7 0.5 = 35 GB\n</code></pre></p> <p>KV Cache: <pre><code>min(4.0, context_length / 2048)\n</code></pre></p> <p>Example: 8K context <pre><code>8192 / 2048 = 4.0 GB\n</code></pre></p> <p>Overhead: <pre><code>2 GB (CUDA, framework, etc.)\n</code></pre></p> <p>Activations: <pre><code>base_weights \u00d7 0.1\n</code></pre></p> <p>Example: 35 GB base <pre><code>35 \u00d7 0.1 = 3.5 GB\n</code></pre></p> <p>Total: <pre><code>35 + 4 + 2 + 3.5 = 44.5 GB\n</code></pre></p>"},{"location":"guides/model-management/#safety-margin","title":"Safety Margin","text":"<p>Recommended GPU sizes include a 10% headroom:</p> <pre><code>estimated_vram \u00d7 1.1 \u2264 gpu_vram\n</code></pre> <p>For 44.5 GB estimated, minimum 48 GB GPU recommended (A6000).</p>"},{"location":"guides/model-management/#gpu-recommendations","title":"GPU Recommendations","text":"<p>soong automatically recommends the cheapest GPU that fits a model:</p> <pre><code>graph TD\n    A[Model: 49 GB VRAM] --&gt; B{Check GPUs}\n    B --&gt; C[A10: 24 GB] --&gt; D[Too small \u274c]\n    B --&gt; E[A6000: 48 GB] --&gt; F[Too small \u274c]\n    B --&gt; G[A100: 80 GB] --&gt; H[Fits \u2705]\n    H --&gt; I[Recommend: A100 80GB]</code></pre>"},{"location":"guides/model-management/#gpu-vram-sizes","title":"GPU VRAM Sizes","text":"GPU VRAM Typical Use A10 24 GB Small models (\u226420 GB) A100 40GB 40 GB Medium models (\u226436 GB) A6000 48 GB Medium-large models (\u226443 GB) RTX 6000 48 GB Same as A6000 A100 80GB 80 GB Large models (\u226472 GB) H100 80 GB Large models, fastest"},{"location":"guides/model-management/#removing-custom-models","title":"Removing Custom Models","text":"<p>Delete a custom model from configuration:</p> <pre><code>soong models remove mistral-nemo-12b\n</code></pre> <p>With confirmation:</p> <pre><code>? Remove custom model 'mistral-nemo-12b'? (y/N)\n</code></pre> <p>Skip confirmation with <code>--yes</code>:</p> <pre><code>soong models remove mistral-nemo-12b --yes\n</code></pre> <p>Built-in Models</p> <p>You cannot remove pre-configured models (like <code>deepseek-r1-70b</code>). Only custom models can be removed.</p>"},{"location":"guides/model-management/#model-selection-strategy","title":"Model Selection Strategy","text":"<pre><code>graph TD\n    A[What are you building?] --&gt; B{Task Type}\n\n    B --&gt;|Complex reasoning| C[DeepSeek-R1 70B]\n    B --&gt;|Fast coding| D{Need long context?}\n    B --&gt;|General purpose| E[Llama 3.1 70B]\n    B --&gt;|Simple/quick| F[Llama 3.1 8B]\n\n    D --&gt;|Yes 32K+| G{Budget?}\n    D --&gt;|No 8K OK| H[Qwen2.5-Coder 32B INT4]\n\n    G --&gt;|High quality| I[Qwen2.5-Coder 32B FP16]\n    G --&gt;|Budget| J[Qwen2.5-Coder 32B INT4]\n\n    C --&gt; K[A100 80GB ~$1.29/hr]\n    E --&gt; K\n    I --&gt; L[A6000 48GB ~$0.80/hr]\n    H --&gt; M[A10 24GB ~$0.60/hr]\n    J --&gt; M\n    F --&gt; M</code></pre>"},{"location":"guides/model-management/#decision-matrix","title":"Decision Matrix","text":"Scenario Model GPU Est. Cost Debugging hard issue DeepSeek-R1 70B A100 80GB $1.29/hr Refactoring large files Qwen2.5-Coder 32B A6000 48GB $0.80/hr Budget coding Qwen2.5-Coder 32B INT4 A10 24GB $0.60/hr Documentation writing Llama 3.1 70B A100 80GB $1.29/hr Quick code fix Llama 3.1 8B A10 24GB $0.60/hr"},{"location":"guides/model-management/#best-practices","title":"Best Practices","text":""},{"location":"guides/model-management/#start-with-defaults","title":"Start with Defaults","text":"<p>Use pre-configured models first:</p> <pre><code>soong start --model qwen2.5-coder-32b\n</code></pre> <p>Only add custom models when necessary.</p>"},{"location":"guides/model-management/#verify-huggingface-paths","title":"Verify HuggingFace Paths","text":"<p>Before adding a custom model, check it exists:</p> <pre><code># Visit HuggingFace\nhttps://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407\n</code></pre>"},{"location":"guides/model-management/#use-int4-for-budget","title":"Use INT4 for Budget","text":"<p>When cost is a concern, use INT4 quantization:</p> <pre><code># Instead of FP16 (46 GB, $0.80/hr)\nsoong start --model qwen2.5-coder-32b\n\n# Use INT4 (26 GB, $0.60/hr)\nsoong start --model qwen2.5-coder-32b-int4\n</code></pre> <p>Quality loss is typically ~5%, acceptable for most tasks.</p>"},{"location":"guides/model-management/#match-context-to-task","title":"Match Context to Task","text":"<p>Don't pay for unused context:</p> <ul> <li>8K context: Most tasks (99% of code files)</li> <li>32K context: Large file refactoring, documentation</li> <li>128K+ context: Entire codebases (rare)</li> </ul>"},{"location":"guides/model-management/#test-before-adding-custom-models","title":"Test Before Adding Custom Models","text":"<p>Before adding a custom model permanently:</p> <pre><code># Test with SGLang directly on instance\nsoong ssh\n\nubuntu@instance:~$ python -m sglang.launch_server \\\n  --model mistralai/Mistral-Nemo-Instruct-2407 \\\n  --port 8000\n\n# Monitor VRAM usage\nubuntu@instance:~$ nvidia-smi\n</code></pre> <p>If it works, then add to config.</p>"},{"location":"guides/model-management/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/model-management/#model-not-found","title":"Model Not Found","text":"<pre><code>Error: Model 'custom-model' not found.\nUse 'soong models' to see available models.\n</code></pre> <p>Solution: List models and check spelling:</p> <pre><code>soong models\n</code></pre>"},{"location":"guides/model-management/#invalid-quantization","title":"Invalid Quantization","text":"<pre><code>Error: Invalid quantization 'fp8'. Must be one of: fp32, fp16, int8, int4\n</code></pre> <p>Solution: Use a valid quantization level:</p> <pre><code>soong models add \\\n  --name my-model \\\n  --quantization fp16  # Valid\n</code></pre>"},{"location":"guides/model-management/#gpu-too-small","title":"GPU Too Small","text":"<p>During launch:</p> <pre><code>Warning: Selected GPU has 24GB but model needs ~46GB\n</code></pre> <p>Solution: Use a larger GPU:</p> <pre><code>soong start --model qwen2.5-coder-32b --gpu gpu_1x_a6000\n</code></pre> <p>Or use a quantized version:</p> <pre><code>soong start --model qwen2.5-coder-32b-int4  # Needs only 26GB\n</code></pre>"},{"location":"guides/model-management/#model-wont-load-on-instance","title":"Model Won't Load on Instance","text":"<p>After launching, model fails to load:</p> <pre><code># On instance\nOutOfMemoryError: CUDA out of memory\n</code></pre> <p>Causes:</p> <ol> <li>VRAM estimate was too low</li> <li>Other processes using GPU memory</li> <li>Model requires more overhead than estimated</li> </ol> <p>Solutions:</p> <ol> <li> <p>Increase VRAM estimate in custom model:    <pre><code>soong models remove my-model\nsoong models add --params 35  # Was 32, bump up\n</code></pre></p> </li> <li> <p>Use a larger GPU or INT4 quantization</p> </li> <li> <p>Check for memory leaks:    <pre><code>ubuntu@instance:~$ nvidia-smi\n</code></pre></p> </li> </ol>"},{"location":"guides/model-management/#next-steps","title":"Next Steps","text":"<ul> <li>Launch instances with your chosen model</li> <li>Optimize costs by selecting the right model/GPU combo</li> <li>Manage leases to maximize model uptime</li> </ul>"},{"location":"guides/ssh-tunneling/","title":"SSH Tunneling","text":"<p>Securely access remote services running on your GPU instance through SSH port forwarding.</p>"},{"location":"guides/ssh-tunneling/#overview","title":"Overview","text":"<p>SSH tunneling creates encrypted connections from your local machine to services running on the GPU instance:</p> <pre><code>Your Computer                GPU Instance\nlocalhost:8000  &lt;---SSH---&gt;  remote:8000 (SGLang API)\nlocalhost:5678  &lt;---SSH---&gt;  remote:5678 (n8n)\nlocalhost:8080  &lt;---SSH---&gt;  remote:8080 (Status Daemon)\n</code></pre> <p>All traffic flows through a single encrypted SSH connection, eliminating the need to expose ports publicly.</p>"},{"location":"guides/ssh-tunneling/#quick-start","title":"Quick Start","text":"<p>Start a tunnel to your active instance:</p> <pre><code>soong tunnel start\n</code></pre> <p>This forwards three ports by default:</p> Local Port Remote Service Purpose <code>8000</code> SGLang API Model inference endpoint <code>5678</code> n8n Workflow automation UI <code>8080</code> Status Daemon Instance monitoring API"},{"location":"guides/ssh-tunneling/#starting-a-tunnel","title":"Starting a Tunnel","text":""},{"location":"guides/ssh-tunneling/#basic-usage","title":"Basic Usage","text":"<pre><code>soong tunnel start\n</code></pre> <p>The tunnel runs in the background as a daemon process:</p> <pre><code>Starting SSH tunnel to 123.45.67.89...\nSSH tunnel started (PID: 12345)\n  localhost:8000 -&gt; 123.45.67.89:8000\n  localhost:5678 -&gt; 123.45.67.89:5678\n  localhost:8080 -&gt; 123.45.67.89:8080\n</code></pre> <p>Background Process</p> <p>The tunnel uses <code>ssh -N -f</code> to fork into the background. It will persist until explicitly stopped or your machine reboots.</p>"},{"location":"guides/ssh-tunneling/#custom-ports","title":"Custom Ports","text":"<p>Override default ports:</p> <pre><code>soong tunnel start \\\n  --sglang-port 8001 \\\n  --n8n-port 5679 \\\n  --status-port 8081\n</code></pre> <p>This is useful when local ports are already in use.</p>"},{"location":"guides/ssh-tunneling/#specify-instance","title":"Specify Instance","text":"<p>Connect to a specific instance (when running multiple):</p> <pre><code>soong tunnel start --instance-id a1b2c3d4\n</code></pre>"},{"location":"guides/ssh-tunneling/#how-it-works","title":"How It Works","text":"<pre><code>sequenceDiagram\n    participant User\n    participant CLI\n    participant SSH Client\n    participant GPU Instance\n\n    User-&gt;&gt;CLI: tunnel start\n    CLI-&gt;&gt;CLI: Check if tunnel already running\n    CLI-&gt;&gt;SSH Client: Start background tunnel\n    Note over SSH Client: ssh -N -f -L 8000:localhost:8000 ...\n    SSH Client-&gt;&gt;GPU Instance: Establish connection\n    SSH Client-&gt;&gt;CLI: Find process PID\n    CLI-&gt;&gt;CLI: Store PID to ~/.config/gpu-dashboard/tunnel.pid\n    CLI-&gt;&gt;User: Tunnel started (PID: 12345)\n\n    Note over User,GPU Instance: Tunnel runs in background\n\n    User-&gt;&gt;User: Access http://localhost:8000\n    Note over User,GPU Instance: Traffic flows through SSH tunnel</code></pre>"},{"location":"guides/ssh-tunneling/#ssh-command-details","title":"SSH Command Details","text":"<p>The tunnel uses these SSH flags:</p> <pre><code>ssh -N -f \\\n  -o StrictHostKeyChecking=no \\\n  -o UserKnownHostsFile=/dev/null \\\n  -o ServerAliveInterval=60 \\\n  -L 8000:localhost:8000 \\\n  -L 5678:localhost:5678 \\\n  -L 8080:localhost:8080 \\\n  -i ~/.ssh/id_rsa \\\n  ubuntu@123.45.67.89\n</code></pre> Flag Purpose <code>-N</code> No remote command (just forwarding) <code>-f</code> Fork to background after authentication <code>-L</code> Local port forwarding specification <code>-o ServerAliveInterval=60</code> Keep connection alive with heartbeat <code>-i</code> SSH private key for authentication"},{"location":"guides/ssh-tunneling/#pid-management","title":"PID Management","text":"<p>The tunnel process ID (PID) is stored in:</p> <pre><code>~/.config/gpu-dashboard/tunnel.pid\n</code></pre> <p>This allows the CLI to:</p> <ul> <li>Check if a tunnel is already running</li> <li>Stop the tunnel by PID</li> <li>Prevent duplicate tunnels</li> </ul>"},{"location":"guides/ssh-tunneling/#accessing-services","title":"Accessing Services","text":"<p>Once the tunnel is running, access services at localhost URLs:</p>"},{"location":"guides/ssh-tunneling/#sglang-api-port-8000","title":"SGLang API (Port 8000)","text":"<pre><code>curl http://localhost:8000/v1/models\n</code></pre> <pre><code>curl http://localhost:8000/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"qwen2.5-coder-32b\",\n    \"prompt\": \"def fibonacci(n):\",\n    \"max_tokens\": 100\n  }'\n</code></pre>"},{"location":"guides/ssh-tunneling/#n8n-workflow-ui-port-5678","title":"n8n Workflow UI (Port 5678)","text":"<p>Open in browser:</p> <pre><code>http://localhost:5678\n</code></pre>"},{"location":"guides/ssh-tunneling/#status-daemon-api-port-8080","title":"Status Daemon API (Port 8080)","text":"<pre><code># Get instance status\ncurl http://localhost:8080/status \\\n  -H \"Authorization: Bearer YOUR_TOKEN\"\n\n# Extend lease\ncurl -X POST http://localhost:8080/extend \\\n  -H \"Authorization: Bearer YOUR_TOKEN\" \\\n  -d \"hours=2\"\n</code></pre>"},{"location":"guides/ssh-tunneling/#checking-tunnel-status","title":"Checking Tunnel Status","text":"<pre><code>soong tunnel status\n</code></pre> <p>Output:</p> <pre><code>Tunnel is running\n</code></pre> <p>or</p> <pre><code>Tunnel is not running\n</code></pre>"},{"location":"guides/ssh-tunneling/#stopping-a-tunnel","title":"Stopping a Tunnel","text":"<pre><code>soong tunnel stop\n</code></pre> <p>This terminates the background SSH process:</p> <pre><code>Stopped tunnel (PID: 12345)\n</code></pre> <p>The PID file (<code>~/.config/gpu-dashboard/tunnel.pid</code>) is also removed.</p> <p>Auto-Cleanup</p> <p>If the tunnel process dies unexpectedly, the CLI detects the stale PID file and cleans it up automatically.</p>"},{"location":"guides/ssh-tunneling/#direct-ssh-access","title":"Direct SSH Access","text":"<p>For interactive shell access (not tunneling), use:</p> <pre><code>soong ssh\n</code></pre> <p>This opens an interactive SSH session:</p> <pre><code>ubuntu@gpu-instance:~$ nvidia-smi\nubuntu@gpu-instance:~$ cd ~/workspace\nubuntu@gpu-instance:~$ python train.py\n</code></pre> <p>Difference: Tunnel vs SSH</p> <ul> <li>Tunnel: Background port forwarding, no shell</li> <li>SSH: Interactive shell session</li> </ul>"},{"location":"guides/ssh-tunneling/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/ssh-tunneling/#tunnel-already-running","title":"Tunnel Already Running","text":"<pre><code>Tunnel already running. Stop it first.\n</code></pre> <p>Solution: Stop the existing tunnel:</p> <pre><code>soong tunnel stop\nsoong tunnel start\n</code></pre>"},{"location":"guides/ssh-tunneling/#port-already-in-use","title":"Port Already in Use","text":"<pre><code>Error starting tunnel: bind: Address already in use\n</code></pre> <p>Solution: Either:</p> <ol> <li>Stop the conflicting service on the local port, or</li> <li>Use custom ports:</li> </ol> <pre><code>soong tunnel start --sglang-port 8001\n</code></pre>"},{"location":"guides/ssh-tunneling/#tunnel-process-not-found","title":"Tunnel Process Not Found","text":"<pre><code>Tunnel process not found (already stopped?)\n</code></pre> <p>This happens when the tunnel died unexpectedly. The CLI cleans up the stale PID file automatically.</p> <p>Solution: Start a new tunnel:</p> <pre><code>soong tunnel start\n</code></pre>"},{"location":"guides/ssh-tunneling/#ssh-connection-timeout","title":"SSH Connection Timeout","text":"<pre><code>SSH tunnel command timed out\n</code></pre> <p>Possible causes:</p> <ol> <li> <p>Instance not ready yet:    <pre><code>soong status  # Check if instance is \"active\"\n</code></pre></p> </li> <li> <p>Network issues or firewall blocking SSH (port 22)</p> </li> <li> <p>Incorrect SSH key:    <pre><code>soong configure  # Reconfigure SSH key path\n</code></pre></p> </li> </ol>"},{"location":"guides/ssh-tunneling/#cant-access-localhost8000","title":"Can't Access localhost:8000","text":"<p>Checklist:</p> <ol> <li> <p>Verify tunnel is running:    <pre><code>soong tunnel status\n</code></pre></p> </li> <li> <p>Check if service is actually running on the instance:    <pre><code>soong ssh\nubuntu@instance:~$ curl localhost:8000/v1/models\n</code></pre></p> </li> <li> <p>Verify firewall isn't blocking local connections:    <pre><code># macOS/Linux\nsudo lsof -i :8000\n</code></pre></p> </li> </ol>"},{"location":"guides/ssh-tunneling/#advanced-usage","title":"Advanced Usage","text":""},{"location":"guides/ssh-tunneling/#multiple-instances","title":"Multiple Instances","text":"<p>Run tunnels to multiple instances on different port ranges:</p> <pre><code># Instance 1 (dev)\nsoong tunnel start \\\n  --instance-id a1b2c3d4 \\\n  --sglang-port 8000 \\\n  --n8n-port 5678 \\\n  --status-port 8080\n\n# Instance 2 (staging)\nsoong tunnel start \\\n  --instance-id x9y8z7w6 \\\n  --sglang-port 8100 \\\n  --n8n-port 5778 \\\n  --status-port 8180\n</code></pre> <p>One Tunnel at a Time</p> <p>The current implementation stores only one PID, so only one tunnel can be managed by <code>soong tunnel</code>. For multiple tunnels, use raw SSH commands (see below).</p>"},{"location":"guides/ssh-tunneling/#manual-ssh-tunneling","title":"Manual SSH Tunneling","text":"<p>For custom configurations, use SSH directly:</p> <pre><code>ssh -N -f \\\n  -L 8000:localhost:8000 \\\n  -L 5678:localhost:5678 \\\n  -i ~/.ssh/id_rsa \\\n  ubuntu@123.45.67.89\n</code></pre> <p>To stop:</p> <pre><code># Find PID\npgrep -f \"ssh.*123.45.67.89\"\n\n# Kill by PID\nkill 12345\n</code></pre>"},{"location":"guides/ssh-tunneling/#port-forwarding-patterns","title":"Port Forwarding Patterns","text":"<p>Local forwarding (what we use): <pre><code>-L local_port:remote_host:remote_port\n</code></pre></p> <p>Forward a local port to a remote destination.</p> <p>Remote forwarding: <pre><code>-R remote_port:local_host:local_port\n</code></pre></p> <p>Expose a local service on the remote instance (less common).</p> <p>Dynamic forwarding (SOCKS proxy): <pre><code>-D local_port\n</code></pre></p> <p>Create a SOCKS proxy for routing all traffic through the instance.</p>"},{"location":"guides/ssh-tunneling/#security-considerations","title":"Security Considerations","text":""},{"location":"guides/ssh-tunneling/#encrypted-connection","title":"Encrypted Connection","text":"<p>All traffic through the tunnel is encrypted by SSH, even if the underlying service (like HTTP) is unencrypted.</p> <pre><code>Plaintext HTTP  \u2192  SSH Encryption  \u2192  SSH Decryption  \u2192  Plaintext HTTP\n(Your App)         (Tunnel)            (Tunnel)           (Remote Service)\n</code></pre>"},{"location":"guides/ssh-tunneling/#authentication","title":"Authentication","text":"<p>Tunnel authentication uses your SSH private key:</p> <pre><code>~/.ssh/id_rsa  # Default location\n</code></pre> <p>Protect Your Private Key</p> <p>Never share your SSH private key. It grants full access to your instances.</p>"},{"location":"guides/ssh-tunneling/#status-daemon-token","title":"Status Daemon Token","text":"<p>API calls to the status daemon (port 8080) require token authentication:</p> <pre><code>curl http://localhost:8080/status \\\n  -H \"Authorization: Bearer YOUR_TOKEN\"\n</code></pre> <p>The token is stored in <code>~/.config/soong/config.json</code> and auto-generated during setup.</p>"},{"location":"guides/ssh-tunneling/#best-practices","title":"Best Practices","text":""},{"location":"guides/ssh-tunneling/#always-use-tunnels","title":"Always Use Tunnels","text":"<p>Instead of exposing services publicly, use SSH tunnels:</p> <pre><code>\u2705 http://localhost:8000  # Through tunnel\n\u274c http://123.45.67.89:8000  # Publicly exposed\n</code></pre>"},{"location":"guides/ssh-tunneling/#stop-tunnels-when-done","title":"Stop Tunnels When Done","text":"<p>Free up ports and resources:</p> <pre><code>soong tunnel stop\n</code></pre>"},{"location":"guides/ssh-tunneling/#check-status-before-starting","title":"Check Status Before Starting","text":"<p>Avoid port conflicts:</p> <pre><code>soong tunnel status\n# If running, stop first\nsoong tunnel stop\nsoong tunnel start\n</code></pre>"},{"location":"guides/ssh-tunneling/#use-instance-specific-tunnels","title":"Use Instance-Specific Tunnels","text":"<p>When running multiple instances, specify which one:</p> <pre><code>soong tunnel start --instance-id a1b2c3d4\n</code></pre>"},{"location":"guides/ssh-tunneling/#next-steps","title":"Next Steps","text":"<ul> <li>Launch an instance to tunnel to</li> <li>Manage leases to keep tunnels alive</li> <li>Cost optimization to manage your GPU spending</li> </ul>"},{"location":"reference/","title":"Reference Documentation","text":"<p>Complete technical reference for the <code>soong</code> CLI tool.</p>"},{"location":"reference/#documentation-sections","title":"Documentation Sections","text":""},{"location":"reference/#cli-commands","title":"CLI Commands","text":"<p>Comprehensive command reference with all flags, options, and examples.</p>"},{"location":"reference/#configuration-file","title":"Configuration File","text":"<p>YAML configuration file schema, settings, and examples.</p>"},{"location":"reference/#model-registry","title":"Model Registry","text":"<p>Built-in AI models with VRAM requirements, quantization, and HuggingFace paths.</p>"},{"location":"reference/#gpu-types","title":"GPU Types","text":"<p>Available GPU types with VRAM specifications and pricing information.</p>"},{"location":"reference/#quick-links","title":"Quick Links","text":"<ul> <li>Getting Started: See the Quick Start Guide</li> <li>Configuration: Run <code>soong configure</code> or see configuration reference</li> <li>Model Selection: Use <code>soong models</code> or see model registry</li> <li>Available GPUs: Check <code>soong available</code> or see GPU types</li> </ul>"},{"location":"reference/#configuration-location","title":"Configuration Location","text":"<p>The configuration file is stored at:</p> <pre><code>~/.config/gpu-dashboard/config.yaml\n</code></pre> <p>File permissions are automatically set to <code>0600</code> (owner read/write only) for security.</p>"},{"location":"reference/#api-integration","title":"API Integration","text":"<p><code>soong</code> uses the Lambda Labs API for instance management. You'll need a Lambda Labs API key to use this tool.</p>"},{"location":"reference/#version-information","title":"Version Information","text":"<p>This reference documentation is for <code>soong</code> version 0.1.0+.</p>"},{"location":"reference/cli-commands/","title":"CLI Commands Reference","text":"<p>Complete reference for all <code>soong</code> commands, flags, and options.</p>"},{"location":"reference/cli-commands/#global-options","title":"Global Options","text":"<p>All commands support these global options:</p> Flag Description <code>--help</code> Show help message and exit"},{"location":"reference/cli-commands/#core-commands","title":"Core Commands","text":""},{"location":"reference/cli-commands/#configure","title":"<code>configure</code>","text":"<p>Interactive configuration wizard to set up Lambda Labs credentials and defaults.</p> <pre><code>soong configure\n</code></pre> <p>What it does:</p> <ol> <li>Prompts for Lambda API key (validates it)</li> <li>Generates or accepts status daemon token</li> <li>Helps select default model (shows VRAM requirements)</li> <li>Helps select default GPU type (shows pricing and availability)</li> <li>Selects default region</li> <li>Sets persistent filesystem name</li> <li>Sets default lease duration (with cost estimates)</li> <li>Configures SSH key path</li> </ol> <p>No flags or options - fully interactive.</p> <p>Example:</p> <pre><code>$ soong configure\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Soong Configuration Wizard           \u2502\n\u2502                                             \u2502\n\u2502 This will guide you through setting up     \u2502\n\u2502 your Lambda Labs credentials and defaults. \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\nLambda API key: sk_****************************\n\u2713 API key valid.\n\nDefault model: DeepSeek-R1 70B (70B INT4) - needs 40GB+ VRAM\nGPU type: 1x A100 SXM4 (80 GB) - $1.29/hr (available) \u27f5 RECOMMENDED\nDefault region: us-west-1\nFilesystem name: coding-stack\nDefault lease duration: 4 hours ($5.16)\nSSH private key path: ~/.ssh/id_rsa\n\nConfiguration saved!\n</code></pre>"},{"location":"reference/cli-commands/#start","title":"<code>start</code>","text":"<p>Launch a new GPU instance with specified model and configuration.</p> <pre><code>soong start [OPTIONS]\n</code></pre> <p>Options:</p> Flag Type Default Description <code>--model TEXT</code> String Config default Model to load (overrides default) <code>--gpu TEXT</code> String Config default GPU type (overrides default) <code>--region TEXT</code> String Config default Region (overrides default) <code>--hours INTEGER</code> Integer Config default Lease duration in hours <code>--name TEXT</code> String None Custom instance name <code>--wait / --no-wait</code> Boolean True Wait for instance to be ready <code>-y, --yes</code> Boolean False Skip cost confirmation <p>Examples:</p> <pre><code># Start with defaults\nsoong start\n\n# Start with specific model\nsoong start --model qwen2.5-coder-32b\n\n# Start with custom GPU and skip confirmation\nsoong start --gpu gpu_1x_h100_pcie --yes\n\n# Start with all custom options\nsoong start \\\n  --model deepseek-r1-70b \\\n  --gpu gpu_1x_a100_sxm4_80gb \\\n  --region us-east-1 \\\n  --hours 6 \\\n  --name my-coding-session\n\n# Quick start without waiting\nsoong start --no-wait\n</code></pre> <p>Output:</p> <pre><code>Preparing to launch instance...\n  Model: deepseek-r1-70b\n  GPU: gpu_1x_a100_sxm4_80gb\n  Region: us-west-1\n  Lease: 4 hours\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Launch Instance             \u2502\n\u2502                             \u2502\n\u2502 GPU: 1x A100 SXM4 (80 GB)  \u2502\n\u2502 Rate: $1.29/hr              \u2502\n\u2502 Duration: 4 hours           \u2502\n\u2502                             \u2502\n\u2502 Estimated cost: $5.16       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\n? Proceed with launch? Yes\n\nLaunching instance...\nInstance launched: i-abc123def456\nInstance ready at 203.0.113.42\n\nSSH: soong ssh\nStatus: soong status\n</code></pre>"},{"location":"reference/cli-commands/#status","title":"<code>status</code>","text":"<p>Show status of running instances with uptime, cost, and lease information.</p> <pre><code>soong status [OPTIONS]\n</code></pre> <p>Options:</p> Flag Type Default Description <code>--instance-id TEXT</code> String Active instance Specific instance ID to check <code>-h, --history</code> Boolean False Show termination history <code>-s, --stopped</code> Boolean False Show stopped instances <code>--history-hours INTEGER</code> Integer 24 Hours of history to show <code>--worker-url TEXT</code> String None Cloudflare Worker URL for remote history <p>Examples:</p> <pre><code># Show running instances\nsoong status\n\n# Show specific instance\nsoong status --instance-id i-abc123\n\n# Show termination history (last 24 hours)\nsoong status --history\n\n# Show last 48 hours of history\nsoong status --history --history-hours 48\n\n# Show stopped instances\nsoong status --stopped\n\n# Sync history from Cloudflare Worker\nsoong status --history --worker-url https://worker.example.com\n</code></pre> <p>Output:</p> <pre><code>GPU Instances\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ID     \u2503 Name   \u2503 Status \u2503 IP          \u2503 GPU            \u2503 Uptime \u2503 Time Left\u2503 Cost Now \u2503 Est.Total\u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 abc123 \u2502 coding \u2502 active \u2502 203.0.113.42\u2502 gpu_1x_a100... \u2502 2h 15m \u2502 1h 45m   \u2502 $2.91    \u2502 $5.16    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reference/cli-commands/#extend","title":"<code>extend</code>","text":"<p>Extend the lease duration of a running instance.</p> <pre><code>soong extend HOURS [OPTIONS]\n</code></pre> <p>Arguments:</p> Argument Type Required Description <code>HOURS</code> Integer Yes Hours to extend lease by <p>Options:</p> Flag Type Default Description <code>--instance-id TEXT</code> String Active instance Instance to extend <code>-y, --yes</code> Boolean False Skip cost confirmation <p>Examples:</p> <pre><code># Extend active instance by 2 hours\nsoong extend 2\n\n# Extend specific instance\nsoong extend 3 --instance-id i-abc123\n\n# Quick extend without confirmation\nsoong extend 1 --yes\n</code></pre> <p>Output:</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Extend Lease                \u2502\n\u2502                             \u2502\n\u2502 Instance: abc123            \u2502\n\u2502 GPU: 1x A100 SXM4 (80 GB)  \u2502\n\u2502 Rate: $1.29/hr              \u2502\n\u2502 Extension: 2 hours          \u2502\n\u2502                             \u2502\n\u2502 Additional cost: $2.58      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\n? Extend lease by 2 hours? Yes\n\nLease extended by 2 hours\nNew shutdown time: 2025-01-01T18:30:00Z\n</code></pre>"},{"location":"reference/cli-commands/#stop","title":"<code>stop</code>","text":"<p>Terminate a running instance.</p> <pre><code>soong stop [OPTIONS]\n</code></pre> <p>Options:</p> Flag Type Default Description <code>--instance-id TEXT</code> String Active instance Instance to terminate <code>-y, --yes</code> Boolean False Skip confirmation <p>Examples:</p> <pre><code># Stop active instance (with confirmation)\nsoong stop\n\n# Stop specific instance\nsoong stop --instance-id i-abc123\n\n# Force stop without confirmation\nsoong stop --yes\n</code></pre>"},{"location":"reference/cli-commands/#ssh","title":"<code>ssh</code>","text":"<p>SSH into a running instance.</p> <pre><code>soong ssh [OPTIONS]\n</code></pre> <p>Options:</p> Flag Type Default Description <code>--instance-id TEXT</code> String Active instance Instance to connect to <p>Examples:</p> <pre><code># SSH to active instance\nsoong ssh\n\n# SSH to specific instance\nsoong ssh --instance-id i-abc123\n</code></pre> <p>Notes:</p> <ul> <li>Uses SSH key path from configuration</li> <li>Connects as <code>ubuntu</code> user</li> <li>Opens interactive SSH session</li> </ul>"},{"location":"reference/cli-commands/#available","title":"<code>available</code>","text":"<p>Show available GPU types and their current capacity.</p> <pre><code>soong available\n</code></pre> <p>No options - displays all GPU types with availability.</p> <p>Output:</p> <pre><code>Available GPU Types\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 GPU Type              \u2503 Regions           \u2503 Available \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 gpu_1x_a100_sxm4_80gb \u2502 us-west-1, us-... \u2502 Yes       \u2502\n\u2502 gpu_1x_h100_pcie      \u2502 us-east-1         \u2502 Yes       \u2502\n\u2502 gpu_1x_a6000          \u2502 -                 \u2502 No        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nRecommended Models:\n  deepseek-r1-70b (requires A100 80GB)\n  qwen2.5-coder-32b (works on RTX 6000)\n</code></pre>"},{"location":"reference/cli-commands/#models-subcommand","title":"Models Subcommand","text":"<p>Commands for managing AI models.</p>"},{"location":"reference/cli-commands/#models-list","title":"<code>models</code> (list)","text":"<p>List all available models with VRAM requirements.</p> <pre><code>soong models\n</code></pre> <p>Output:</p> <pre><code>Available Models\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 ID                  \u2503 Params \u2503 Quant \u2503 VRAM  \u2503 Min GPU            \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 llama-3.1-8b        \u2502    8B  \u2502  FP16 \u2502 18GB  \u2502 1x A10 (24 GB)     \u2502\n\u2502 mistral-7b          \u2502    7B  \u2502  FP16 \u2502 20GB  \u2502 1x A10 (24 GB)     \u2502\n\u2502 qwen2.5-coder-32b-..\u2502   32B  \u2502  INT4 \u2502 22GB  \u2502 1x A10 (24 GB)     \u2502\n\u2502 codellama-34b       \u2502   34B  \u2502  FP16 \u2502 73GB  \u2502 1x A100 SXM4 (80..)\u2502\n\u2502 qwen2.5-coder-32b   \u2502   32B  \u2502  FP16 \u2502 69GB  \u2502 1x A100 SXM4 (80..)\u2502\n\u2502 deepseek-r1-70b     \u2502   70B  \u2502  INT4 \u2502 44GB  \u2502 1x A100 SXM4 (80..)\u2502\n\u2502 llama-3.1-70b       \u2502   70B  \u2502  INT4 \u2502 44GB  \u2502 1x A100 SXM4 (80..)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nCustom models: 0 configured\n</code></pre>"},{"location":"reference/cli-commands/#models-info","title":"<code>models info</code>","text":"<p>Display detailed information about a specific model.</p> <pre><code>soong models info MODEL_ID\n</code></pre> <p>Arguments:</p> Argument Type Required Description <code>MODEL_ID</code> String Yes Model ID to display info for <p>Examples:</p> <pre><code># Show DeepSeek-R1 details\nsoong models info deepseek-r1-70b\n\n# Show Qwen2.5 Coder details\nsoong models info qwen2.5-coder-32b\n</code></pre> <p>Output:</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 DeepSeek-R1 70B             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\nHuggingFace Path: deepseek-ai/DeepSeek-R1-Distill-Llama-70B\nParameters: 70B\nQuantization: INT4\nContext Length: 8,192 tokens\n\nVRAM Breakdown:\n  Base weights:       35.0 GB\n  KV cache:            4.0 GB\n  Overhead:            2.0 GB\n  Activations:         3.5 GB\n  Total estimated:    44.5 GB\n\nRecommended GPU: 1x A100 SXM4 (80 GB)\n  Price: $1.29/hr\n\nGood for:\n  \u2022 Complex multi-step reasoning\n  \u2022 Debugging difficult issues\n  \u2022 Architecture decisions\n  \u2022 Code review with explanations\n\nNot good for:\n  \u2022 Simple/quick tasks (overkill)\n  \u2022 Long context windows (8K limit)\n  \u2022 Speed-critical applications\n\nNotes: Chain-of-thought reasoning. Slower but more accurate.\n</code></pre>"},{"location":"reference/cli-commands/#models-add","title":"<code>models add</code>","text":"<p>Add a custom model to configuration.</p> <pre><code>soong models add [OPTIONS]\n</code></pre> <p>Interactive Mode (no flags):</p> <pre><code>soong models add\n</code></pre> <p>Prompts for: name, HuggingFace path, parameters, quantization, context length.</p> <p>Flag Mode (all required):</p> Flag Type Required Description <code>--name TEXT</code> String Yes Model name/ID <code>--hf-path TEXT</code> String Yes HuggingFace model path <code>--params FLOAT</code> Float Yes Parameter count in billions <code>--quantization TEXT</code> String Yes <code>fp32</code>, <code>fp16</code>, <code>int8</code>, or <code>int4</code> <code>--context INTEGER</code> Integer Yes Context length in tokens <p>Examples:</p> <pre><code># Interactive mode\nsoong models add\n\n# Flag mode\nsoong models add \\\n  --name my-custom-70b \\\n  --hf-path myorg/custom-model-70b \\\n  --params 70 \\\n  --quantization int4 \\\n  --context 8192\n</code></pre>"},{"location":"reference/cli-commands/#models-remove","title":"<code>models remove</code>","text":"<p>Remove a custom model from configuration.</p> <pre><code>soong models remove MODEL_ID [OPTIONS]\n</code></pre> <p>Arguments:</p> Argument Type Required Description <code>MODEL_ID</code> String Yes Custom model ID to remove <p>Options:</p> Flag Type Default Description <code>-y, --yes</code> Boolean False Skip confirmation <p>Examples:</p> <pre><code># Remove with confirmation\nsoong models remove my-custom-model\n\n# Quick remove\nsoong models remove my-custom-model --yes\n</code></pre> <p>Notes:</p> <ul> <li>Cannot remove built-in models</li> <li>Only removes from local configuration (doesn't delete files)</li> </ul>"},{"location":"reference/cli-commands/#tunnel-subcommand","title":"Tunnel Subcommand","text":"<p>Commands for SSH tunnel management.</p>"},{"location":"reference/cli-commands/#tunnel-start","title":"<code>tunnel start</code>","text":"<p>Start SSH tunnel to instance with port forwarding.</p> <pre><code>soong tunnel start [OPTIONS]\n</code></pre> <p>Options:</p> Flag Type Default Description <code>--instance-id TEXT</code> String Active Instance to tunnel to <code>--sglang-port INTEGER</code> Integer 8000 Local port for SGLang API <code>--n8n-port INTEGER</code> Integer 5678 Local port for n8n web UI <code>--status-port INTEGER</code> Integer 8080 Local port for status daemon <p>Examples:</p> <pre><code># Start with defaults\nsoong tunnel start\n\n# Custom local ports\nsoong tunnel start --sglang-port 9000 --n8n-port 6000\n\n# Tunnel to specific instance\nsoong tunnel start --instance-id i-abc123\n</code></pre> <p>Port Mappings:</p> Service Remote Port Default Local Port SGLang API 8000 8000 n8n Web UI 5678 5678 Status Daemon 8080 8080"},{"location":"reference/cli-commands/#tunnel-stop","title":"<code>tunnel stop</code>","text":"<p>Stop the active SSH tunnel.</p> <pre><code>soong tunnel stop\n</code></pre> <p>No options.</p>"},{"location":"reference/cli-commands/#tunnel-status","title":"<code>tunnel status</code>","text":"<p>Check if SSH tunnel is running.</p> <pre><code>soong tunnel status\n</code></pre> <p>No options.</p> <p>Output:</p> <pre><code>Tunnel is running\n</code></pre> <p>or</p> <pre><code>Tunnel is not running\n</code></pre>"},{"location":"reference/cli-commands/#exit-codes","title":"Exit Codes","text":"Code Meaning <code>0</code> Success <code>1</code> General error (API error, instance not found, etc.) <code>2</code> Invalid arguments or missing required flags"},{"location":"reference/cli-commands/#environment-variables","title":"Environment Variables","text":"<p>Currently, <code>soong</code> does not use environment variables. All configuration is in <code>~/.config/gpu-dashboard/config.yaml</code>.</p>"},{"location":"reference/cli-commands/#shell-completion","title":"Shell Completion","text":"<p>Shell completion is not currently enabled. This may be added in future versions.</p>"},{"location":"reference/cli-commands/#aliases-and-shortcuts","title":"Aliases and Shortcuts","text":"<p>Commonly used command shortcuts:</p> <pre><code># Quick status check\nalias gs='soong status'\n\n# Quick SSH\nalias gsh='soong ssh'\n\n# Start and SSH in one go\nsoong start &amp;&amp; soong ssh\n\n# Extend by 1 hour without confirmation\nsoong extend 1 -y\n</code></pre>"},{"location":"reference/configuration-file/","title":"Configuration File Reference","text":"<p>Complete reference for the <code>soong</code> YAML configuration file.</p>"},{"location":"reference/configuration-file/#file-location","title":"File Location","text":"<pre><code>~/.config/gpu-dashboard/config.yaml\n</code></pre>"},{"location":"reference/configuration-file/#file-permissions","title":"File Permissions","text":"<p>The configuration file is automatically created with <code>0600</code> permissions (owner read/write only) to protect sensitive data like API keys.</p> <pre><code># Check permissions\nls -l ~/.config/gpu-dashboard/config.yaml\n# Should show: -rw------- 1 user user ...\n</code></pre>"},{"location":"reference/configuration-file/#configuration-schema","title":"Configuration Schema","text":""},{"location":"reference/configuration-file/#complete-example","title":"Complete Example","text":"<pre><code># Lambda Labs API Configuration\nlambda:\n  api_key: sk_1234567890abcdef1234567890abcdef\n  default_region: us-west-1\n  filesystem_name: coding-stack\n\n# Status Daemon Configuration\nstatus_daemon:\n  token: abc123xyz789_secure_token_here\n  port: 8080\n\n# Default Session Settings\ndefaults:\n  model: deepseek-r1-70b\n  gpu: gpu_1x_a100_sxm4_80gb\n  lease_hours: 4\n\n# SSH Configuration\nssh:\n  key_path: ~/.ssh/id_rsa\n\n# Custom Models (optional)\ncustom_models:\n  my-custom-model:\n    hf_path: myorg/my-model-70b\n    params_billions: 70\n    quantization: int4\n    context_length: 8192\n    notes: My custom fine-tuned model\n</code></pre>"},{"location":"reference/configuration-file/#section-reference","title":"Section Reference","text":""},{"location":"reference/configuration-file/#lambda","title":"<code>lambda</code>","text":"<p>Lambda Labs API credentials and defaults.</p> Field Type Required Default Description <code>api_key</code> String Yes - Lambda Labs API key (starts with <code>sk_</code>) <code>default_region</code> String No <code>us-west-1</code> Default region for launching instances <code>filesystem_name</code> String No <code>coding-stack</code> Persistent filesystem to attach <p>Example:</p> <pre><code>lambda:\n  api_key: sk_1234567890abcdef1234567890abcdef\n  default_region: us-east-1\n  filesystem_name: my-projects\n</code></pre> <p>Notes:</p> <ul> <li>Get your API key at: https://cloud.lambdalabs.com/api-keys</li> <li>Available regions: <code>us-west-1</code>, <code>us-east-1</code>, <code>us-south-1</code>, <code>us-midwest-1</code>, <code>europe-central-1</code>, <code>asia-northeast-1</code>, etc.</li> <li>Filesystem must exist in your Lambda Labs account</li> </ul>"},{"location":"reference/configuration-file/#status_daemon","title":"<code>status_daemon</code>","text":"<p>Configuration for the status daemon running on instances.</p> Field Type Required Default Description <code>token</code> String Yes - Shared secret for daemon authentication <code>port</code> Integer No <code>8080</code> Port daemon listens on <p>Example:</p> <pre><code>status_daemon:\n  token: my_secure_random_token_here\n  port: 8080\n</code></pre> <p>Notes:</p> <ul> <li>Token is auto-generated during <code>configure</code> if left blank</li> <li>Use a long random string (at least 32 characters)</li> <li>Token authenticates CLI commands like <code>extend</code></li> <li>Port must not conflict with other services (SGLang uses 8000, n8n uses 5678)</li> </ul>"},{"location":"reference/configuration-file/#defaults","title":"<code>defaults</code>","text":"<p>Default settings for new instances.</p> Field Type Required Default Description <code>model</code> String No <code>deepseek-r1-70b</code> Default model to load <code>gpu</code> String No <code>gpu_1x_a100_sxm4_80gb</code> Default GPU type <code>lease_hours</code> Integer No <code>4</code> Default lease duration in hours <p>Example:</p> <pre><code>defaults:\n  model: qwen2.5-coder-32b\n  gpu: gpu_1x_a6000\n  lease_hours: 6\n</code></pre> <p>Valid GPU Types:</p> <p>See GPU Types Reference for complete list.</p> <p>Valid Models:</p> <p>See Model Registry for built-in models, or use custom model IDs from <code>custom_models</code> section.</p> <p>Lease Hours:</p> <ul> <li>Minimum: 1 hour</li> <li>Maximum: 8 hours (Lambda Labs limit)</li> <li>Can be extended later with <code>soong extend</code></li> </ul>"},{"location":"reference/configuration-file/#ssh","title":"<code>ssh</code>","text":"<p>SSH connection settings.</p> Field Type Required Default Description <code>key_path</code> String No <code>~/.ssh/id_rsa</code> Path to SSH private key <p>Example:</p> <pre><code>ssh:\n  key_path: ~/.ssh/lambda_labs_key\n</code></pre> <p>Notes:</p> <ul> <li>Key must be registered in your Lambda Labs account: https://cloud.lambdalabs.com/ssh-keys</li> <li>Supports <code>~</code> expansion for home directory</li> <li>Key must have correct permissions (0600)</li> </ul>"},{"location":"reference/configuration-file/#custom_models","title":"<code>custom_models</code>","text":"<p>Define custom models not in the built-in registry.</p> Field Type Required Description <code>hf_path</code> String Yes HuggingFace model path <code>params_billions</code> Float Yes Parameter count in billions <code>quantization</code> String Yes Quantization: <code>fp32</code>, <code>fp16</code>, <code>bf16</code>, <code>int8</code>, <code>int4</code> <code>context_length</code> Integer Yes Context window size (\u2265 512) <code>name</code> String No Model name for display <code>notes</code> String No Description or usage notes <p>Example:</p> <pre><code>custom_models:\n  my-llama-70b-finetune:\n    hf_path: myorg/llama-70b-custom\n    params_billions: 70\n    quantization: int4\n    context_length: 8192\n    name: My Custom Llama 70B\n    notes: Fine-tuned on domain-specific data\n\n  small-test-model:\n    hf_path: test/tiny-model\n    params_billions: 1.5\n    quantization: fp16\n    context_length: 2048\n</code></pre> <p>Quantization Values:</p> Value Bytes/Param Use Case <code>fp32</code> 4.0 Maximum precision (rarely needed) <code>fp16</code> 2.0 Standard precision, good quality <code>bf16</code> 2.0 Brain float, similar to FP16 <code>int8</code> 1.0 Quantized, some quality loss <code>int4</code> 0.5 GPTQ/AWQ, 2x memory savings <p>Managing Custom Models:</p> <pre><code># Add via CLI (interactive)\nsoong models add\n\n# Add via CLI (flags)\nsoong models add --name my-model --hf-path org/model --params 70 --quantization int4 --context 8192\n\n# Remove custom model\nsoong models remove my-model\n\n# List all models (shows custom ones too)\nsoong models\n</code></pre>"},{"location":"reference/configuration-file/#validation-rules","title":"Validation Rules","text":"<p>The configuration is validated on load:</p>"},{"location":"reference/configuration-file/#api-key","title":"API Key","text":"<ul> <li>Must be non-empty string</li> <li>Typically starts with <code>sk_</code></li> <li>Validated by attempting API call during <code>configure</code></li> </ul>"},{"location":"reference/configuration-file/#status-daemon-token","title":"Status Daemon Token","text":"<ul> <li>Must be non-empty string</li> <li>Recommended: at least 32 characters</li> <li>Auto-generated during <code>configure</code> using <code>secrets.token_urlsafe(32)</code></li> </ul>"},{"location":"reference/configuration-file/#gpu-type","title":"GPU Type","text":"<ul> <li>Must match Lambda Labs GPU names</li> <li>Checked against available instance types</li> </ul>"},{"location":"reference/configuration-file/#lease-hours","title":"Lease Hours","text":"<ul> <li>Must be integer</li> <li>Between 1 and 8 hours (Lambda Labs limit)</li> </ul>"},{"location":"reference/configuration-file/#custom-model-fields","title":"Custom Model Fields","text":"<ul> <li><code>params_billions</code>: Must be positive number</li> <li><code>context_length</code>: Must be \u2265 512</li> <li><code>quantization</code>: Must be one of: <code>fp32</code>, <code>fp16</code>, <code>bf16</code>, <code>int8</code>, <code>int4</code></li> </ul>"},{"location":"reference/configuration-file/#example-configurations","title":"Example Configurations","text":""},{"location":"reference/configuration-file/#minimal-configuration","title":"Minimal Configuration","text":"<pre><code>lambda:\n  api_key: sk_1234567890abcdef1234567890abcdef\n\nstatus_daemon:\n  token: auto_generated_token_here\n</code></pre> <p>All other fields use defaults.</p>"},{"location":"reference/configuration-file/#budget-conscious-setup","title":"Budget-Conscious Setup","text":"<pre><code>lambda:\n  api_key: sk_1234567890abcdef1234567890abcdef\n  default_region: us-west-1\n  filesystem_name: shared-workspace\n\nstatus_daemon:\n  token: my_secure_token\n\ndefaults:\n  model: llama-3.1-8b          # Cheapest model\n  gpu: gpu_1x_a10              # Cheapest GPU\n  lease_hours: 2               # Short sessions\n\nssh:\n  key_path: ~/.ssh/id_rsa\n</code></pre>"},{"location":"reference/configuration-file/#power-user-setup","title":"Power User Setup","text":"<pre><code>lambda:\n  api_key: sk_1234567890abcdef1234567890abcdef\n  default_region: us-east-1\n  filesystem_name: ml-projects\n\nstatus_daemon:\n  token: super_secure_64_char_token_here_for_production_use_12345678\n  port: 8080\n\ndefaults:\n  model: deepseek-r1-70b       # Best reasoning\n  gpu: gpu_1x_h100_pcie        # Fastest GPU\n  lease_hours: 8               # Maximum allowed\n\nssh:\n  key_path: ~/.ssh/lambda_dedicated\n\ncustom_models:\n  my-finetune-70b:\n    hf_path: myorg/llama-70b-code-finetune\n    params_billions: 70\n    quantization: int4\n    context_length: 16384\n    notes: Fine-tuned on internal codebase\n\n  test-model:\n    hf_path: test/small-debug-model\n    params_billions: 7\n    quantization: fp16\n    context_length: 4096\n</code></pre>"},{"location":"reference/configuration-file/#editing-configuration","title":"Editing Configuration","text":""},{"location":"reference/configuration-file/#manual-editing","title":"Manual Editing","text":"<pre><code># Open in default editor\n${EDITOR:-nano} ~/.config/gpu-dashboard/config.yaml\n\n# Validate after editing\nsoong status\n</code></pre>"},{"location":"reference/configuration-file/#via-cli","title":"Via CLI","text":"<pre><code># Re-run configuration wizard\nsoong configure\n\n# Add custom models\nsoong models add\n\n# Remove custom models\nsoong models remove my-model\n</code></pre>"},{"location":"reference/configuration-file/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/configuration-file/#invalid-api-key","title":"Invalid API Key","text":"<p>Symptom: <code>Error: Invalid API key</code></p> <p>Solution:</p> <ol> <li>Check key at: https://cloud.lambdalabs.com/api-keys</li> <li>Ensure no extra whitespace in YAML</li> <li>Re-run <code>soong configure</code></li> </ol>"},{"location":"reference/configuration-file/#filesystem-not-found","title":"Filesystem Not Found","text":"<p>Symptom: Launch fails with filesystem error</p> <p>Solution:</p> <ol> <li>Check filesystem name at: https://cloud.lambdalabs.com/file-systems</li> <li>Update <code>lambda.filesystem_name</code> in config</li> <li>Or create filesystem in Lambda Labs dashboard</li> </ol>"},{"location":"reference/configuration-file/#ssh-permission-denied","title":"SSH Permission Denied","text":"<p>Symptom: Cannot SSH to instance</p> <p>Solution:</p> <ol> <li>Check SSH key registered: https://cloud.lambdalabs.com/ssh-keys</li> <li>Verify <code>ssh.key_path</code> points to correct private key</li> <li>Check key permissions: <code>chmod 600 ~/.ssh/id_rsa</code></li> </ol>"},{"location":"reference/configuration-file/#invalid-custom-model","title":"Invalid Custom Model","text":"<p>Symptom: <code>Warning: Invalid custom model</code></p> <p>Solution:</p> <ol> <li>Check all required fields present</li> <li>Verify <code>quantization</code> is valid value</li> <li>Ensure <code>params_billions</code> is positive number</li> <li>Ensure <code>context_length</code> \u2265 512</li> </ol>"},{"location":"reference/configuration-file/#security-best-practices","title":"Security Best Practices","text":"<ol> <li> <p>Never commit config to git <pre><code>echo \"~/.config/gpu-dashboard/config.yaml\" &gt;&gt; ~/.gitignore\n</code></pre></p> </li> <li> <p>Use environment variables for CI/CD <pre><code># Don't store API keys in config for CI\n# Use Lambda Labs API directly or secure secrets management\n</code></pre></p> </li> <li> <p>Rotate tokens periodically</p> </li> <li>Regenerate <code>status_daemon.token</code> every few months</li> <li> <p>Update instances with new token</p> </li> <li> <p>Restrict file permissions <pre><code>chmod 600 ~/.config/gpu-dashboard/config.yaml\n</code></pre></p> </li> <li> <p>Use dedicated SSH keys</p> </li> <li>Don't use your primary SSH key</li> <li>Create Lambda-specific key: <code>ssh-keygen -t ed25519 -f ~/.ssh/lambda_labs</code></li> </ol>"},{"location":"reference/gpu-types/","title":"GPU Types Reference","text":"<p>Complete reference for Lambda Labs GPU types available through <code>soong</code>.</p>"},{"location":"reference/gpu-types/#overview","title":"Overview","text":"<p>Lambda Labs offers GPUs from NVIDIA's professional and data center lines:</p> <ul> <li>A10: Entry-level, 24 GB VRAM</li> <li>A6000/RTX 6000: Workstation class, 48 GB VRAM</li> <li>A100: Data center standard, 40-80 GB VRAM</li> <li>H100: Latest generation, 80 GB VRAM with faster performance</li> </ul>"},{"location":"reference/gpu-types/#gpu-comparison-table","title":"GPU Comparison Table","text":"GPU Type GPU VRAM vCPUs RAM Storage Est. Price/hr <code>gpu_1x_a10</code> 1x A10 24 GB 30 200 GB 1.4 TB $0.60 <code>gpu_1x_a6000</code> 1x A6000 48 GB 28 200 GB 512 GB $0.80 <code>gpu_1x_rtx6000</code> 1x RTX 6000 Ada 48 GB 30 200 GB 1.4 TB $0.80 <code>gpu_1x_a100</code> 1x A100 PCIe 40 GB 30 200 GB 512 GB $1.10 <code>gpu_1x_a100_sxm4</code> 1x A100 SXM4 40 GB 30 200 GB 512 GB $1.10 <code>gpu_1x_a100_sxm4_80gb</code> 1x A100 SXM4 80 GB 30 200 GB 1.4 TB $1.29 <code>gpu_1x_h100_pcie</code> 1x H100 PCIe 80 GB 26 200 GB 512 GB $1.99 <code>gpu_1x_h100_sxm5</code> 1x H100 SXM5 80 GB 52 1000 GB 2 TB $2.49 <code>gpu_2x_a100</code> 2x A100 SXM4 80 GB (2\u00d740) 48 900 GB 6 TB $2.20 <code>gpu_4x_a100</code> 4x A100 SXM4 160 GB (4\u00d740) 120 1800 GB 14 TB $4.40 <code>gpu_8x_a100</code> 8x A100 SXM4 320 GB (8\u00d740) 240 1800 GB 14 TB $8.80 <code>gpu_8x_h100</code> 8x H100 SXM5 640 GB (8\u00d780) 416 7800 GB 30 TB $19.92 <p>Pricing</p> <p>Prices are approximate and may vary by region and availability. Check Lambda Labs dashboard for current pricing.</p>"},{"location":"reference/gpu-types/#single-gpu-instances","title":"Single GPU Instances","text":""},{"location":"reference/gpu-types/#a10-24-gb-budget-option","title":"A10 (24 GB) - Budget Option","text":"<pre><code>Type: gpu_1x_a10\nGPU: 1x NVIDIA A10\nVRAM: 24 GB\nPrice: ~$0.60/hr\n</code></pre> <p>Best For: - Small models (7-8B parameters) - Quantized 32B models (INT4) - Development and testing - Cost-conscious workloads</p> <p>Compatible Models: - \u2705 Llama 3.1 8B (FP16) - \u2705 Mistral 7B (FP16) - \u2705 Qwen 2.5 Coder 32B (INT4)</p> <p>Regions: Usually good availability in <code>us-west-1</code>, <code>us-east-1</code></p>"},{"location":"reference/gpu-types/#a6000-rtx-6000-ada-48-gb-mid-range","title":"A6000 / RTX 6000 Ada (48 GB) - Mid-Range","text":"<pre><code>Type: gpu_1x_a6000 or gpu_1x_rtx6000\nGPU: 1x NVIDIA A6000 or RTX 6000 Ada\nVRAM: 48 GB\nPrice: ~$0.80/hr\n</code></pre> <p>Best For: - Medium models (30-70B INT4) - Small multi-GPU experiments - Professional workstation workloads</p> <p>Compatible Models: - \u2705 DeepSeek-R1 70B (INT4) - \u2705 Llama 3.1 70B (INT4) - \u2705 All smaller models</p> <p>Regions: Limited availability, check <code>soong available</code></p>"},{"location":"reference/gpu-types/#a100-40-gb-standard-data-center","title":"A100 40 GB - Standard Data Center","text":"<pre><code>Type: gpu_1x_a100 or gpu_1x_a100_sxm4\nGPU: 1x NVIDIA A100 (PCIe or SXM4)\nVRAM: 40 GB\nPrice: ~$1.10/hr\n</code></pre> <p>Best For: - 70B models with tight VRAM (INT4) - Production inference - Training small models</p> <p>Compatible Models: - \u2705 DeepSeek-R1 70B (INT4) - \u2705 Llama 3.1 70B (INT4) - \u26a0\ufe0f Qwen 2.5 Coder 32B (FP16) - tight fit</p> <p>Notes: - SXM4 has faster interconnect (useful for multi-GPU) - PCIe version has slightly lower bandwidth</p>"},{"location":"reference/gpu-types/#a100-80-gb-recommended","title":"A100 80 GB - Recommended \u2b50","text":"<pre><code>Type: gpu_1x_a100_sxm4_80gb\nGPU: 1x NVIDIA A100 SXM4\nVRAM: 80 GB\nPrice: ~$1.29/hr\n</code></pre> <p>Best For: - All 70B models (FP16 and INT4) - 32B models at full precision (FP16) - Most flexible option - Default for <code>soong</code></p> <p>Compatible Models: - \u2705 All models in registry - \u2705 Qwen 2.5 Coder 32B (FP16) - recommended - \u2705 DeepSeek-R1 70B (INT4) - \u2705 Code Llama 34B (FP16)</p> <p>Regions: Best availability across all regions</p> <p>Why Recommended: - 2x VRAM of A100 40GB for only 17% more cost - Runs all models comfortably - Good availability</p>"},{"location":"reference/gpu-types/#h100-pcie-80-gb-latest-generation","title":"H100 PCIe (80 GB) - Latest Generation","text":"<pre><code>Type: gpu_1x_h100_pcie\nGPU: 1x NVIDIA H100 PCIe\nVRAM: 80 GB\nPrice: ~$1.99/hr\n</code></pre> <p>Best For: - Faster inference (2-3x vs A100) - Production with tight latency requirements - Latest Transformer Engine features</p> <p>Compatible Models: - \u2705 Same as A100 80GB - \u26a1 2-3x faster inference</p> <p>Notes: - Worth the premium for production workloads - Not necessary for development</p>"},{"location":"reference/gpu-types/#h100-sxm5-80-gb-maximum-performance","title":"H100 SXM5 (80 GB) - Maximum Performance","text":"<pre><code>Type: gpu_1x_h100_sxm5\nGPU: 1x NVIDIA H100 SXM5\nVRAM: 80 GB\nPrice: ~$2.49/hr\nCPU: 52 vCPUs\nRAM: 1 TB\n</code></pre> <p>Best For: - Maximum single-GPU performance - Large batch sizes - NVLink for multi-GPU scaling</p> <p>Compatible Models: - \u2705 Same as A100 80GB - \u26a1 3-4x faster than A100</p> <p>Notes: - SXM5 has faster interconnect than PCIe - 2\u00d7 more CPU cores and RAM than H100 PCIe</p>"},{"location":"reference/gpu-types/#multi-gpu-instances","title":"Multi-GPU Instances","text":"<p>Multi-GPU Support</p> <p>Multi-GPU instances require distributed inference setup (e.g., vLLM, Ray). Not currently automated in <code>soong</code>.</p>"},{"location":"reference/gpu-types/#2x-a100-80-gb-total","title":"2x A100 (80 GB Total)","text":"<pre><code>Type: gpu_2x_a100\nGPUs: 2x A100 SXM4 (40 GB each)\nTotal VRAM: 80 GB\nPrice: ~$2.20/hr\n</code></pre> <p>When to Use: - Models that need 60-80 GB total - Cheaper than 1x A100 80GB? No - use single 80GB instead - Training with data parallelism</p>"},{"location":"reference/gpu-types/#4x-a100-160-gb-total","title":"4x A100 (160 GB Total)","text":"<pre><code>Type: gpu_4x_a100\nGPUs: 4x A100 SXM4 (40 GB each)\nTotal VRAM: 160 GB\nPrice: ~$4.40/hr\n</code></pre> <p>When to Use: - 70B models at FP32 precision - Large 175B+ models with quantization - Multi-GPU training</p>"},{"location":"reference/gpu-types/#8x-a100-320-gb-total","title":"8x A100 (320 GB Total)","text":"<pre><code>Type: gpu_8x_a100\nGPUs: 8x A100 SXM4 (40 GB each)\nTotal VRAM: 320 GB\nPrice: ~$8.80/hr\n</code></pre> <p>When to Use: - 175B models (FP16) - Large-scale training - Multi-user inference serving</p>"},{"location":"reference/gpu-types/#8x-h100-640-gb-total","title":"8x H100 (640 GB Total)","text":"<pre><code>Type: gpu_8x_h100\nGPUs: 8x H100 SXM5 (80 GB each)\nTotal VRAM: 640 GB\nPrice: ~$19.92/hr\nCPU: 416 vCPUs\nRAM: 7.8 TB\n</code></pre> <p>When to Use: - Largest models (400B+) - High-throughput production serving - Multi-GPU training at scale</p>"},{"location":"reference/gpu-types/#gpu-selection-guide","title":"GPU Selection Guide","text":""},{"location":"reference/gpu-types/#by-model-size","title":"By Model Size","text":"Model Size Recommended GPU Alternative 7-8B FP16 A10 (24 GB) A6000 (48 GB) 32B INT4 A10 (24 GB) A6000 (48 GB) 32B FP16 A100 (80 GB) H100 (80 GB) 70B INT4 A100 (80 GB) A6000 (48 GB) 70B FP16 2x A100 (80 GB) H100 (80 GB)"},{"location":"reference/gpu-types/#by-budget","title":"By Budget","text":"Budget/hr GPU Type Models &lt; $1 A10 (24 GB) Small models, INT4 quantized $1-2 A100 80GB All common models $2-5 H100 or 2-4x A100 Large models, fast inference $5+ 8x A100/H100 Multi-GPU production"},{"location":"reference/gpu-types/#by-use-case","title":"By Use Case","text":"Use Case Recommended Why Development/Testing A10 (24 GB) Cheapest, fast iteration Production Inference A100 80GB Best price/performance Low Latency H100 PCIe 2-3x faster inference Training Multi-GPU A100 NVLink, large batches Maximum Performance H100 SXM5 Latest tech, fastest"},{"location":"reference/gpu-types/#availability","title":"Availability","text":"<p>GPU availability varies by region and time. Check current availability:</p> <pre><code>soong available\n</code></pre>"},{"location":"reference/gpu-types/#typical-availability-by-region","title":"Typical Availability (by region)","text":"Region A10 A100 80GB H100 <code>us-west-1</code> \u2705 High \u2705 High \u26a0\ufe0f Limited <code>us-east-1</code> \u2705 High \u2705 High \u26a0\ufe0f Limited <code>us-south-1</code> \u2705 Medium \u2705 Medium \u274c Rare <code>europe-central-1</code> \u26a0\ufe0f Limited \u2705 Medium \u274c Rare <p>Availability Strategy</p> <ul> <li>A100 80GB has best availability across regions</li> <li>H100s are often scarce - check multiple regions</li> <li>A10 is usually available but may have waitlists</li> <li>Multi-GPU instances (4x, 8x) often require scheduling</li> </ul>"},{"location":"reference/gpu-types/#cost-examples","title":"Cost Examples","text":""},{"location":"reference/gpu-types/#4-hour-coding-session","title":"4-Hour Coding Session","text":"GPU Cost Suitable Models A10 $2.40 Llama 8B, Qwen 32B INT4 A100 80GB $5.16 All models H100 PCIe $7.96 All models, faster"},{"location":"reference/gpu-types/#full-day-8-hours","title":"Full Day (8 hours)","text":"GPU Cost Suitable Models A10 $4.80 Small models A100 80GB $10.32 All models H100 PCIe $15.92 All models, production"},{"location":"reference/gpu-types/#weekly-development-40-hours","title":"Weekly Development (40 hours)","text":"GPU Cost Suitable Models A10 $24 Budget coding A100 80GB $51.60 Professional use H100 PCIe $79.60 High performance"},{"location":"reference/gpu-types/#gpu-specifications-deep-dive","title":"GPU Specifications Deep Dive","text":""},{"location":"reference/gpu-types/#compute-architecture","title":"Compute Architecture","text":"GPU Architecture CUDA Cores Tensor Cores TDP A10 Ampere 9,216 288 (3<sup>rd</sup> gen) 150W A6000 Ampere 10,752 336 (3<sup>rd</sup> gen) 300W RTX 6000 Ada Ada Lovelace 18,176 568 (4<sup>th</sup> gen) 300W A100 Ampere 6,912 432 (3<sup>rd</sup> gen) 400W H100 Hopper 16,896 528 (4<sup>th</sup> gen) 700W"},{"location":"reference/gpu-types/#memory-bandwidth","title":"Memory Bandwidth","text":"GPU Memory Type Bandwidth ECC A10 GDDR6 600 GB/s Yes A6000 GDDR6 768 GB/s Yes RTX 6000 Ada GDDR6 960 GB/s Yes A100 40GB HBM2e 1,555 GB/s Yes A100 80GB HBM2e 2,039 GB/s Yes H100 HBM3 3,350 GB/s Yes"},{"location":"reference/gpu-types/#inference-performance-relative","title":"Inference Performance (relative)","text":"GPU FP16 INT8 INT4 A10 1.0\u00d7 1.0\u00d7 1.0\u00d7 A100 1.5\u00d7 2.0\u00d7 - H100 3.0\u00d7 4.0\u00d7 6.0\u00d7 <p>Performance relative to A10 baseline</p>"},{"location":"reference/gpu-types/#multi-gpu-interconnects","title":"Multi-GPU Interconnects","text":""},{"location":"reference/gpu-types/#pcie-standard","title":"PCIe (Standard)","text":"<ul> <li>Bandwidth: 64 GB/s (PCIe 4.0 x16)</li> <li>Use case: Single GPU or CPU-bound workloads</li> <li>GPUs: A10, A6000, RTX 6000, H100 PCIe</li> </ul>"},{"location":"reference/gpu-types/#nvlink-a100-sxm4","title":"NVLink (A100 SXM4)","text":"<ul> <li>Bandwidth: 600 GB/s (12 NVLink lanes)</li> <li>Use case: Multi-GPU training, large models</li> <li>GPUs: A100 SXM4 in multi-GPU configs</li> </ul>"},{"location":"reference/gpu-types/#nvlink-4-h100-sxm5","title":"NVLink 4 (H100 SXM5)","text":"<ul> <li>Bandwidth: 900 GB/s (18 NVLink lanes)</li> <li>Use case: Highest multi-GPU performance</li> <li>GPUs: H100 SXM5 in multi-GPU configs</li> </ul>"},{"location":"reference/gpu-types/#choosing-the-right-gpu","title":"Choosing the Right GPU","text":""},{"location":"reference/gpu-types/#decision-tree","title":"Decision Tree","text":"<pre><code>Need model &gt; 70B?\n\u251c\u2500 Yes \u2192 Multi-GPU or wait for larger models\n\u2514\u2500 No\n    \u251c\u2500 Need FP16 precision for 32B model?\n    \u2502   \u2514\u2500 Yes \u2192 A100 80GB\n    \u2514\u2500 No\n        \u251c\u2500 Budget &lt; $1/hr?\n        \u2502   \u2514\u2500 Yes \u2192 A10 (use INT4 models)\n        \u2514\u2500 No\n            \u251c\u2500 Need fastest inference?\n            \u2502   \u2514\u2500 Yes \u2192 H100\n            \u2514\u2500 No \u2192 A100 80GB (best value)\n</code></pre>"},{"location":"reference/gpu-types/#common-mistakes","title":"Common Mistakes","text":"<p>\u274c Using A100 40GB for Qwen 32B FP16 - Too tight, may OOM - Use A100 80GB instead</p> <p>\u274c Using H100 for development - 2\u00d7 cost for minimal benefit in dev - Use A100 80GB instead</p> <p>\u274c Using 2x A100 for 70B INT4 - Doesn't need multi-GPU - Use 1x A100 80GB instead</p> <p>\u274c Using A10 for DeepSeek-R1 FP16 - Won't fit (needs 140+ GB) - Use A100 80GB with INT4</p>"},{"location":"reference/gpu-types/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"reference/gpu-types/#can-i-use-multiple-gpus-for-one-model","title":"Can I use multiple GPUs for one model?","text":"<p>Yes, but requires setup: - vLLM supports tensor parallelism - Ray supports pipeline parallelism - Not currently automated in <code>soong</code></p>"},{"location":"reference/gpu-types/#whats-the-difference-between-pcie-and-sxm","title":"What's the difference between PCIe and SXM?","text":"<ul> <li>PCIe: Plugs into motherboard slot, lower bandwidth</li> <li>SXM: Direct socket, higher bandwidth, NVLink support</li> <li>For single GPU, minimal difference</li> <li>For multi-GPU, SXM much faster</li> </ul>"},{"location":"reference/gpu-types/#should-i-get-h100-over-a100","title":"Should I get H100 over A100?","text":"<p>Get H100 if: - Production workload with tight latency SLA - Need maximum throughput - Budget allows</p> <p>Get A100 if: - Development/testing - Cost-conscious - 2-3x inference speed sufficient</p>"},{"location":"reference/gpu-types/#why-is-a100-80gb-only-17-more-than-40gb","title":"Why is A100 80GB only 17% more than 40GB?","text":"<p>Lambda Labs prices by total cost of ownership. 80GB variant has: - 2\u00d7 VRAM - 31% higher memory bandwidth - Better availability</p> <p>It's the best value in their lineup.</p>"},{"location":"reference/gpu-types/#what-about-rtx-4090-or-consumer-gpus","title":"What about RTX 4090 or consumer GPUs?","text":"<p>Lambda Labs only offers professional/data center GPUs: - Better reliability (ECC memory) - Official driver support - Better multi-GPU scaling - Data center warranties</p>"},{"location":"reference/gpu-types/#see-also","title":"See Also","text":"<ul> <li>Model Registry - Which models fit on which GPUs</li> <li>CLI Commands - <code>available</code> and <code>start</code> commands</li> <li>Configuration - Setting default GPU type</li> <li>Lambda Labs Instance Types - Official specs</li> </ul>"},{"location":"reference/models/","title":"Model Registry Reference","text":"<p>Complete reference for built-in AI models supported by <code>soong</code>.</p>"},{"location":"reference/models/#overview","title":"Overview","text":"<p><code>soong</code> includes 7 pre-configured models optimized for different use cases. Each model includes:</p> <ul> <li>VRAM estimation - Automatic calculation of memory requirements</li> <li>GPU recommendation - Best GPU type for the model</li> <li>Quantization - Memory-efficient compression</li> <li>Use case guidance - What the model is good (and not good) for</li> </ul>"},{"location":"reference/models/#quick-model-comparison","title":"Quick Model Comparison","text":"Model Size VRAM Min GPU Best For Cost/hr Llama 3.1 8B 8B 18 GB A10 (24GB) Quick tasks, prototyping ~$0.60 Mistral 7B 7B 20 GB A10 (24GB) Long contexts, efficiency ~$0.60 Qwen 2.5 Coder 32B INT4 32B 22 GB A10 (24GB) Code on budget ~$0.60 Code Llama 34B 34B 73 GB A100 (80GB) Code completion ~$1.29 Qwen 2.5 Coder 32B 32B 69 GB A100 (80GB) Fast code generation ~$1.29 DeepSeek-R1 70B 70B 44 GB A100 (80GB) Complex reasoning ~$1.29 Llama 3.1 70B 70B 44 GB A100 (80GB) General purpose ~$1.29"},{"location":"reference/models/#built-in-models","title":"Built-in Models","text":""},{"location":"reference/models/#deepseek-r1-70b-recommended","title":"DeepSeek-R1 70B \u2b50 Recommended","text":"<p>Best for complex coding tasks requiring deep reasoning.</p> <pre><code>Model ID: deepseek-r1-70b\nHuggingFace: deepseek-ai/DeepSeek-R1-Distill-Llama-70B\nParameters: 70 billion\nQuantization: INT4 (GPTQ/AWQ)\nContext: 8,192 tokens\nEst. VRAM: 44 GB\nMin GPU: 1x A100 SXM4 (80 GB)\n</code></pre>"},{"location":"reference/models/#vram-breakdown","title":"VRAM Breakdown","text":"Component Memory Base weights 35.0 GB KV cache 4.0 GB CUDA overhead 2.0 GB Activations 3.5 GB Total 44.5 GB"},{"location":"reference/models/#use-cases","title":"Use Cases","text":"<p>Good for: - \u2705 Complex multi-step reasoning - \u2705 Debugging difficult issues - \u2705 Architecture decisions - \u2705 Code review with explanations - \u2705 Chain-of-thought problem solving</p> <p>Not good for: - \u274c Simple/quick tasks (overkill) - \u274c Long context windows (8K limit) - \u274c Speed-critical applications</p>"},{"location":"reference/models/#notes","title":"Notes","text":"<p>Chain-of-thought reasoning model. Slower but more accurate than general models. Uses reinforcement learning from human feedback (RLHF) for better reasoning.</p>"},{"location":"reference/models/#qwen-25-coder-32b","title":"Qwen 2.5 Coder 32B","text":"<p>Fast coding specialist with exceptional context length.</p> <pre><code>Model ID: qwen2.5-coder-32b\nHuggingFace: Qwen/Qwen2.5-Coder-32B-Instruct\nParameters: 32 billion\nQuantization: FP16\nContext: 32,768 tokens (4x longer than DeepSeek)\nEst. VRAM: 69 GB\nMin GPU: 1x A100 SXM4 (80 GB)\n</code></pre>"},{"location":"reference/models/#vram-breakdown_1","title":"VRAM Breakdown","text":"Component Memory Base weights 64.0 GB KV cache 4.0 GB CUDA overhead 2.0 GB Activations 6.4 GB Total 76.4 GB"},{"location":"reference/models/#use-cases_1","title":"Use Cases","text":"<p>Good for: - \u2705 Code generation and completion - \u2705 Large file refactoring (32K context) - \u2705 Fast iteration cycles - \u2705 Multiple programming languages - \u2705 Reviewing entire modules at once</p> <p>Not good for: - \u274c Complex reasoning chains - \u274c Non-coding tasks - \u274c Tasks requiring world knowledge</p>"},{"location":"reference/models/#notes_1","title":"Notes","text":"<p>Purpose-built for code. Supports 80+ programming languages. 4x longer context than DeepSeek makes it ideal for working with large files.</p>"},{"location":"reference/models/#qwen-25-coder-32b-int4","title":"Qwen 2.5 Coder 32B INT4","text":"<p>Budget-friendly quantized version of Qwen Coder.</p> <pre><code>Model ID: qwen2.5-coder-32b-int4\nHuggingFace: Qwen/Qwen2.5-Coder-32B-Instruct-AWQ\nParameters: 32 billion\nQuantization: INT4 (AWQ)\nContext: 32,768 tokens\nEst. VRAM: 22 GB\nMin GPU: 1x A10 (24 GB)\n</code></pre>"},{"location":"reference/models/#vram-breakdown_2","title":"VRAM Breakdown","text":"Component Memory Base weights 16.0 GB KV cache 4.0 GB CUDA overhead 2.0 GB Activations 1.6 GB Total 23.6 GB"},{"location":"reference/models/#use-cases_2","title":"Use Cases","text":"<p>Good for: - \u2705 Same tasks as Qwen FP16 - \u2705 Cheaper GPU requirements (A10 vs A100) - \u2705 Good quality/cost ratio - \u2705 Cost-conscious development</p> <p>Not good for: - \u274c Maximum accuracy (slight quality loss) - \u274c Tasks where FP16 precision matters</p>"},{"location":"reference/models/#notes_2","title":"Notes","text":"<p>~5% quality loss vs FP16, but runs on cheaper GPUs. Uses AWQ (Activation-aware Weight Quantization) for better quality than naive INT4.</p>"},{"location":"reference/models/#llama-31-70b","title":"Llama 3.1 70B","text":"<p>General-purpose powerhouse for diverse tasks.</p> <pre><code>Model ID: llama-3.1-70b\nHuggingFace: meta-llama/Llama-3.1-70B-Instruct\nParameters: 70 billion\nQuantization: INT4\nContext: 8,192 tokens\nEst. VRAM: 44 GB\nMin GPU: 1x A100 SXM4 (80 GB)\n</code></pre>"},{"location":"reference/models/#vram-breakdown_3","title":"VRAM Breakdown","text":"Component Memory Base weights 35.0 GB KV cache 4.0 GB CUDA overhead 2.0 GB Activations 3.5 GB Total 44.5 GB"},{"location":"reference/models/#use-cases_3","title":"Use Cases","text":"<p>Good for: - \u2705 Broad task coverage - \u2705 Instruction following - \u2705 Writing and documentation - \u2705 Code + general knowledge - \u2705 Balanced performance</p> <p>Not good for: - \u274c Pure coding (Qwen better) - \u274c Deep reasoning (DeepSeek better) - \u274c Very long contexts</p>"},{"location":"reference/models/#notes_3","title":"Notes","text":"<p>Jack of all trades. Good baseline choice when task type is unclear. Strong instruction following and safer outputs due to extensive RLHF training.</p>"},{"location":"reference/models/#llama-31-8b","title":"Llama 3.1 8B","text":"<p>Fast and economical for simple tasks.</p> <pre><code>Model ID: llama-3.1-8b\nHuggingFace: meta-llama/Llama-3.1-8B-Instruct\nParameters: 8 billion\nQuantization: FP16\nContext: 8,192 tokens\nEst. VRAM: 18 GB\nMin GPU: 1x A10 (24 GB)\n</code></pre>"},{"location":"reference/models/#vram-breakdown_4","title":"VRAM Breakdown","text":"Component Memory Base weights 16.0 GB KV cache 4.0 GB CUDA overhead 2.0 GB Activations 1.6 GB Total 23.6 GB"},{"location":"reference/models/#use-cases_4","title":"Use Cases","text":"<p>Good for: - \u2705 Quick simple tasks - \u2705 Lowest cost - \u2705 Rapid prototyping - \u2705 Simple code changes - \u2705 Testing workflows</p> <p>Not good for: - \u274c Complex reasoning - \u274c Large codebases - \u274c Nuanced decisions - \u274c Multi-step tasks</p>"},{"location":"reference/models/#notes_4","title":"Notes","text":"<p>Use when speed/cost matters more than quality. Great for testing infrastructure or simple repetitive tasks.</p>"},{"location":"reference/models/#code-llama-34b","title":"Code Llama 34B","text":"<p>Meta's dedicated coding model (older generation).</p> <pre><code>Model ID: codellama-34b\nHuggingFace: codellama/CodeLlama-34b-Instruct-hf\nParameters: 34 billion\nQuantization: FP16\nContext: 16,384 tokens\nEst. VRAM: 73 GB\nMin GPU: 1x A100 SXM4 (80 GB)\n</code></pre>"},{"location":"reference/models/#vram-breakdown_5","title":"VRAM Breakdown","text":"Component Memory Base weights 68.0 GB KV cache 4.0 GB CUDA overhead 2.0 GB Activations 6.8 GB Total 80.8 GB"},{"location":"reference/models/#use-cases_5","title":"Use Cases","text":"<p>Good for: - \u2705 Code completion - \u2705 Infilling (fill-in-the-middle) - \u2705 16K context window - \u2705 Python, Java, C++</p> <p>Not good for: - \u274c Latest techniques (2023 model) - \u274c Complex reasoning - \u274c Non-code tasks</p>"},{"location":"reference/models/#notes_5","title":"Notes","text":"<p>Older but still capable. Consider Qwen 2.5 Coder for newer alternative with better performance.</p>"},{"location":"reference/models/#mistral-7b","title":"Mistral 7B","text":"<p>Efficient model with sliding window attention.</p> <pre><code>Model ID: mistral-7b\nHuggingFace: mistralai/Mistral-7B-Instruct-v0.3\nParameters: 7 billion\nQuantization: FP16\nContext: 32,768 tokens\nEst. VRAM: 20 GB\nMin GPU: 1x A10 (24 GB)\n</code></pre>"},{"location":"reference/models/#vram-breakdown_6","title":"VRAM Breakdown","text":"Component Memory Base weights 14.0 GB KV cache 4.0 GB CUDA overhead 2.0 GB Activations 1.4 GB Total 21.4 GB"},{"location":"reference/models/#use-cases_6","title":"Use Cases","text":"<p>Good for: - \u2705 Long documents (32K context) - \u2705 Low resource usage - \u2705 Fast inference - \u2705 Sliding window attention</p> <p>Not good for: - \u274c Complex coding tasks - \u274c Tasks needing large model capacity</p>"},{"location":"reference/models/#notes_6","title":"Notes","text":"<p>Great efficiency but limited capacity. Uses sliding window attention for efficient long context processing.</p>"},{"location":"reference/models/#model-selection-guide","title":"Model Selection Guide","text":""},{"location":"reference/models/#by-use-case","title":"By Use Case","text":"Task Recommended Model Why Complex debugging DeepSeek-R1 70B Chain-of-thought reasoning Large file refactoring Qwen 2.5 Coder 32B 32K context window Budget coding Qwen 2.5 Coder 32B INT4 Good quality, cheap GPU General purpose Llama 3.1 70B Balanced performance Quick testing Llama 3.1 8B Fast and cheap Long documents Mistral 7B 32K context, efficient"},{"location":"reference/models/#by-gpu-budget","title":"By GPU Budget","text":"GPU Type VRAM Suitable Models A10 (24 GB) 24 GB Llama 8B, Mistral 7B, Qwen 32B INT4 A6000 (48 GB) 48 GB DeepSeek-R1 70B, Llama 70B A100 (80 GB) 80 GB All models, especially Qwen 32B FP16 H100 (80 GB) 80 GB All models with faster inference"},{"location":"reference/models/#by-context-length-needs","title":"By Context Length Needs","text":"Context Needed Models 8K tokens DeepSeek-R1, Llama 8B/70B 16K tokens Code Llama 34B 32K tokens Qwen 2.5 Coder, Mistral 7B"},{"location":"reference/models/#vram-estimation-formula","title":"VRAM Estimation Formula","text":"<p>For any model, VRAM is estimated as:</p> <pre><code>Total VRAM = Base + KV Cache + Overhead + Activations\n\nBase = params_billions \u00d7 bytes_per_param\nKV Cache = min(4.0 GB, context_length / 2048)\nOverhead = 2.0 GB (CUDA, framework)\nActivations = base \u00d7 0.1\n</code></pre>"},{"location":"reference/models/#quantization-levels","title":"Quantization Levels","text":"Quantization Bytes/Param Memory vs FP32 Quality Loss FP32 4.0 100% 0% (baseline) FP16/BF16 2.0 50% ~0-1% INT8 1.0 25% ~3-5% INT4 (GPTQ/AWQ) 0.5 12.5% ~5-8%"},{"location":"reference/models/#custom-models","title":"Custom Models","text":"<p>You can add your own models using the CLI or config file.</p>"},{"location":"reference/models/#via-cli","title":"Via CLI","text":"<pre><code># Interactive\nsoong models add\n\n# With flags\nsoong models add \\\n  --name my-model-70b \\\n  --hf-path myorg/custom-llama-70b \\\n  --params 70 \\\n  --quantization int4 \\\n  --context 8192\n</code></pre>"},{"location":"reference/models/#via-config-file","title":"Via Config File","text":"<p>Edit <code>~/.config/gpu-dashboard/config.yaml</code>:</p> <pre><code>custom_models:\n  my-model-70b:\n    hf_path: myorg/custom-llama-70b\n    params_billions: 70\n    quantization: int4\n    context_length: 8192\n    notes: Fine-tuned on domain data\n</code></pre> <p>See Configuration Reference for full schema.</p>"},{"location":"reference/models/#listing-models","title":"Listing Models","text":"<pre><code># List all models (built-in + custom)\nsoong models\n\n# Get detailed info\nsoong models info deepseek-r1-70b\n</code></pre>"},{"location":"reference/models/#model-update-policy","title":"Model Update Policy","text":"<p>Built-in models are updated when:</p> <ol> <li>New high-quality models release (e.g., Llama 4, GPT-4 class)</li> <li>Better quantization methods (e.g., ExLlamaV2, GGUF improvements)</li> <li>Lambda Labs adds new GPUs (requiring different recommendations)</li> </ol> <p>Check releases for model registry updates.</p>"},{"location":"reference/models/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Approximate tokens/second on Lambda Labs GPUs:</p> Model A10 (24GB) A100 (80GB) H100 (80GB) Llama 8B FP16 80 tok/s 120 tok/s 200 tok/s Qwen 32B INT4 40 tok/s 60 tok/s 100 tok/s DeepSeek 70B INT4 N/A 30 tok/s 50 tok/s Qwen 32B FP16 N/A 45 tok/s 75 tok/s <p>Benchmarks approximate. Actual performance varies by context length and prompt complexity.</p>"},{"location":"reference/models/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"reference/models/#why-int4-for-70b-models","title":"Why INT4 for 70B models?","text":"<p>INT4 quantization (GPTQ/AWQ) allows 70B models to fit on 80GB GPUs with minimal quality loss (~5-8%). Without it, you'd need multi-GPU setups.</p>"},{"location":"reference/models/#can-i-use-gguf-models","title":"Can I use GGUF models?","text":"<p>Currently, <code>soong</code> uses SGLang which doesn't support GGUF. Models must be in HuggingFace Transformers format (safetensors).</p>"},{"location":"reference/models/#what-about-lora-adapters","title":"What about LoRA adapters?","text":"<p>Custom LoRA adapters can be loaded by specifying the adapter path in cloud-init scripts. Not currently automated in CLI.</p>"},{"location":"reference/models/#how-do-i-choose-between-deepseek-and-qwen","title":"How do I choose between DeepSeek and Qwen?","text":"<ul> <li>DeepSeek: Better for complex reasoning, architecture decisions, debugging</li> <li>Qwen: Better for code generation, refactoring, working with large files</li> </ul>"},{"location":"reference/models/#why-is-qwen-fp16-so-expensive-69-gb","title":"Why is Qwen FP16 so expensive (69 GB)?","text":"<p>FP16 doubles memory vs INT4. Qwen 32B FP16 = 64 GB base + overhead = 69 GB total. Use INT4 version if cost matters.</p>"},{"location":"reference/models/#see-also","title":"See Also","text":"<ul> <li>GPU Types Reference - Available GPUs</li> <li>Configuration Reference - Custom model setup</li> <li>CLI Commands - <code>models</code> command reference</li> </ul>"}]}